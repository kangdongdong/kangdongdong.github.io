<!-- build time:Sun Mar 08 2020 13:43:15 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html lang="zh"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no,minimal-ui"><meta name="renderer" content="webkit"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=no,email=no,adress=no"><meta name="theme-color" content="#000000"><meta http-equiv="window-target" content="_top"><title>MapReduce框架原理 | kaiden</title><meta name="description" content="总结MapReduce框架原理1 InputFotmat数据输入待写2 MapReduce工作流程待写3 Shuffle机制3.1 Shuffle机制介绍Map方法之后，Reduce方法之前的数据称之为Shuffle3.2 主要功能Partition分区，WritableComparable排序，Combiner合并，GroupingComparator分组（辅助排序）4 MapTask工作机制​"><meta property="og:type" content="article"><meta property="og:title" content="MapReduce框架原理"><meta property="og:url" content="http://kiedeng.github.io/2020/02/29/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86/index.html"><meta property="og:site_name" content="Hexo"><meta property="og:description" content="总结MapReduce框架原理1 InputFotmat数据输入待写2 MapReduce工作流程待写3 Shuffle机制3.1 Shuffle机制介绍Map方法之后，Reduce方法之前的数据称之为Shuffle3.2 主要功能Partition分区，WritableComparable排序，Combiner合并，GroupingComparator分组（辅助排序）4 MapTask工作机制​"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2020-02-29T09:08:56.000Z"><meta property="article:modified_time" content="2020-02-29T13:57:55.988Z"><meta property="article:author" content="John Doe"><meta property="article:tag" content="默认"><meta name="twitter:card" content="summary"><link rel="canonical" href="http://kiedeng.github.io/2020/02/29/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86/index.html"><link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml"><link rel="icon" href="/favicon.png" type="image/x-icon"><link rel="stylesheet" href="../../../../css/style.css"><link href="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.3.5/dist/jquery.fancybox.min.css" rel="stylesheet"><meta name="generator" content="Hexo 4.2.0"></head><body class="main-center theme-black" itemscope itemtype="http://schema.org/WebPage"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="slimContent"><div class="navbar-header"><div class="profile-block text-center"><a id="avatar" href="https://github.com/kiedeng" target="_blank"><img class="img-circle img-rotate" src="../../../../images/avatar.jpg" width="200" height="200"></a><h2 id="name" class="hidden-xs hidden-sm">康栋</h2><h3 id="title" class="hidden-xs hidden-sm hidden-md">爬虫爱好者 &amp; 大数据学习者</h3><small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> 湖南, 中国</small></div><div class="search" id="search-form-wrap"><form class="search-form sidebar-form" method="GET" action="https://www.baidu.com/s?"><div class="input-group"><input name="wd" type="text" class="form-control search-form-input" placeholder="搜索"> <span class="input-group-btn"><button type="submit" class="btn btn-flat search-form-submit"><i class="icon icon-search"></i></button></span></div></form></div><button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false"><span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span></button></div><nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation"><ul class="nav navbar-nav main-nav menu-highlight"><li class="menu-item menu-item-home"><a href="../../../../."><i class="icon icon-home-fill"></i> <span class="menu-title">首页</span></a></li><li class="menu-item menu-item-about"><a href="../../../../about"><i class="icon icon-cup-fill"></i> <span class="menu-title">简历</span></a></li><li class="menu-item menu-item-archives"><a href="../../../../archives"><i class="icon icon-archives-fill"></i> <span class="menu-title">归档</span></a></li><li class="menu-item menu-item-categories"><a href="../../../../categories"><i class="icon icon-folder"></i> <span class="menu-title">分类</span></a></li><li class="menu-item menu-item-tags"><a href="../../../../tags"><i class="icon icon-tags"></i> <span class="menu-title">标签</span></a></li><li class="menu-item menu-item-repository"><a href="../../../../repository"><i class="icon icon-project"></i> <span class="menu-title">项目</span></a></li><li class="menu-item menu-item-links"><a href="../../../../links"><i class="icon icon-friendship"></i> <span class="menu-title">友链</span></a></li></ul><ul class="social-links"><li><a href="../../../../https:/github.com/kiedeng" target="_blank" title="Github" data-toggle="tooltip" data-placement="top"><i class="icon icon-github"></i></a></li><li><a href="../../../../https:/weibo.com/7098863005" target="_blank" title="Weibo" data-toggle="tooltip" data-placement="top"><i class="icon icon-weibo"></i></a></li></ul></nav></div></header><aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar"><div class="slimContent"><div class="widget"><h3 class="widget-title">公告</h3><div class="widget-body"><div id="board"><div class="content"><p>欢迎交流与分享经验!</p></div></div></div></div><div class="widget"><h3 class="widget-title">分类</h3><div class="widget-body"><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="../../../../categories/HDFS/">HDFS</a><span class="category-list-count">4</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="../../../../categories/HDFS/Hadoop/">Hadoop</a><span class="category-list-count">4</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/Hadoop/">Hadoop</a><span class="category-list-count">11</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="../../../../categories/Hadoop/Mapreduce/">Mapreduce</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/Linux/">Linux</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/MySQL/">MySQL</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/Tomcat/">Tomcat</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/Zookeeper/">Zookeeper</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/">使用手册</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E5%B7%A5%E5%85%B7/">工具</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/">常见错误</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E6%9C%AA%E5%88%86%E7%B1%BB/">未分类</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E7%AE%97%E6%B3%95/">算法</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/%E9%83%A8%E7%BD%B2/">部署</a><span class="category-list-count">3</span></li></ul></div></div><div class="widget"><h3 class="widget-title">标签</h3><div class="widget-body"><ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/Git/" rel="tag">Git</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/GitHub/" rel="tag">GitHub</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/Hadoop/" rel="tag">Hadoop</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/LeetCode/" rel="tag">LeetCode</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/Linux/" rel="tag">Linux</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/MapReduce/" rel="tag">MapReduce</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/VMware/" rel="tag">VMware</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/centos/" rel="tag">centos</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/hexo/" rel="tag">hexo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/%E4%BB%A3%E7%A0%81/" rel="tag">代码</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/%E5%8E%9F%E7%90%86/" rel="tag">原理</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag">大数据</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/%E6%95%99%E7%A8%8B/" rel="tag">教程</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/%E6%A1%88%E4%BE%8B/" rel="tag">案例</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/%E8%87%AA%E5%8A%A8%E5%8C%96/" rel="tag">自动化</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/%E9%83%A8%E7%BD%B2/" rel="tag">部署</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/%E9%BB%98%E8%AE%A4/" rel="tag">默认</a><span class="tag-list-count">20</span></li></ul></div></div><div class="widget"><h3 class="widget-title">标签云</h3><div class="widget-body tagcloud"><a href="../../../../tags/Git/" style="font-size:13px">Git</a> <a href="../../../../tags/GitHub/" style="font-size:13px">GitHub</a> <a href="../../../../tags/Hadoop/" style="font-size:13px">Hadoop</a> <a href="../../../../tags/LeetCode/" style="font-size:13px">LeetCode</a> <a href="../../../../tags/Linux/" style="font-size:13.33px">Linux</a> <a href="../../../../tags/MapReduce/" style="font-size:13.33px">MapReduce</a> <a href="../../../../tags/VMware/" style="font-size:13px">VMware</a> <a href="../../../../tags/centos/" style="font-size:13px">centos</a> <a href="../../../../tags/hexo/" style="font-size:13px">hexo</a> <a href="../../../../tags/%E4%BB%A3%E7%A0%81/" style="font-size:13px">代码</a> <a href="../../../../tags/%E5%8E%9F%E7%90%86/" style="font-size:13.67px">原理</a> <a href="../../../../tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" style="font-size:13px">大数据</a> <a href="../../../../tags/%E6%95%99%E7%A8%8B/" style="font-size:13px">教程</a> <a href="../../../../tags/%E6%A1%88%E4%BE%8B/" style="font-size:13px">案例</a> <a href="../../../../tags/%E8%87%AA%E5%8A%A8%E5%8C%96/" style="font-size:13px">自动化</a> <a href="../../../../tags/%E9%83%A8%E7%BD%B2/" style="font-size:13px">部署</a> <a href="../../../../tags/%E9%BB%98%E8%AE%A4/" style="font-size:14px">默认</a></div></div><div class="widget"><h3 class="widget-title">归档</h3><div class="widget-body"><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2020/03/">三月 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2020/02/">二月 2020</a><span class="archive-list-count">31</span></li></ul></div></div><div class="widget"><h3 class="widget-title">最新文章</h3><div class="widget-body"><ul class="recent-post-list list-unstyled"><li><div class="item-thumb"><a href="../../../03/08/shell%E8%84%9A%E6%9C%AC%E6%96%87%E4%BB%B6/" class="thumb"><span class="thumb-image thumb-none"></span></a></div><div class="item-inner"><p class="item-category"><a class="category-link" href="../../../../categories/Linux/">Linux</a></p><p class="item-title"><a href="../../../03/08/shell%E8%84%9A%E6%9C%AC%E6%96%87%E4%BB%B6/" class="title">shell脚本文件</a></p><p class="item-date"><time datetime="2020-03-07T16:39:20.000Z" itemprop="datePublished">2020-03-08</time></p></div></li><li><div class="item-thumb"><a href="../../../03/07/shell%E6%93%8D%E4%BD%9C/" class="thumb"><span class="thumb-image thumb-none"></span></a></div><div class="item-inner"><p class="item-category"><a class="category-link" href="../../../../categories/Linux/">Linux</a></p><p class="item-title"><a href="../../../03/07/shell%E6%93%8D%E4%BD%9C/" class="title">shell操作</a></p><p class="item-date"><time datetime="2020-03-07T09:28:34.000Z" itemprop="datePublished">2020-03-07</time></p></div></li><li><div class="item-thumb"><a href="../../../03/04/Zookeeper/" class="thumb"><span class="thumb-image thumb-none"></span></a></div><div class="item-inner"><p class="item-category"><a class="category-link" href="../../../../categories/Zookeeper/">Zookeeper</a></p><p class="item-title"><a href="../../../03/04/Zookeeper/" class="title">Zookeeper</a></p><p class="item-date"><time datetime="2020-03-04T05:09:50.000Z" itemprop="datePublished">2020-03-04</time></p></div></li><li><div class="item-thumb"><a href="../../../03/01/%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9/" class="thumb"><span class="thumb-image thumb-none"></span></a></div><div class="item-inner"><p class="item-category"><a class="category-link" href="../../../../categories/Hadoop/">Hadoop</a></p><p class="item-title"><a href="../../../03/01/%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9/" class="title">数据压缩</a></p><p class="item-date"><time datetime="2020-03-01T04:39:39.000Z" itemprop="datePublished">2020-03-01</time></p></div></li><li><div class="item-thumb"><a href="" class="thumb"><span class="thumb-image thumb-none"></span></a></div><div class="item-inner"><p class="item-category"><a class="category-link" href="../../../../categories/Hadoop/">Hadoop</a></p><p class="item-title"><a href="" class="title">MapReduce框架原理</a></p><p class="item-date"><time datetime="2020-02-29T09:08:56.000Z" itemprop="datePublished">2020-02-29</time></p></div></li></ul></div></div></div></aside><aside class="sidebar sidebar-toc collapse" id="collapseToc" itemscope itemtype="http://schema.org/WPSideBar"><div class="slimContent"><nav id="toc" class="article-toc"><h3 class="toc-title">文章目录</h3><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-InputFotmat数据输入"><span class="toc-number">1.</span> <span class="toc-text">1 InputFotmat数据输入</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-MapReduce工作流程"><span class="toc-number">2.</span> <span class="toc-text">2 MapReduce工作流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Shuffle机制"><span class="toc-number">3.</span> <span class="toc-text">3 Shuffle机制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Shuffle机制介绍"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 Shuffle机制介绍</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-MapTask工作机制"><span class="toc-number">4.</span> <span class="toc-text">4 MapTask工作机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-RedeceTask工作机制"><span class="toc-number">5.</span> <span class="toc-text">5 RedeceTask工作机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-OutputFormat数据输出"><span class="toc-number">6.</span> <span class="toc-text">6 OutputFormat数据输出</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-自定义OutputFomat步骤"><span class="toc-number">6.1.</span> <span class="toc-text">6.1 自定义OutputFomat步骤</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-Join多种应用"><span class="toc-number">7.</span> <span class="toc-text">7 Join多种应用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-工作原理"><span class="toc-number">7.1.</span> <span class="toc-text">7.1 工作原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-Reduce-join"><span class="toc-number">7.2.</span> <span class="toc-text">7.2 Reduce join</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#map-join"><span class="toc-number">7.3.</span> <span class="toc-text">map join</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-计数器应用与数据清洗（ETL）"><span class="toc-number">8.</span> <span class="toc-text">8 计数器应用与数据清洗（ETL）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-MapRedece开发总结"><span class="toc-number">9.</span> <span class="toc-text">9 MapRedece开发总结</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-输入数据接口：InputFormat"><span class="toc-number">9.0.1.</span> <span class="toc-text">1. 输入数据接口：InputFormat</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-逻辑处理接口：Mapper"><span class="toc-number">9.0.2.</span> <span class="toc-text">2.逻辑处理接口：Mapper</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-Partitioner分区"><span class="toc-number">9.0.3.</span> <span class="toc-text">3.Partitioner分区</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-Comparable排序"><span class="toc-number">9.0.4.</span> <span class="toc-text">4.Comparable排序</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-Combiner合并"><span class="toc-number">9.0.5.</span> <span class="toc-text">5.Combiner合并</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-Reduce端分组：GroupingComparator"><span class="toc-number">9.0.6.</span> <span class="toc-text">6.Reduce端分组：GroupingComparator</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-逻辑处理接口："><span class="toc-number">9.0.7.</span> <span class="toc-text">7.逻辑处理接口：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8-输出数据接口：OutputFormat"><span class="toc-number">9.0.8.</span> <span class="toc-text">8.输出数据接口：OutputFormat</span></a></li></ol></li></ol></nav></div></aside><main class="main" role="main"><div class="content"><article id="post-MapReduce框架原理" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting"><div class="article-header"><h1 class="article-title" itemprop="name">MapReduce框架原理</h1><div class="article-meta"><span class="article-date"><i class="icon icon-calendar-check"></i> <a href="" class="article-date"><time datetime="2020-02-29T09:08:56.000Z" itemprop="datePublished">2020-02-29</time></a></span> <span class="article-category"><i class="icon icon-folder"></i> <a class="article-category-link" href="../../../../categories/Hadoop/">Hadoop</a></span> <span class="article-tag"><i class="icon icon-tags"></i> <a class="article-tag-link" href="../../../../tags/%E9%BB%98%E8%AE%A4/" rel="tag">默认</a></span> <span class="article-read hidden-xs"><i class="icon icon-eye-fill" aria-hidden="true"></i> <span id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span></span></span> <span class="post-comment"><i class="icon icon-comment"></i> <a href="#comments" class="article-comment-link">评论</a></span> <span class="post-wordcount hidden-xs" itemprop="wordCount">字数统计: 3.3k(字)</span> <span class="post-readcount hidden-xs" itemprop="timeRequired">阅读时长: 14(分)</span></div></div><div class="article-entry marked-body" itemprop="articleBody"><ul><li>总结MapReduce框架原理</li></ul><h2 id="1-InputFotmat数据输入"><a href="#1-InputFotmat数据输入" class="headerlink" title="1 InputFotmat数据输入"></a>1 InputFotmat数据输入</h2><p>待写</p><h2 id="2-MapReduce工作流程"><a href="#2-MapReduce工作流程" class="headerlink" title="2 MapReduce工作流程"></a>2 MapReduce工作流程</h2><p>待写</p><h2 id="3-Shuffle机制"><a href="#3-Shuffle机制" class="headerlink" title="3 Shuffle机制"></a>3 Shuffle机制</h2><h3 id="3-1-Shuffle机制介绍"><a href="#3-1-Shuffle机制介绍" class="headerlink" title="3.1 Shuffle机制介绍"></a>3.1 Shuffle机制介绍</h3><p>Map方法之后，Reduce方法之前的数据称之为Shuffle</p><p>3.2 主要功能</p><p>Partition分区，WritableComparable排序，Combiner合并，GroupingComparator分组（辅助排序）</p><h2 id="4-MapTask工作机制"><a href="#4-MapTask工作机制" class="headerlink" title="4 MapTask工作机制"></a>4 MapTask工作机制</h2><p>​ （1）Read阶段：MapTask通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value。</p><p>​ （2）Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value。</p><p>​ （3）Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner），并写入一个环形内存缓冲区中。</p><p>​ （4）Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。</p><p>​ 溢写阶段详情：</p><p>​ 步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号Partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。</p><p>​ 步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。</p><p>​ 步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中。</p><p>​ （5）Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。</p><p>​ 当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。</p><p>​ 在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor（默认10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。</p><p>​ 让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。</p><h2 id="5-RedeceTask工作机制"><a href="#5-RedeceTask工作机制" class="headerlink" title="5 RedeceTask工作机制"></a>5 RedeceTask工作机制</h2><p>​ （1）Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</p><p>​ （2）Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。</p><p>​ （3）Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。</p><p>​ （4）Reduce阶段：reduce()函数将计算结果写到HDFS上。</p><h2 id="6-OutputFormat数据输出"><a href="#6-OutputFormat数据输出" class="headerlink" title="6 OutputFormat数据输出"></a>6 OutputFormat数据输出</h2><p>​ OutputFormat是MapReduce输出的基类，所有实现MapReduce输出都实现了OutputFormat接口。</p><p>下面我们介绍几种常见的OutputFormat实现类。</p><p>​ 1.文本输出TextOutputFormat默认的输出格式是TextOutputFormat，它把每条记录写为文本行。它的键和值可以是任意类型，因为TextOutputFormat调用toString0方法把它们转换为字符串。</p><p>​ 2.SequenceFileOutputFormat将SequenceFileOutputFormat输出作为后续MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑，很容易被压缩。</p><p>​ 3.自定义OutputFormat根据用户需求，自定义实现输出。</p><h3 id="6-1-自定义OutputFomat步骤"><a href="#6-1-自定义OutputFomat步骤" class="headerlink" title="6.1 自定义OutputFomat步骤"></a>6.1 自定义OutputFomat步骤</h3><p>​ （1）自定义一个类继承FileOutputFormat。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.mapreduce.outputformat;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">import org.apache.hadoop.io.NullWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line">import org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line">public class FilterOutputFormat extends FileOutputFormat&lt;Text, NullWritable&gt;&#123;</span><br><span class="line"></span><br><span class="line">	@Override</span><br><span class="line">	public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter(TaskAttemptContext job)			throws IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; 创建一个RecordWriter</span><br><span class="line">		return new FilterRecordWriter(job);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​ （2）改写RecordWriter，具体改写输出数据的方法write。</p><p>​ <strong>RecordWriter格式：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.mapreduce.outputformat;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">import org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line">import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.NullWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line">import org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"></span><br><span class="line">public class FilterRecordWriter extends RecordWriter&lt;Text, NullWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">	FSDataOutputStream atguiguOut &#x3D; null;</span><br><span class="line">	FSDataOutputStream otherOut &#x3D; null;</span><br><span class="line"></span><br><span class="line">	public FilterRecordWriter(TaskAttemptContext job) &#123;</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; 1 获取文件系统</span><br><span class="line">		FileSystem fs;</span><br><span class="line"></span><br><span class="line">		try &#123;</span><br><span class="line">			fs &#x3D; FileSystem.get(job.getConfiguration());</span><br><span class="line"></span><br><span class="line">			&#x2F;&#x2F; 2 创建输出文件路径</span><br><span class="line">			Path atguiguPath &#x3D; new Path(&quot;e:&#x2F;atguigu.log&quot;);</span><br><span class="line">			Path otherPath &#x3D; new Path(&quot;e:&#x2F;other.log&quot;);</span><br><span class="line"></span><br><span class="line">			&#x2F;&#x2F; 3 创建输出流</span><br><span class="line">			atguiguOut &#x3D; fs.create(atguiguPath);</span><br><span class="line">			otherOut &#x3D; fs.create(otherPath);</span><br><span class="line">		&#125; catch (IOException e) &#123;</span><br><span class="line">			e.printStackTrace();</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	@Override</span><br><span class="line">	public void write(Text key, NullWritable value) throws IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; 判断是否包含“atguigu”输出到不同文件</span><br><span class="line">		if (key.toString().contains(&quot;atguigu&quot;)) &#123;</span><br><span class="line">			atguiguOut.write(key.toString().getBytes());</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">			otherOut.write(key.toString().getBytes());</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	@Override</span><br><span class="line">	public void close(TaskAttemptContext context) throws IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; 关闭资源</span><br><span class="line">IOUtils.closeStream(atguiguOut);</span><br><span class="line">		IOUtils.closeStream(otherOut);	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意：记得将自定义输出格式设置到job中</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 要将自定义的输出格式组件设置到job中</span></span><br><span class="line">job.setOutputFormatClass(FilterOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure><h2 id="7-Join多种应用"><a href="#7-Join多种应用" class="headerlink" title="7 Join多种应用"></a>7 Join多种应用</h2><h3 id="7-1-工作原理"><a href="#7-1-工作原理" class="headerlink" title="7.1 工作原理"></a>7.1 工作原理</h3><p>​ Map端的主要工作：为来自不同表或文件的key/value对，打标签以区别不同来源的记录。然后用连接字段作为key，其余部分和新加的标志作为value，最后进行输出。</p><p>​ Reduce端的主要工作：在Reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录（在Map阶段已经打标志分开，最后进行合并就ok了。</p><h3 id="7-2-Reduce-join"><a href="#7-2-Reduce-join" class="headerlink" title="7.2 Reduce join"></a>7.2 Reduce join</h3><p>​ 在Mapper阶段使得连接属性为key，其余属性为value（自定义bean对象，记得序列化），再用一个标记属性标记来自于哪个文件</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce.table;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">TableBean</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">String name;</span><br><span class="line">	TableBean bean = <span class="keyword">new</span> TableBean();</span><br><span class="line">	Text k = <span class="keyword">new</span> Text();</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 获取输入文件切片</span></span><br><span class="line">		FileSplit split = (FileSplit) context.getInputSplit();</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 2 获取输入文件名称</span></span><br><span class="line">		name = split.getPath().getName();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 1 获取输入数据</span></span><br><span class="line">		String line = value.toString();</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 2 不同文件分别处理</span></span><br><span class="line">		<span class="keyword">if</span> (name.startsWith(<span class="string">"order"</span>)) &#123;<span class="comment">// 订单表处理</span></span><br><span class="line"></span><br><span class="line">			<span class="comment">// 2.1 切割</span></span><br><span class="line">			String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line">			</span><br><span class="line">			<span class="comment">// 2.2 封装bean对象</span></span><br><span class="line">			bean.setOrder_id(fields[<span class="number">0</span>]);</span><br><span class="line">			bean.setP_id(fields[<span class="number">1</span>]);</span><br><span class="line">			bean.setAmount(Integer.parseInt(fields[<span class="number">2</span>]));</span><br><span class="line">			bean.setPname(<span class="string">""</span>);</span><br><span class="line">			bean.setFlag(<span class="string">"order"</span>);</span><br><span class="line">			</span><br><span class="line">			k.set(fields[<span class="number">1</span>]);</span><br><span class="line">		&#125;<span class="keyword">else</span> &#123;<span class="comment">// 产品表处理</span></span><br><span class="line"></span><br><span class="line">			<span class="comment">// 2.3 切割</span></span><br><span class="line">			String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line">			</span><br><span class="line">			<span class="comment">// 2.4 封装bean对象</span></span><br><span class="line">			bean.setP_id(fields[<span class="number">0</span>]);</span><br><span class="line">			bean.setPname(fields[<span class="number">1</span>]);</span><br><span class="line">			bean.setFlag(<span class="string">"pd"</span>);</span><br><span class="line">			bean.setAmount(<span class="number">0</span>);</span><br><span class="line">			bean.setOrder_id(<span class="string">""</span>);</span><br><span class="line">			</span><br><span class="line">			k.set(fields[<span class="number">0</span>]);</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 3 写出</span></span><br><span class="line">		context.write(k, bean);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​ 在Reduce阶段，输出连接成功的bean对象</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce.table;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.beanutils.BeanUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">TableBean</span>, <span class="title">TableBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;TableBean&gt; values, Context context)</span>	<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1准备存储订单的集合</span></span><br><span class="line">		ArrayList&lt;TableBean&gt; orderBeans = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">		</span><br><span class="line"><span class="comment">// 2 准备bean对象</span></span><br><span class="line">		TableBean pdBean = <span class="keyword">new</span> TableBean();</span><br><span class="line"></span><br><span class="line">		<span class="keyword">for</span> (TableBean bean : values) &#123;</span><br><span class="line"></span><br><span class="line">			<span class="keyword">if</span> (<span class="string">"order"</span>.equals(bean.getFlag())) &#123;<span class="comment">// 订单表</span></span><br><span class="line"></span><br><span class="line">				<span class="comment">// 拷贝传递过来的每条订单数据到集合中</span></span><br><span class="line">				TableBean orderBean = <span class="keyword">new</span> TableBean();</span><br><span class="line"></span><br><span class="line">				<span class="keyword">try</span> &#123;</span><br><span class="line">					BeanUtils.copyProperties(orderBean, bean);</span><br><span class="line">				&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">					e.printStackTrace();</span><br><span class="line">				&#125;</span><br><span class="line"></span><br><span class="line">				orderBeans.add(orderBean);</span><br><span class="line">			&#125; <span class="keyword">else</span> &#123;<span class="comment">// 产品表</span></span><br><span class="line"></span><br><span class="line">				<span class="keyword">try</span> &#123;</span><br><span class="line">					<span class="comment">// 拷贝传递过来的产品表到内存中</span></span><br><span class="line">					BeanUtils.copyProperties(pdBean, bean);</span><br><span class="line">				&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">					e.printStackTrace();</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 3 表的拼接</span></span><br><span class="line">		<span class="keyword">for</span>(TableBean bean:orderBeans)&#123;</span><br><span class="line"></span><br><span class="line">			bean.setPname (pdBean.getPname());</span><br><span class="line">			</span><br><span class="line">			<span class="comment">// 4 数据写出去</span></span><br><span class="line">			context.write(bean, NullWritable.get());</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>缺点：这种方式中，合并的操作是在Reduce阶段完成，Reduce端的处理压力太大，Map节点的运算负载则很低，资源利用率不高，且在Reduce阶段极易产生数据倾斜。</p><h3 id="map-join"><a href="#map-join" class="headerlink" title="map join"></a>map join</h3><p>使用场景：Map Join适用于一张表十分小、一张表很大的场景。</p><p>具体办法：采用DistributedCache</p><p>​ （1）在Mapper的setup阶段，将文件读取到缓存集合中。</p><p>​ （2）在驱动函数中加载缓存。</p><p>​ // 缓存普通文件到Task运行节点。</p><p>​ job.addCacheFile(new URI(“file://e:/cache/pd.txt”));</p><p>注：简单说就是把一个小表用map表示，用大表的连接属性直接映射出小表的属性</p><p><strong>对于驱动模块Driver来说</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 6 加载缓存数据</span><br><span class="line">job.addCacheFile(new URI(&quot;file:&#x2F;&#x2F;&#x2F;e:&#x2F;input&#x2F;inputcache&#x2F;pd.txt&quot;));	</span><br><span class="line">&#x2F;&#x2F; 7 Map端Join的逻辑不需要Reduce阶段，设置reduceTask数量为0</span><br><span class="line">job.setNumReduceTasks(0);</span><br></pre></td></tr></table></figure><p>对于mapper来说，先在map前读取缓存数据</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> test;</span><br><span class="line"><span class="keyword">import</span> java.io.BufferedReader;</span><br><span class="line"><span class="keyword">import</span> java.io.FileInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStreamReader;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang.StringUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DistributedCacheMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">	Map&lt;String, String&gt; pdMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 获取缓存的文件</span></span><br><span class="line">		URI[] cacheFiles = context.getCacheFiles();</span><br><span class="line">		String path = cacheFiles[<span class="number">0</span>].getPath().toString();</span><br><span class="line">		</span><br><span class="line">		BufferedReader reader = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(<span class="keyword">new</span> FileInputStream(path), <span class="string">"UTF-8"</span>));</span><br><span class="line">		</span><br><span class="line">		String line;</span><br><span class="line">		<span class="keyword">while</span>(StringUtils.isNotEmpty(line = reader.readLine()))&#123;</span><br><span class="line"></span><br><span class="line">			<span class="comment">// 2 切割</span></span><br><span class="line">			String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line">			</span><br><span class="line">			<span class="comment">// 3 缓存数据到集合</span></span><br><span class="line">			pdMap.put(fields[<span class="number">0</span>], fields[<span class="number">1</span>]);</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 4 关流</span></span><br><span class="line">		reader.close();</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	Text k = <span class="keyword">new</span> Text();</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 获取一行</span></span><br><span class="line">		String line = value.toString();</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 2 截取</span></span><br><span class="line">		String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 3 获取产品id</span></span><br><span class="line">		String pId = fields[<span class="number">1</span>];</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 4 获取商品名称</span></span><br><span class="line">		String pdName = pdMap.get(pId);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 5 拼接</span></span><br><span class="line">		k.set(line + <span class="string">"\t"</span>+ pdName);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 6 写出</span></span><br><span class="line">		context.write(k, NullWritable.get());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="8-计数器应用与数据清洗（ETL）"><a href="#8-计数器应用与数据清洗（ETL）" class="headerlink" title="8 计数器应用与数据清洗（ETL）"></a>8 计数器应用与数据清洗（ETL）</h2><p>​ Hadoop为每个作业维护若干内置计数器，以描述多项指标。例如，某些计数器记录已处理的字节数和记录数，使用户可监控已处理的输入数据量和已产生的输出数据量。</p><p>使用方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">context.getCounter(<span class="string">"map"</span>,<span class="string">"失败"</span>).increment(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><h2 id="9-MapRedece开发总结"><a href="#9-MapRedece开发总结" class="headerlink" title="9 MapRedece开发总结"></a>9 MapRedece开发总结</h2><h4 id="1-输入数据接口：InputFormat"><a href="#1-输入数据接口：InputFormat" class="headerlink" title="1. 输入数据接口：InputFormat"></a>1. 输入数据接口：InputFormat</h4><p>（1）默认使用的实现类是：TextInputFormat</p><p>（2）TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回。</p><p>（3）KeyValueTextlnputFomat每一行均为一条记录，被分隔符分割为key，value。默认分隔符是tab（\t）。<br>（4）NlinelnputFormat按照指定的行数N来划分切片。</p><p>（5）CombineTextlnputFormat可以把多个小文件合并成一个切片处理，提高处理效率。</p><p>（6）用户还可以自定义ImputFormat。</p><h4 id="2-逻辑处理接口：Mapper"><a href="#2-逻辑处理接口：Mapper" class="headerlink" title="2.逻辑处理接口：Mapper"></a>2.逻辑处理接口：Mapper</h4><p>用户根据业务需求实现其中三个方法：map）setup）cleanup）</p><h4 id="3-Partitioner分区"><a href="#3-Partitioner分区" class="headerlink" title="3.Partitioner分区"></a>3.Partitioner分区</h4><p>（1）有默认实现HashPartitioner，逻辑是根据key的哈希值和numReduces来返回一个分区号；key hashCode（）&amp;Integer.MAXVALUE%<br>numReduces</p><p>（2）如果业务上有特别的需求，可以自定义分区。</p><h4 id="4-Comparable排序"><a href="#4-Comparable排序" class="headerlink" title="4.Comparable排序"></a>4.Comparable排序</h4><p>（1）当我们用自定义的对象作为key来输出时，就必须要实现WritableComparable接口，重写其中的compareTo0方法。</p><p>（2）部分排序：对最终输出的每一个文件进行内部排序。</p><p>（3）全排序：对所有数据进行排序，通常只有一个Reduce。</p><p>（4）二次排序：排序的条件有两个。</p><h4 id="5-Combiner合并"><a href="#5-Combiner合并" class="headerlink" title="5.Combiner合并"></a>5.Combiner合并</h4><p>Combiner合并可以提高程序执行效率，减少I0传输。但是使用时必须不能影响原有的业务处理结果。</p><h4 id="6-Reduce端分组：GroupingComparator"><a href="#6-Reduce端分组：GroupingComparator" class="headerlink" title="6.Reduce端分组：GroupingComparator"></a>6.Reduce端分组：GroupingComparator</h4><p>在Reduce端对key进行分组。应用于：在接收的key为bean对象时，想让一个或几个字段相同（全部字段比较不相同）的key进入到同一个reduce方法时，可以采用分组排序。</p><h4 id="7-逻辑处理接口："><a href="#7-逻辑处理接口：" class="headerlink" title="7.逻辑处理接口："></a>7.逻辑处理接口：</h4><p>Reducer用户根据业务需求实现其中三个方法：reduce();setup();cleanup()</p><h4 id="8-输出数据接口：OutputFormat"><a href="#8-输出数据接口：OutputFormat" class="headerlink" title="8.输出数据接口：OutputFormat"></a>8.输出数据接口：OutputFormat</h4><p>（1）默认实现类是TextOutputFormat，功能逻辑是：将每一个KV对，向目标文本文件输出一行。</p><p>（2）将SequenceFileOutputFormat输出作为后续MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑，很容易被压缩。</p><p>（3）用户还可以自定义OutputFormat。</p></div><div class="article-footer"><blockquote class="mt-2x"><ul class="post-copyright list-unstyled"><li class="post-copyright-link hidden-xs"><strong>本文链接：</strong> <a href="http://kiedeng.github.io/2020/02/29/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86/" title="MapReduce框架原理" target="_blank" rel="external">http://kiedeng.github.io/2020/02/29/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86/</a></li><li class="post-copyright-license"><strong>版权声明： </strong><a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external"></a> 转载请注明出处！</li></ul></blockquote><div class="panel panel-default panel-badger"><div class="panel-body"><figure class="media"><div class="media-left"><a href="https://github.com/kiedeng" target="_blank" class="img-burn thumb-sm visible-lg"><img src="../../../../images/avatar.jpg" class="img-rounded w-full" alt=""></a></div><div class="media-body"><h3 class="media-heading"><a href="https://github.com/kiedeng" target="_blank"><span class="text-dark">康栋</span><small class="ml-1x">爬虫爱好者 &amp; 大数据学习者</small></a></h3><div>每天进步一点！！</div></div></figure></div></div></div></article><section id="comments"><div id="lv-container" data-id="city" data-uid="MTAyMC80ODU1OC8yNTA1Mg"><noscript>为正常使用来必力评论功能请激活JavaScript</noscript></div></section></div><nav class="bar bar-footer clearfix" data-stick-bottom><div class="bar-inner"><ul class="pager pull-left"><li class="prev"><a href="../../../03/01/%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9/" title="数据压缩"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;上一篇</span></a></li><li class="next"><a href="../bean%E5%AF%B9%E8%B1%A1%E6%B2%A1%E6%9C%89%E5%BA%8F%E5%88%97%E5%8C%96/" title="bean对象没有序列化"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a></li><li class="toggle-toc"><a class="toggle-btn collapsed" data-toggle="collapse" href="#collapseToc" aria-expanded="false" title="文章目录" role="button"><span>[&nbsp;</span><span>文章目录</span> <i class="text-collapsed icon icon-anchor"></i> <i class="text-in icon icon-close"></i> <span>]</span></a></li></ul><button type="button" class="btn btn-fancy btn-donate pop-onhover bg-gradient-warning" data-toggle="modal" data-target="#donateModal"><span>赏</span></button><div class="bar-right"><div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,qzone"></div></div></div></nav><div class="modal modal-center modal-small modal-xs-full fade" id="donateModal" tabindex="-1" role="dialog"><div class="modal-dialog" role="document"><div class="modal-content donate"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button><div class="modal-body"><div class="donate-box"><div class="donate-head"><p>感谢您的支持，我会继续努力的!</p></div><div class="tab-content"><div role="tabpanel" class="tab-pane fade active in" id="alipay"><div class="donate-payimg"><img src="../../../../images/donate/alipayimg.png" alt="扫码支持" title="扫一扫"></div><p class="text-muted mv">扫码打赏，你说多少就多少</p><p class="text-grey">打开支付宝扫一扫，即可进行扫码打赏哦</p></div><div role="tabpanel" class="tab-pane fade" id="wechatpay"><div class="donate-payimg"><img src="../../../../images/donate/wechatpayimg.png" alt="扫码支持" title="扫一扫"></div><p class="text-muted mv">扫码打赏，你说多少就多少</p><p class="text-grey">打开微信扫一扫，即可进行扫码打赏哦</p></div></div><div class="donate-footer"><ul class="nav nav-tabs nav-justified" role="tablist"><li role="presentation" class="active"><a href="#alipay" id="alipay-tab" role="tab" data-toggle="tab" aria-controls="alipay" aria-expanded="true"><i class="icon icon-alipay"></i> 支付宝</a></li><li role="presentation"><a href="#wechatpay" role="tab" id="wechatpay-tab" data-toggle="tab" aria-controls="wechatpay" aria-expanded="false"><i class="icon icon-wepay"></i> 微信支付</a></li></ul></div></div></div></div></div></div></main><footer class="footer" itemscope itemtype="http://schema.org/WPFooter"><ul class="social-links"><li><a href="../../../../https:/github.com/kiedeng" target="_blank" title="Github" data-toggle="tooltip" data-placement="top"><i class="icon icon-github"></i></a></li><li><a href="../../../../https:/weibo.com/7098863005" target="_blank" title="Weibo" data-toggle="tooltip" data-placement="top"><i class="icon icon-weibo"></i></a></li></ul><div class="copyright"><div class="publishby">2020.2.9 <a href="https://github.com/kiedeng" target="_blank">kiedeng </a>加油 <a href="https://github.com/kiedeng" target="_blank">kangdong</a>.</div></div></footer><script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script><script>window.jQuery||document.write('<script src="js/jquery.min.js"><\/script>')</script><script src="../../../../js/plugin.min.js"></script><script src="../../../../js/application.js"></script><script>!function(i){i(".search-form").on("submit",function(n){var o=i('.search-form-input[name="wd"]').val();return window.location="https://www.baidu.com/s?wd=site:kiedeng.github.io "+o,!1})}(jQuery)</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script defer type="text/javascript">!function(e,t){var n,c=e.getElementsByTagName(t)[0];"function"!=typeof LivereTower&&(n=e.createElement(t),n.src="https://cdn-city.livere.com/js/embed.dist.js",n.async=!0,c.parentNode.insertBefore(n,c))}(document,"script")</script><script src="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.3.5/dist/jquery.fancybox.min.js"></script><script>$(document).ready(function(){$("article img").not("[hidden]").not(".panel-body img").each(function(){var a=$(this),t=a.attr("alt"),n=a.parent("a");if(n.length<1){var e=this.getAttribute("src"),r=e.lastIndexOf("?");-1!=r&&(e=e.substring(0,r)),n=a.wrap('<a href="'+e+'"></a>').parent("a")}n.attr("data-fancybox","images"),t&&n.attr("data-caption",t)}),$().fancybox({selector:'[data-fancybox="images"]',hash:!1,loop:!1})})</script></body></html><!-- rebuild by neat -->