<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="MapReduce框架原理, 成长印记"><meta name="description" content=""><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><title>MapReduce框架原理 | 成长印记</title><link rel="icon" type="image/png" href="../../../../favicon.png"><link rel="stylesheet" type="text/css" href="../../../../libs/awesome/css/all.css"><link rel="stylesheet" type="text/css" href="../../../../libs/materialize/materialize.min.css"><link rel="stylesheet" type="text/css" href="../../../../libs/aos/aos.css"><link rel="stylesheet" type="text/css" href="../../../../libs/animate/animate.min.css"><link rel="stylesheet" type="text/css" href="../../../../libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" type="text/css" href="../../../../css/matery.css"><link rel="stylesheet" type="text/css" href="../../../../css/my.css"><script src="../../../../libs/jquery/jquery.min.js"></script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="atom.xml" title="成长印记" type="application/atom+xml"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="../../../../index.html" class="waves-effect waves-light"><img src="../../../../medias/logo.png" class="logo-img" alt="LOGO"> <span class="logo-span">成长印记</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="../../../../index.html" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="../../../../tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="../../../../categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="../../../../archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:.6"></i> <span>归档</span></a></li><li class="hide-on-med-and-down nav-item"><a href="../../../../about" class="waves-effect waves-light"><i class="fas fa-user-circle" style="zoom:.6"></i> <span>关于</span></a></li><li class="hide-on-med-and-down nav-item"><a href="../../../../contact" class="waves-effect waves-light"><i class="fas fa-comments" style="zoom:.6"></i> <span>留言板</span></a></li><li class="hide-on-med-and-down nav-item"><a href="../../../../friends" class="waves-effect waves-light"><i class="fas fa-address-book" style="zoom:.6"></i> <span>友情链接</span></a></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="../../../../medias/logo.png" class="logo-img circle responsive-img"><div class="logo-name">成长印记</div><div class="logo-desc">Never really desperate, only the lost of the soul.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="../../../../index.html" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="../../../../tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="../../../../categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="../../../../archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> 归档</a></li><li class="m-nav-item"><a href="../../../../about" class="waves-effect waves-light"><i class="fa-fw fas fa-user-circle"></i> 关于</a></li><li class="m-nav-item"><a href="../../../../contact" class="waves-effect waves-light"><i class="fa-fw fas fa-comments"></i> 留言板</a></li><li class="m-nav-item"><a href="../../../../friends" class="waves-effect waves-light"><i class="fa-fw fas fa-address-book"></i> 友情链接</a></li><li><div class="divider"></div></li><li><a href="https://github.com/blinkfox/hexo-theme-matery" class="waves-effect waves-light" target="_blank"><i class="fab fa-github-square fa-fw"></i>Fork Me</a></li></ul></div></div><style>.nav-transparent .github-corner{display:none!important}.github-corner{position:absolute;z-index:10;top:0;right:0;border:0;transform:scale(1.1)}.github-corner svg{color:#0f9d58;fill:#fff;height:64px;width:64px}.github-corner:hover .octo-arm{animation:a .56s ease-in-out}.github-corner .octo-arm{animation:none}@keyframes a{0%,to{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}</style><a href="https://github.com/blinkfox/hexo-theme-matery" class="github-corner tooltipped hide-on-med-and-down" target="_blank" data-tooltip="Fork Me" data-position="left" data-delay="50"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a></nav></header><div class="bg-cover pd-header post-cover" style="background-image:url(../../medias/featureimages/17.jpg)"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">MapReduce框架原理</h1></div></div></div></div></div><main class="post-container content"><link rel="stylesheet" href="../../../../libs/tocbot/tocbot.css"><style>#articleContent h1::before,#articleContent h2::before,#articleContent h3::before,#articleContent h4::before,#articleContent h5::before,#articleContent h6::before{display:block;content:" ";height:100px;margin-top:-100px;visibility:hidden}#articleContent :focus{outline:0}.toc-fixed{position:fixed;top:64px}.toc-widget{width:345px;padding-left:20px}.toc-widget .toc-title{padding:35px 0 15px 17px;font-size:1.5rem;font-weight:700;line-height:1.5rem}.toc-widget ol{padding:0;list-style:none}#toc-content{padding-bottom:30px;overflow:auto}#toc-content ol{padding-left:10px}#toc-content ol li{padding-left:10px}#toc-content .toc-link:hover{color:#42b983;font-weight:700;text-decoration:underline}#toc-content .toc-link::before{background-color:transparent;max-height:25px;position:absolute;right:23.5vw;display:block}#toc-content .is-active-link{color:#42b983}#floating-toc-btn{position:fixed;right:15px;bottom:76px;padding-top:15px;margin-bottom:0;z-index:998}#floating-toc-btn .btn-floating{width:48px;height:48px}#floating-toc-btn .btn-floating i{line-height:48px;font-size:1.4rem}</style><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="../../../../tags/%E9%BB%98%E8%AE%A4/"><span class="chip bg-color">默认</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="../../../../categories/Hadoop/" class="post-category">Hadoop</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp; 2020-02-29</div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><ul><li>总结MapReduce框架原理</li></ul><h2 id="1-InputFotmat数据输入"><a href="#1-InputFotmat数据输入" class="headerlink" title="1 InputFotmat数据输入"></a>1 InputFotmat数据输入</h2><p>待写</p><h2 id="2-MapReduce工作流程"><a href="#2-MapReduce工作流程" class="headerlink" title="2 MapReduce工作流程"></a>2 MapReduce工作流程</h2><p>待写</p><h2 id="3-Shuffle机制"><a href="#3-Shuffle机制" class="headerlink" title="3 Shuffle机制"></a>3 Shuffle机制</h2><h3 id="3-1-Shuffle机制介绍"><a href="#3-1-Shuffle机制介绍" class="headerlink" title="3.1 Shuffle机制介绍"></a>3.1 Shuffle机制介绍</h3><p>Map方法之后，Reduce方法之前的数据称之为Shuffle</p><p>3.2 主要功能</p><p>Partition分区，WritableComparable排序，Combiner合并，GroupingComparator分组（辅助排序）</p><h2 id="4-MapTask工作机制"><a href="#4-MapTask工作机制" class="headerlink" title="4 MapTask工作机制"></a>4 MapTask工作机制</h2><p>​ （1）Read阶段：MapTask通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value。</p><p>​ （2）Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value。</p><p>​ （3）Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner），并写入一个环形内存缓冲区中。</p><p>​ （4）Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。</p><p>​ 溢写阶段详情：</p><p>​ 步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号Partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。</p><p>​ 步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。</p><p>​ 步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中。</p><p>​ （5）Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。</p><p>​ 当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。</p><p>​ 在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor（默认10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。</p><p>​ 让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。</p><h2 id="5-RedeceTask工作机制"><a href="#5-RedeceTask工作机制" class="headerlink" title="5 RedeceTask工作机制"></a>5 RedeceTask工作机制</h2><p>​ （1）Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</p><p>​ （2）Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。</p><p>​ （3）Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。</p><p>​ （4）Reduce阶段：reduce()函数将计算结果写到HDFS上。</p><h2 id="6-OutputFormat数据输出"><a href="#6-OutputFormat数据输出" class="headerlink" title="6 OutputFormat数据输出"></a>6 OutputFormat数据输出</h2><p>​ OutputFormat是MapReduce输出的基类，所有实现MapReduce输出都实现了OutputFormat接口。</p><p>下面我们介绍几种常见的OutputFormat实现类。</p><p>​ 1.文本输出TextOutputFormat默认的输出格式是TextOutputFormat，它把每条记录写为文本行。它的键和值可以是任意类型，因为TextOutputFormat调用toString0方法把它们转换为字符串。</p><p>​ 2.SequenceFileOutputFormat将SequenceFileOutputFormat输出作为后续MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑，很容易被压缩。</p><p>​ 3.自定义OutputFormat根据用户需求，自定义实现输出。</p><h3 id="6-1-自定义OutputFomat步骤"><a href="#6-1-自定义OutputFomat步骤" class="headerlink" title="6.1 自定义OutputFomat步骤"></a>6.1 自定义OutputFomat步骤</h3><p>​ （1）自定义一个类继承FileOutputFormat。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.mapreduce.outputformat;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">import org.apache.hadoop.io.NullWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line">import org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line">public class FilterOutputFormat extends FileOutputFormat&lt;Text, NullWritable&gt;&#123;</span><br><span class="line"></span><br><span class="line">	@Override</span><br><span class="line">	public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter(TaskAttemptContext job)			throws IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">		// 创建一个RecordWriter</span><br><span class="line">		return new FilterRecordWriter(job);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​ （2）改写RecordWriter，具体改写输出数据的方法write。</p><p>​ <strong>RecordWriter格式：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.mapreduce.outputformat;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">import org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line">import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.NullWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line">import org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"></span><br><span class="line">public class FilterRecordWriter extends RecordWriter&lt;Text, NullWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">	FSDataOutputStream atguiguOut = null;</span><br><span class="line">	FSDataOutputStream otherOut = null;</span><br><span class="line"></span><br><span class="line">	public FilterRecordWriter(TaskAttemptContext job) &#123;</span><br><span class="line"></span><br><span class="line">		// 1 获取文件系统</span><br><span class="line">		FileSystem fs;</span><br><span class="line"></span><br><span class="line">		try &#123;</span><br><span class="line">			fs = FileSystem.get(job.getConfiguration());</span><br><span class="line"></span><br><span class="line">			// 2 创建输出文件路径</span><br><span class="line">			Path atguiguPath = new Path(&quot;e:/atguigu.log&quot;);</span><br><span class="line">			Path otherPath = new Path(&quot;e:/other.log&quot;);</span><br><span class="line"></span><br><span class="line">			// 3 创建输出流</span><br><span class="line">			atguiguOut = fs.create(atguiguPath);</span><br><span class="line">			otherOut = fs.create(otherPath);</span><br><span class="line">		&#125; catch (IOException e) &#123;</span><br><span class="line">			e.printStackTrace();</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	@Override</span><br><span class="line">	public void write(Text key, NullWritable value) throws IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">		// 判断是否包含“atguigu”输出到不同文件</span><br><span class="line">		if (key.toString().contains(&quot;atguigu&quot;)) &#123;</span><br><span class="line">			atguiguOut.write(key.toString().getBytes());</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">			otherOut.write(key.toString().getBytes());</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	@Override</span><br><span class="line">	public void close(TaskAttemptContext context) throws IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">		// 关闭资源</span><br><span class="line">IOUtils.closeStream(atguiguOut);</span><br><span class="line">		IOUtils.closeStream(otherOut);	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>注意：记得将自定义输出格式设置到job中</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 要将自定义的输出格式组件设置到job中</span></span><br><span class="line">job.setOutputFormatClass(FilterOutputFormat.class);</span><br></pre></td></tr></table></figure><h2 id="7-Join多种应用"><a href="#7-Join多种应用" class="headerlink" title="7 Join多种应用"></a>7 Join多种应用</h2><h3 id="7-1-工作原理"><a href="#7-1-工作原理" class="headerlink" title="7.1 工作原理"></a>7.1 工作原理</h3><p>​ Map端的主要工作：为来自不同表或文件的key/value对，打标签以区别不同来源的记录。然后用连接字段作为key，其余部分和新加的标志作为value，最后进行输出。</p><p>​ Reduce端的主要工作：在Reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录（在Map阶段已经打标志分开，最后进行合并就ok了。</p><h3 id="7-2-Reduce-join"><a href="#7-2-Reduce-join" class="headerlink" title="7.2 Reduce join"></a>7.2 Reduce join</h3><p>​ 在Mapper阶段使得连接属性为key，其余属性为value（自定义bean对象，记得序列化），再用一个标记属性标记来自于哪个文件</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce.table;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">TableBean</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">String name;</span><br><span class="line">	TableBean bean = <span class="keyword">new</span> TableBean();</span><br><span class="line">	Text k = <span class="keyword">new</span> Text();</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 获取输入文件切片</span></span><br><span class="line">		FileSplit split = (FileSplit) context.getInputSplit();</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 2 获取输入文件名称</span></span><br><span class="line">		name = split.getPath().getName();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 1 获取输入数据</span></span><br><span class="line">		String line = value.toString();</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 2 不同文件分别处理</span></span><br><span class="line">		<span class="keyword">if</span> (name.startsWith(<span class="string">&quot;order&quot;</span>)) &#123;<span class="comment">// 订单表处理</span></span><br><span class="line"></span><br><span class="line">			<span class="comment">// 2.1 切割</span></span><br><span class="line">			String[] fields = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">			</span><br><span class="line">			<span class="comment">// 2.2 封装bean对象</span></span><br><span class="line">			bean.setOrder_id(fields[<span class="number">0</span>]);</span><br><span class="line">			bean.setP_id(fields[<span class="number">1</span>]);</span><br><span class="line">			bean.setAmount(Integer.parseInt(fields[<span class="number">2</span>]));</span><br><span class="line">			bean.setPname(<span class="string">&quot;&quot;</span>);</span><br><span class="line">			bean.setFlag(<span class="string">&quot;order&quot;</span>);</span><br><span class="line">			</span><br><span class="line">			k.set(fields[<span class="number">1</span>]);</span><br><span class="line">		&#125;<span class="keyword">else</span> &#123;<span class="comment">// 产品表处理</span></span><br><span class="line"></span><br><span class="line">			<span class="comment">// 2.3 切割</span></span><br><span class="line">			String[] fields = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">			</span><br><span class="line">			<span class="comment">// 2.4 封装bean对象</span></span><br><span class="line">			bean.setP_id(fields[<span class="number">0</span>]);</span><br><span class="line">			bean.setPname(fields[<span class="number">1</span>]);</span><br><span class="line">			bean.setFlag(<span class="string">&quot;pd&quot;</span>);</span><br><span class="line">			bean.setAmount(<span class="number">0</span>);</span><br><span class="line">			bean.setOrder_id(<span class="string">&quot;&quot;</span>);</span><br><span class="line">			</span><br><span class="line">			k.set(fields[<span class="number">0</span>]);</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 3 写出</span></span><br><span class="line">		context.write(k, bean);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​ 在Reduce阶段，输出连接成功的bean对象</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce.table;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.beanutils.BeanUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">TableBean</span>, <span class="title">TableBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;TableBean&gt; values, Context context)</span>	<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1准备存储订单的集合</span></span><br><span class="line">		ArrayList&lt;TableBean&gt; orderBeans = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">		</span><br><span class="line"><span class="comment">// 2 准备bean对象</span></span><br><span class="line">		TableBean pdBean = <span class="keyword">new</span> TableBean();</span><br><span class="line"></span><br><span class="line">		<span class="keyword">for</span> (TableBean bean : values) &#123;</span><br><span class="line"></span><br><span class="line">			<span class="keyword">if</span> (<span class="string">&quot;order&quot;</span>.equals(bean.getFlag())) &#123;<span class="comment">// 订单表</span></span><br><span class="line"></span><br><span class="line">				<span class="comment">// 拷贝传递过来的每条订单数据到集合中</span></span><br><span class="line">				TableBean orderBean = <span class="keyword">new</span> TableBean();</span><br><span class="line"></span><br><span class="line">				<span class="keyword">try</span> &#123;</span><br><span class="line">					BeanUtils.copyProperties(orderBean, bean);</span><br><span class="line">				&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">					e.printStackTrace();</span><br><span class="line">				&#125;</span><br><span class="line"></span><br><span class="line">				orderBeans.add(orderBean);</span><br><span class="line">			&#125; <span class="keyword">else</span> &#123;<span class="comment">// 产品表</span></span><br><span class="line"></span><br><span class="line">				<span class="keyword">try</span> &#123;</span><br><span class="line">					<span class="comment">// 拷贝传递过来的产品表到内存中</span></span><br><span class="line">					BeanUtils.copyProperties(pdBean, bean);</span><br><span class="line">				&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">					e.printStackTrace();</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 3 表的拼接</span></span><br><span class="line">		<span class="keyword">for</span>(TableBean bean:orderBeans)&#123;</span><br><span class="line"></span><br><span class="line">			bean.setPname (pdBean.getPname());</span><br><span class="line">			</span><br><span class="line">			<span class="comment">// 4 数据写出去</span></span><br><span class="line">			context.write(bean, NullWritable.get());</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>缺点：这种方式中，合并的操作是在Reduce阶段完成，Reduce端的处理压力太大，Map节点的运算负载则很低，资源利用率不高，且在Reduce阶段极易产生数据倾斜。</p><h3 id="map-join"><a href="#map-join" class="headerlink" title="map join"></a>map join</h3><p>使用场景：Map Join适用于一张表十分小、一张表很大的场景。</p><p>具体办法：采用DistributedCache</p><p>​ （1）在Mapper的setup阶段，将文件读取到缓存集合中。</p><p>​ （2）在驱动函数中加载缓存。</p><p>​ // 缓存普通文件到Task运行节点。</p><p>​ job.addCacheFile(new URI(“file://e:/cache/pd.txt”));</p><p>注：简单说就是把一个小表用map表示，用大表的连接属性直接映射出小表的属性</p><p><strong>对于驱动模块Driver来说</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// 6 加载缓存数据</span><br><span class="line">job.addCacheFile(new URI(&quot;file:///e:/input/inputcache/pd.txt&quot;));	</span><br><span class="line">// 7 Map端Join的逻辑不需要Reduce阶段，设置reduceTask数量为0</span><br><span class="line">job.setNumReduceTasks(0);</span><br></pre></td></tr></table></figure><p>对于mapper来说，先在map前读取缓存数据</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> test;</span><br><span class="line"><span class="keyword">import</span> java.io.BufferedReader;</span><br><span class="line"><span class="keyword">import</span> java.io.FileInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStreamReader;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang.StringUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DistributedCacheMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">	Map&lt;String, String&gt; pdMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 获取缓存的文件</span></span><br><span class="line">		URI[] cacheFiles = context.getCacheFiles();</span><br><span class="line">		String path = cacheFiles[<span class="number">0</span>].getPath().toString();</span><br><span class="line">		</span><br><span class="line">		BufferedReader reader = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(<span class="keyword">new</span> FileInputStream(path), <span class="string">&quot;UTF-8&quot;</span>));</span><br><span class="line">		</span><br><span class="line">		String line;</span><br><span class="line">		<span class="keyword">while</span>(StringUtils.isNotEmpty(line = reader.readLine()))&#123;</span><br><span class="line"></span><br><span class="line">			<span class="comment">// 2 切割</span></span><br><span class="line">			String[] fields = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">			</span><br><span class="line">			<span class="comment">// 3 缓存数据到集合</span></span><br><span class="line">			pdMap.put(fields[<span class="number">0</span>], fields[<span class="number">1</span>]);</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 4 关流</span></span><br><span class="line">		reader.close();</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	Text k = <span class="keyword">new</span> Text();</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 获取一行</span></span><br><span class="line">		String line = value.toString();</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 2 截取</span></span><br><span class="line">		String[] fields = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 3 获取产品id</span></span><br><span class="line">		String pId = fields[<span class="number">1</span>];</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 4 获取商品名称</span></span><br><span class="line">		String pdName = pdMap.get(pId);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 5 拼接</span></span><br><span class="line">		k.set(line + <span class="string">&quot;\t&quot;</span>+ pdName);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 6 写出</span></span><br><span class="line">		context.write(k, NullWritable.get());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="8-计数器应用与数据清洗（ETL）"><a href="#8-计数器应用与数据清洗（ETL）" class="headerlink" title="8 计数器应用与数据清洗（ETL）"></a>8 计数器应用与数据清洗（ETL）</h2><p>​ Hadoop为每个作业维护若干内置计数器，以描述多项指标。例如，某些计数器记录已处理的字节数和记录数，使用户可监控已处理的输入数据量和已产生的输出数据量。</p><p>使用方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">context.getCounter(<span class="string">&quot;map&quot;</span>,<span class="string">&quot;失败&quot;</span>).increment(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><h2 id="9-MapRedece开发总结"><a href="#9-MapRedece开发总结" class="headerlink" title="9 MapRedece开发总结"></a>9 MapRedece开发总结</h2><h4 id="1-输入数据接口：InputFormat"><a href="#1-输入数据接口：InputFormat" class="headerlink" title="1. 输入数据接口：InputFormat"></a>1. 输入数据接口：InputFormat</h4><p>（1）默认使用的实现类是：TextInputFormat</p><p>（2）TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回。</p><p>（3）KeyValueTextlnputFomat每一行均为一条记录，被分隔符分割为key，value。默认分隔符是tab（\t）。<br>（4）NlinelnputFormat按照指定的行数N来划分切片。</p><p>（5）CombineTextlnputFormat可以把多个小文件合并成一个切片处理，提高处理效率。</p><p>（6）用户还可以自定义ImputFormat。</p><h4 id="2-逻辑处理接口：Mapper"><a href="#2-逻辑处理接口：Mapper" class="headerlink" title="2.逻辑处理接口：Mapper"></a>2.逻辑处理接口：Mapper</h4><p>用户根据业务需求实现其中三个方法：map）setup）cleanup）</p><h4 id="3-Partitioner分区"><a href="#3-Partitioner分区" class="headerlink" title="3.Partitioner分区"></a>3.Partitioner分区</h4><p>（1）有默认实现HashPartitioner，逻辑是根据key的哈希值和numReduces来返回一个分区号；key hashCode（）&amp;Integer.MAXVALUE%<br>numReduces</p><p>（2）如果业务上有特别的需求，可以自定义分区。</p><h4 id="4-Comparable排序"><a href="#4-Comparable排序" class="headerlink" title="4.Comparable排序"></a>4.Comparable排序</h4><p>（1）当我们用自定义的对象作为key来输出时，就必须要实现WritableComparable接口，重写其中的compareTo0方法。</p><p>（2）部分排序：对最终输出的每一个文件进行内部排序。</p><p>（3）全排序：对所有数据进行排序，通常只有一个Reduce。</p><p>（4）二次排序：排序的条件有两个。</p><h4 id="5-Combiner合并"><a href="#5-Combiner合并" class="headerlink" title="5.Combiner合并"></a>5.Combiner合并</h4><p>Combiner合并可以提高程序执行效率，减少I0传输。但是使用时必须不能影响原有的业务处理结果。</p><h4 id="6-Reduce端分组：GroupingComparator"><a href="#6-Reduce端分组：GroupingComparator" class="headerlink" title="6.Reduce端分组：GroupingComparator"></a>6.Reduce端分组：GroupingComparator</h4><p>在Reduce端对key进行分组。应用于：在接收的key为bean对象时，想让一个或几个字段相同（全部字段比较不相同）的key进入到同一个reduce方法时，可以采用分组排序。</p><h4 id="7-逻辑处理接口："><a href="#7-逻辑处理接口：" class="headerlink" title="7.逻辑处理接口："></a>7.逻辑处理接口：</h4><p>Reducer用户根据业务需求实现其中三个方法：reduce();setup();cleanup()</p><h4 id="8-输出数据接口：OutputFormat"><a href="#8-输出数据接口：OutputFormat" class="headerlink" title="8.输出数据接口：OutputFormat"></a>8.输出数据接口：OutputFormat</h4><p>（1）默认实现类是TextOutputFormat，功能逻辑是：将每一个KV对，向目标文本文件输出一行。</p><p>（2）将SequenceFileOutputFormat输出作为后续MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑，很容易被压缩。</p><p>（3）用户还可以自定义OutputFormat。</p></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者: </i></span><span class="reprint-info"><a href="../../../../about" rel="external nofollow noreferrer">康栋</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接: </i></span><span class="reprint-info"><a href="http://kiedeng.github.io">http://kiedeng.github.io</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明: </i></span><span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="../../../../about" target="_blank">康栋</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})})</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="../../../../tags/%E9%BB%98%E8%AE%A4/"><span class="chip bg-color">默认</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" type="text/css" href="../../../../libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="../../../../libs/share/js/social-share.min.js"></script></div></div></div><style>#reward{margin:40px 0;text-align:center}#reward .reward-link{font-size:1.4rem;line-height:38px}#reward .btn-floating:hover{box-shadow:0 6px 12px rgba(0,0,0,.2),0 5px 15px rgba(0,0,0,.2)}#rewardModal{width:320px;height:350px}#rewardModal .reward-title{margin:15px auto;padding-bottom:5px}#rewardModal .modal-content{padding:10px}#rewardModal .close{position:absolute;right:15px;top:15px;color:rgba(0,0,0,.5);font-size:1.3rem;line-height:20px;cursor:pointer}#rewardModal .close:hover{color:#ef5350;transform:scale(1.3);-moz-transform:scale(1.3);-webkit-transform:scale(1.3);-o-transform:scale(1.3)}#rewardModal .reward-tabs{margin:0 auto;width:210px}.reward-tabs .tabs{height:38px;margin:10px auto;padding-left:0}.reward-content ul{padding-left:0!important}.reward-tabs .tabs .tab{height:38px;line-height:38px}.reward-tabs .tab a{color:#fff;background-color:#ccc}.reward-tabs .tab a:hover{background-color:#ccc;color:#fff}.reward-tabs .wechat-tab .active{color:#fff!important;background-color:#22ab38!important}.reward-tabs .alipay-tab .active{color:#fff!important;background-color:#019fe8!important}.reward-tabs .reward-img{width:210px;height:210px}</style><div id="reward"><a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a><div id="rewardModal" class="modal"><div class="modal-content"><a class="close modal-close"><i class="fas fa-times"></i></a><h4 class="reward-title">你的赏识是我前进的动力</h4><div class="reward-content"><div class="reward-tabs"><ul class="tabs row"><li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li><li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li></ul><div id="alipay"><img src="../../../../medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码"></div><div id="wechat"><img src="../../../../medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码"></div></div></div></div></div></div><script>$(function(){$(".tabs").tabs()})</script></div></div><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i>&nbsp;上一篇</div><div class="card"><a href="../../../03/01/%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9/"><div class="card-image"><img src="../../../../medias/featureimages/20.jpg" class="responsive-img" alt="数据压缩"> <span class="card-title">数据压缩</span></div></a><div class="card-content article-content"><div class="summary block-with-text"></div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-03-01 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="../../../../categories/Hadoop/" class="post-category">Hadoop</a></span></div></div><div class="card-action article-tags"><a href="../../../../tags/%E9%BB%98%E8%AE%A4/"><span class="chip bg-color">默认</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="../bean%E5%AF%B9%E8%B1%A1%E6%B2%A1%E6%9C%89%E5%BA%8F%E5%88%97%E5%8C%96/"><div class="card-image"><img src="../../../../medias/featureimages/22.jpg" class="responsive-img" alt="bean对象没有序列化"> <span class="card-title">bean对象没有序列化</span></div></a><div class="card-content article-content"><div class="summary block-with-text"></div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i>2020-02-29 </span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="../../../../categories/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/" class="post-category">常见错误</a></span></div></div><div class="card-action article-tags"><a href="../../../../tags/%E9%BB%98%E8%AE%A4/"><span class="chip bg-color">默认</span></a></div></div></div></div></article></div><script type="text/javascript" src="../../../../libs/codeBlock/codeBlockFuction.js"></script><script type="text/javascript" src="../../../../libs/codeBlock/codeLang.js"></script><script type="text/javascript" src="../../../../libs/codeBlock/codeCopy.js"></script><script type="text/javascript" src="../../../../libs/codeBlock/codeShrink.js"></script></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget card" style="background-color:#fff"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="../../../../libs/tocbot/tocbot.min.js"></script><script>$(function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=0,e="toc-heading-";$("#toc-content a").each(function(){$(this).attr("href","#"+e+ ++t)}),t=0,$("#articleContent").children("h2, h3, h4").each(function(){$(this).attr("id",e+ ++t)});let n=parseInt(.4*$(window).height()-64),o=$(".toc-widget");$(window).scroll(function(){$(window).scrollTop()>n?o.addClass("toc-fixed"):o.removeClass("toc-fixed")});const i="expanded";let c=$("#toc-aside"),l=$("#main-content");$("#floating-toc-btn .btn-floating").click(function(){c.hasClass(i)?(c.removeClass(i).hide(),l.removeClass("l9")):(c.addClass(i).show(),l.addClass("l9")),function(t,e){let n=$("#"+t);if(0!==n.length){let t=n.width();450<=t?t+=21:350<=t&&t<450?t+=18:300<=t&&t<350?t+=16:t+=14,$("#"+e).width(t)}}("artDetail","prenext-posts")})})</script></main><footer class="page-footer bg-color"><link rel="stylesheet" href="../../../../libs/aplayer/APlayer.min.css"><style>.aplayer .aplayer-lrc p{display:none;font-size:12px;font-weight:700;line-height:16px!important}.aplayer .aplayer-lrc p.aplayer-lrc-current{display:none;font-size:15px;color:#42b983}.aplayer.aplayer-fixed.aplayer-narrow .aplayer-body{left:-66px!important}.aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover{left:0!important}</style><div><div class="row"><meting-js class="col l8 offset-l2 m10 offset-m1 s12" server="netease" type="playlist" id="503838841" fixed="true" autoplay theme="#42b983" loop order="random" preload="auto" volume="0.7" list-folded="true"></meting-js></div></div><script src="../../../../libs/aplayer/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script><div class="container row center-align" style="margin-bottom:0!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2019-2021</span> <span id="year">2019</span> <a href="../../../../about" target="_blank">康栋</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br><span id="busuanzi_container_site_pv">|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span>&nbsp;次 </span><span id="busuanzi_container_site_uv">|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span>&nbsp;人</span><br><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/blinkfox" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i> </a><a href="mailto:1181062873@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i> </a><a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=1181062873" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 1181062873" data-position="top" data-delay="50"><i class="fab fa-qq"></i> </a><a href="../../../../atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50"><i class="fas fa-rss"></i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script type="text/javascript">$(function(){!function(t,r,s){"use strict";$.ajax({url:t,dataType:"xml",success:function(t){var e=$("entry",t).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),t=document.getElementById(r),n=document.getElementById(s);t.addEventListener("input",function(){var o='<ul class="search-result-list">',h=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length<=0||(e.forEach(function(t){var n,e,r,s=!0,i=t.title.trim().toLowerCase(),l=t.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),a=0===(a=t.url).indexOf("/")?t.url:"/"+a,c=-1,u=-1;""!==i&&""!==l&&h.forEach(function(t,e){n=i.indexOf(t),c=l.indexOf(t),n<0&&c<0?s=!1:(c<0&&(c=0),0===e&&(u=c))}),s&&(o+="<li><a href='"+a+"' class='search-result-title'>"+i+"</a>",e=t.content.trim().replace(/<[^>]+>/g,""),0<=u&&(a=u+80,(a=0===(t=(t=u-20)<0?0:t)?100:a)>e.length&&(a=e.length),r=e.substr(t,a),h.forEach(function(t){var e=new RegExp(t,"gi");r=r.replace(e,'<em class="search-keyword">'+t+"</em>")}),o+='<p class="search-result">'+r+"...</p>"),o+="</li>")}),n.innerHTML=o+="</ul>")})}})}("../../../../search.xml","searchInput","searchResult")})</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-arrow-up"></i></a></div><script src="../../../../libs/materialize/materialize.min.js"></script><script src="../../../../libs/masonry/masonry.pkgd.min.js"></script><script src="../../../../libs/aos/aos.js"></script><script src="../../../../libs/scrollprogress/scrollProgress.min.js"></script><script src="../../../../libs/lightGallery/js/lightgallery-all.min.js"></script><script src="../../../../js/matery.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";e=document.getElementsByTagName("script")[0];e.parentNode.insertBefore(t,e)}()</script><script src="../../../../libs/others/clicklove.js" async></script><script async src="../../../../libs/others/busuanzi.pure.mini.js"></script><script src="../../../../libs/instantpage/instantpage.js" type="module"></script></body></html>