<!-- build time:Fri Mar 05 2021 21:11:17 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html><head><meta charset="utf-8"><title>MapReduce框架原理 | 学习印记</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#3F51B5"><meta name="keywords" content="默认"><meta name="description" content="总结MapReduce框架原理1 InputFotmat数据输入待写2 MapReduce工作流程待写3 Shuffle机制3.1 Shuffle机制介绍Map方法之后，Reduce方法之前的数据称之为Shuffle3.2 主要功能Partition分区，WritableComparable排序，Combiner合并，GroupingComparator分组（辅助排序）4 MapTask工作机制​"><meta property="og:type" content="article"><meta property="og:title" content="MapReduce框架原理"><meta property="og:url" content="http://kiedeng.github.io/2020/02/29/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86/index.html"><meta property="og:site_name" content="学习印记"><meta property="og:description" content="总结MapReduce框架原理1 InputFotmat数据输入待写2 MapReduce工作流程待写3 Shuffle机制3.1 Shuffle机制介绍Map方法之后，Reduce方法之前的数据称之为Shuffle3.2 主要功能Partition分区，WritableComparable排序，Combiner合并，GroupingComparator分组（辅助排序）4 MapTask工作机制​"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2020-02-29T09:08:56.000Z"><meta property="article:modified_time" content="2020-02-29T13:57:55.988Z"><meta property="article:author" content="康栋"><meta property="article:tag" content="默认"><meta name="twitter:card" content="summary"><link rel="alternate" type="application/atom+xml" title="学习印记" href="../../../../atom.xml"><link rel="shortcut icon" href="../../../../favicon.ico"><link rel="stylesheet" href="../../../../css/style.css?v=1.7.0"><script>window.lazyScripts=[]</script><meta name="generator" content="Hexo 4.2.0"></head><body><div id="loading" class="active"></div><aside id="menu"><div class="inner flex-row-vertical"><a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off"><i class="icon icon-lg icon-close"></i></a><div class="brand-wrap" style="background-image:url(../../../../img/brand.jpg)"><div class="brand"><a href="../../../../index.html" class="avatar waves-effect waves-circle waves-light"><img src="../../../../img/kieden.jpg"></a><hgroup class="introduce"><h5 class="nickname">康栋</h5><a href="mailto:572245955@qq.com" title="572245955@qq.com" class="mail">572245955@qq.com</a></hgroup></div></div><div class="scroll-wrap flex-col"><ul class="nav"><li class="waves-block waves-effect"><a href="../../../../index.html"><i class="icon icon-lg icon-home"></i> 主页</a></li><li class="waves-block waves-effect"><a href="../../../../archives"><i class="icon icon-lg icon-archives"></i> 归档</a></li><li class="waves-block waves-effect"><a href="../../../../tags"><i class="icon icon-lg icon-tags"></i> 标签</a></li><li class="waves-block waves-effect"><a href="../../../../categories"><i class="icon icon-lg icon-th-list"></i> 分类</a></li><li class="waves-block waves-effect"><a href="../../../../https:/github.com/kiedeng" target="_blank"><i class="icon icon-lg icon-github"></i> Github</a></li><li class="waves-block waves-effect"><a href="../../../../about"><i class="icon icon-lg icon-link"></i> 个人介绍</a></li></ul></div></div></aside><main id="main"><header class="top-header" id="header"><div class="flex-row"><a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle"><i class="icon icon-lg icon-navicon"></i></a><div class="flex-col header-title ellipsis">MapReduce框架原理</div><div class="search-wrap" id="search-wrap"><a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back"><i class="icon icon-lg icon-chevron-left"></i> </a><input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字"> <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search"><i class="icon icon-lg icon-search"></i></a></div><a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare"><i class="icon icon-lg icon-share-alt"></i></a></div></header><header class="content-header post-header"><div class="container fade-scale"><h1 class="title">MapReduce框架原理</h1><h5 class="subtitle"><time datetime="2020-02-29T09:08:56.000Z" itemprop="datePublished" class="page-time">2020-02-29</time><ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="../../../../categories/Hadoop/">Hadoop</a></li></ul></h5></div></header><div class="container body-wrap"><aside class="post-widget"><nav class="post-toc-wrap" id="post-toc"><h4>TOC</h4><ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#1-InputFotmat数据输入"><span class="post-toc-number">1.</span> <span class="post-toc-text">1 InputFotmat数据输入</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-MapReduce工作流程"><span class="post-toc-number">2.</span> <span class="post-toc-text">2 MapReduce工作流程</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#3-Shuffle机制"><span class="post-toc-number">3.</span> <span class="post-toc-text">3 Shuffle机制</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#3-1-Shuffle机制介绍"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">3.1 Shuffle机制介绍</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#4-MapTask工作机制"><span class="post-toc-number">4.</span> <span class="post-toc-text">4 MapTask工作机制</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#5-RedeceTask工作机制"><span class="post-toc-number">5.</span> <span class="post-toc-text">5 RedeceTask工作机制</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#6-OutputFormat数据输出"><span class="post-toc-number">6.</span> <span class="post-toc-text">6 OutputFormat数据输出</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#6-1-自定义OutputFomat步骤"><span class="post-toc-number">6.1.</span> <span class="post-toc-text">6.1 自定义OutputFomat步骤</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#7-Join多种应用"><span class="post-toc-number">7.</span> <span class="post-toc-text">7 Join多种应用</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#7-1-工作原理"><span class="post-toc-number">7.1.</span> <span class="post-toc-text">7.1 工作原理</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#7-2-Reduce-join"><span class="post-toc-number">7.2.</span> <span class="post-toc-text">7.2 Reduce join</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#map-join"><span class="post-toc-number">7.3.</span> <span class="post-toc-text">map join</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#8-计数器应用与数据清洗（ETL）"><span class="post-toc-number">8.</span> <span class="post-toc-text">8 计数器应用与数据清洗（ETL）</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#9-MapRedece开发总结"><span class="post-toc-number">9.</span> <span class="post-toc-text">9 MapRedece开发总结</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#1-输入数据接口：InputFormat"><span class="post-toc-number">9.0.1.</span> <span class="post-toc-text">1. 输入数据接口：InputFormat</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2-逻辑处理接口：Mapper"><span class="post-toc-number">9.0.2.</span> <span class="post-toc-text">2.逻辑处理接口：Mapper</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#3-Partitioner分区"><span class="post-toc-number">9.0.3.</span> <span class="post-toc-text">3.Partitioner分区</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#4-Comparable排序"><span class="post-toc-number">9.0.4.</span> <span class="post-toc-text">4.Comparable排序</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#5-Combiner合并"><span class="post-toc-number">9.0.5.</span> <span class="post-toc-text">5.Combiner合并</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#6-Reduce端分组：GroupingComparator"><span class="post-toc-number">9.0.6.</span> <span class="post-toc-text">6.Reduce端分组：GroupingComparator</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#7-逻辑处理接口："><span class="post-toc-number">9.0.7.</span> <span class="post-toc-text">7.逻辑处理接口：</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#8-输出数据接口：OutputFormat"><span class="post-toc-number">9.0.8.</span> <span class="post-toc-text">8.输出数据接口：OutputFormat</span></a></li></ol></li></ol></nav></aside><article id="post-MapReduce框架原理" class="post-article article-type-post fade" itemprop="blogPost"><div class="post-card"><h1 class="post-card-title">MapReduce框架原理</h1><div class="post-meta"><time class="post-time" title="2020-02-29 17:08:56" datetime="2020-02-29T09:08:56.000Z" itemprop="datePublished">2020-02-29</time><ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="../../../../categories/Hadoop/">Hadoop</a></li></ul><span id="busuanzi_container_page_pv" title="文章总阅读量" style="display:none"><i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span></span></div><div class="post-content" id="post-content" itemprop="postContent"><ul><li>总结MapReduce框架原理</li></ul><h2 id="1-InputFotmat数据输入"><a href="#1-InputFotmat数据输入" class="headerlink" title="1 InputFotmat数据输入"></a>1 InputFotmat数据输入</h2><p>待写</p><h2 id="2-MapReduce工作流程"><a href="#2-MapReduce工作流程" class="headerlink" title="2 MapReduce工作流程"></a>2 MapReduce工作流程</h2><p>待写</p><h2 id="3-Shuffle机制"><a href="#3-Shuffle机制" class="headerlink" title="3 Shuffle机制"></a>3 Shuffle机制</h2><h3 id="3-1-Shuffle机制介绍"><a href="#3-1-Shuffle机制介绍" class="headerlink" title="3.1 Shuffle机制介绍"></a>3.1 Shuffle机制介绍</h3><p>Map方法之后，Reduce方法之前的数据称之为Shuffle</p><p>3.2 主要功能</p><p>Partition分区，WritableComparable排序，Combiner合并，GroupingComparator分组（辅助排序）</p><h2 id="4-MapTask工作机制"><a href="#4-MapTask工作机制" class="headerlink" title="4 MapTask工作机制"></a>4 MapTask工作机制</h2><p>​ （1）Read阶段：MapTask通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value。</p><p>​ （2）Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value。</p><p>​ （3）Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner），并写入一个环形内存缓冲区中。</p><p>​ （4）Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。</p><p>​ 溢写阶段详情：</p><p>​ 步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号Partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。</p><p>​ 步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。</p><p>​ 步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中。</p><p>​ （5）Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。</p><p>​ 当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。</p><p>​ 在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor（默认10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。</p><p>​ 让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。</p><h2 id="5-RedeceTask工作机制"><a href="#5-RedeceTask工作机制" class="headerlink" title="5 RedeceTask工作机制"></a>5 RedeceTask工作机制</h2><p>​ （1）Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</p><p>​ （2）Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。</p><p>​ （3）Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。</p><p>​ （4）Reduce阶段：reduce()函数将计算结果写到HDFS上。</p><h2 id="6-OutputFormat数据输出"><a href="#6-OutputFormat数据输出" class="headerlink" title="6 OutputFormat数据输出"></a>6 OutputFormat数据输出</h2><p>​ OutputFormat是MapReduce输出的基类，所有实现MapReduce输出都实现了OutputFormat接口。</p><p>下面我们介绍几种常见的OutputFormat实现类。</p><p>​ 1.文本输出TextOutputFormat默认的输出格式是TextOutputFormat，它把每条记录写为文本行。它的键和值可以是任意类型，因为TextOutputFormat调用toString0方法把它们转换为字符串。</p><p>​ 2.SequenceFileOutputFormat将SequenceFileOutputFormat输出作为后续MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑，很容易被压缩。</p><p>​ 3.自定义OutputFormat根据用户需求，自定义实现输出。</p><h3 id="6-1-自定义OutputFomat步骤"><a href="#6-1-自定义OutputFomat步骤" class="headerlink" title="6.1 自定义OutputFomat步骤"></a>6.1 自定义OutputFomat步骤</h3><p>​ （1）自定义一个类继承FileOutputFormat。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.mapreduce.outputformat;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">import org.apache.hadoop.io.NullWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line">import org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line">public class FilterOutputFormat extends FileOutputFormat&lt;Text, NullWritable&gt;&#123;</span><br><span class="line"></span><br><span class="line">	@Override</span><br><span class="line">	public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter(TaskAttemptContext job)			throws IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; 创建一个RecordWriter</span><br><span class="line">		return new FilterRecordWriter(job);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​ （2）改写RecordWriter，具体改写输出数据的方法write。</p><p>​ <strong>RecordWriter格式：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.mapreduce.outputformat;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">import org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line">import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.NullWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line">import org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"></span><br><span class="line">public class FilterRecordWriter extends RecordWriter&lt;Text, NullWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">	FSDataOutputStream atguiguOut &#x3D; null;</span><br><span class="line">	FSDataOutputStream otherOut &#x3D; null;</span><br><span class="line"></span><br><span class="line">	public FilterRecordWriter(TaskAttemptContext job) &#123;</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; 1 获取文件系统</span><br><span class="line">		FileSystem fs;</span><br><span class="line"></span><br><span class="line">		try &#123;</span><br><span class="line">			fs &#x3D; FileSystem.get(job.getConfiguration());</span><br><span class="line"></span><br><span class="line">			&#x2F;&#x2F; 2 创建输出文件路径</span><br><span class="line">			Path atguiguPath &#x3D; new Path(&quot;e:&#x2F;atguigu.log&quot;);</span><br><span class="line">			Path otherPath &#x3D; new Path(&quot;e:&#x2F;other.log&quot;);</span><br><span class="line"></span><br><span class="line">			&#x2F;&#x2F; 3 创建输出流</span><br><span class="line">			atguiguOut &#x3D; fs.create(atguiguPath);</span><br><span class="line">			otherOut &#x3D; fs.create(otherPath);</span><br><span class="line">		&#125; catch (IOException e) &#123;</span><br><span class="line">			e.printStackTrace();</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	@Override</span><br><span class="line">	public void write(Text key, NullWritable value) throws IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; 判断是否包含“atguigu”输出到不同文件</span><br><span class="line">		if (key.toString().contains(&quot;atguigu&quot;)) &#123;</span><br><span class="line">			atguiguOut.write(key.toString().getBytes());</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">			otherOut.write(key.toString().getBytes());</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	@Override</span><br><span class="line">	public void close(TaskAttemptContext context) throws IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; 关闭资源</span><br><span class="line">IOUtils.closeStream(atguiguOut);</span><br><span class="line">		IOUtils.closeStream(otherOut);	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意：记得将自定义输出格式设置到job中</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 要将自定义的输出格式组件设置到job中</span></span><br><span class="line">job.setOutputFormatClass(FilterOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure><h2 id="7-Join多种应用"><a href="#7-Join多种应用" class="headerlink" title="7 Join多种应用"></a>7 Join多种应用</h2><h3 id="7-1-工作原理"><a href="#7-1-工作原理" class="headerlink" title="7.1 工作原理"></a>7.1 工作原理</h3><p>​ Map端的主要工作：为来自不同表或文件的key/value对，打标签以区别不同来源的记录。然后用连接字段作为key，其余部分和新加的标志作为value，最后进行输出。</p><p>​ Reduce端的主要工作：在Reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录（在Map阶段已经打标志分开，最后进行合并就ok了。</p><h3 id="7-2-Reduce-join"><a href="#7-2-Reduce-join" class="headerlink" title="7.2 Reduce join"></a>7.2 Reduce join</h3><p>​ 在Mapper阶段使得连接属性为key，其余属性为value（自定义bean对象，记得序列化），再用一个标记属性标记来自于哪个文件</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce.table;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">TableBean</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">String name;</span><br><span class="line">	TableBean bean = <span class="keyword">new</span> TableBean();</span><br><span class="line">	Text k = <span class="keyword">new</span> Text();</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 获取输入文件切片</span></span><br><span class="line">		FileSplit split = (FileSplit) context.getInputSplit();</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 2 获取输入文件名称</span></span><br><span class="line">		name = split.getPath().getName();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 1 获取输入数据</span></span><br><span class="line">		String line = value.toString();</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 2 不同文件分别处理</span></span><br><span class="line">		<span class="keyword">if</span> (name.startsWith(<span class="string">"order"</span>)) &#123;<span class="comment">// 订单表处理</span></span><br><span class="line"></span><br><span class="line">			<span class="comment">// 2.1 切割</span></span><br><span class="line">			String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line">			</span><br><span class="line">			<span class="comment">// 2.2 封装bean对象</span></span><br><span class="line">			bean.setOrder_id(fields[<span class="number">0</span>]);</span><br><span class="line">			bean.setP_id(fields[<span class="number">1</span>]);</span><br><span class="line">			bean.setAmount(Integer.parseInt(fields[<span class="number">2</span>]));</span><br><span class="line">			bean.setPname(<span class="string">""</span>);</span><br><span class="line">			bean.setFlag(<span class="string">"order"</span>);</span><br><span class="line">			</span><br><span class="line">			k.set(fields[<span class="number">1</span>]);</span><br><span class="line">		&#125;<span class="keyword">else</span> &#123;<span class="comment">// 产品表处理</span></span><br><span class="line"></span><br><span class="line">			<span class="comment">// 2.3 切割</span></span><br><span class="line">			String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line">			</span><br><span class="line">			<span class="comment">// 2.4 封装bean对象</span></span><br><span class="line">			bean.setP_id(fields[<span class="number">0</span>]);</span><br><span class="line">			bean.setPname(fields[<span class="number">1</span>]);</span><br><span class="line">			bean.setFlag(<span class="string">"pd"</span>);</span><br><span class="line">			bean.setAmount(<span class="number">0</span>);</span><br><span class="line">			bean.setOrder_id(<span class="string">""</span>);</span><br><span class="line">			</span><br><span class="line">			k.set(fields[<span class="number">0</span>]);</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 3 写出</span></span><br><span class="line">		context.write(k, bean);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​ 在Reduce阶段，输出连接成功的bean对象</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce.table;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.beanutils.BeanUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">TableBean</span>, <span class="title">TableBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;TableBean&gt; values, Context context)</span>	<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1准备存储订单的集合</span></span><br><span class="line">		ArrayList&lt;TableBean&gt; orderBeans = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">		</span><br><span class="line"><span class="comment">// 2 准备bean对象</span></span><br><span class="line">		TableBean pdBean = <span class="keyword">new</span> TableBean();</span><br><span class="line"></span><br><span class="line">		<span class="keyword">for</span> (TableBean bean : values) &#123;</span><br><span class="line"></span><br><span class="line">			<span class="keyword">if</span> (<span class="string">"order"</span>.equals(bean.getFlag())) &#123;<span class="comment">// 订单表</span></span><br><span class="line"></span><br><span class="line">				<span class="comment">// 拷贝传递过来的每条订单数据到集合中</span></span><br><span class="line">				TableBean orderBean = <span class="keyword">new</span> TableBean();</span><br><span class="line"></span><br><span class="line">				<span class="keyword">try</span> &#123;</span><br><span class="line">					BeanUtils.copyProperties(orderBean, bean);</span><br><span class="line">				&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">					e.printStackTrace();</span><br><span class="line">				&#125;</span><br><span class="line"></span><br><span class="line">				orderBeans.add(orderBean);</span><br><span class="line">			&#125; <span class="keyword">else</span> &#123;<span class="comment">// 产品表</span></span><br><span class="line"></span><br><span class="line">				<span class="keyword">try</span> &#123;</span><br><span class="line">					<span class="comment">// 拷贝传递过来的产品表到内存中</span></span><br><span class="line">					BeanUtils.copyProperties(pdBean, bean);</span><br><span class="line">				&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">					e.printStackTrace();</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 3 表的拼接</span></span><br><span class="line">		<span class="keyword">for</span>(TableBean bean:orderBeans)&#123;</span><br><span class="line"></span><br><span class="line">			bean.setPname (pdBean.getPname());</span><br><span class="line">			</span><br><span class="line">			<span class="comment">// 4 数据写出去</span></span><br><span class="line">			context.write(bean, NullWritable.get());</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>缺点：这种方式中，合并的操作是在Reduce阶段完成，Reduce端的处理压力太大，Map节点的运算负载则很低，资源利用率不高，且在Reduce阶段极易产生数据倾斜。</p><h3 id="map-join"><a href="#map-join" class="headerlink" title="map join"></a>map join</h3><p>使用场景：Map Join适用于一张表十分小、一张表很大的场景。</p><p>具体办法：采用DistributedCache</p><p>​ （1）在Mapper的setup阶段，将文件读取到缓存集合中。</p><p>​ （2）在驱动函数中加载缓存。</p><p>​ // 缓存普通文件到Task运行节点。</p><p>​ job.addCacheFile(new URI(“file://e:/cache/pd.txt”));</p><p>注：简单说就是把一个小表用map表示，用大表的连接属性直接映射出小表的属性</p><p><strong>对于驱动模块Driver来说</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 6 加载缓存数据</span><br><span class="line">job.addCacheFile(new URI(&quot;file:&#x2F;&#x2F;&#x2F;e:&#x2F;input&#x2F;inputcache&#x2F;pd.txt&quot;));	</span><br><span class="line">&#x2F;&#x2F; 7 Map端Join的逻辑不需要Reduce阶段，设置reduceTask数量为0</span><br><span class="line">job.setNumReduceTasks(0);</span><br></pre></td></tr></table></figure><p>对于mapper来说，先在map前读取缓存数据</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> test;</span><br><span class="line"><span class="keyword">import</span> java.io.BufferedReader;</span><br><span class="line"><span class="keyword">import</span> java.io.FileInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStreamReader;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang.StringUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DistributedCacheMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">	Map&lt;String, String&gt; pdMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 获取缓存的文件</span></span><br><span class="line">		URI[] cacheFiles = context.getCacheFiles();</span><br><span class="line">		String path = cacheFiles[<span class="number">0</span>].getPath().toString();</span><br><span class="line">		</span><br><span class="line">		BufferedReader reader = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(<span class="keyword">new</span> FileInputStream(path), <span class="string">"UTF-8"</span>));</span><br><span class="line">		</span><br><span class="line">		String line;</span><br><span class="line">		<span class="keyword">while</span>(StringUtils.isNotEmpty(line = reader.readLine()))&#123;</span><br><span class="line"></span><br><span class="line">			<span class="comment">// 2 切割</span></span><br><span class="line">			String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line">			</span><br><span class="line">			<span class="comment">// 3 缓存数据到集合</span></span><br><span class="line">			pdMap.put(fields[<span class="number">0</span>], fields[<span class="number">1</span>]);</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 4 关流</span></span><br><span class="line">		reader.close();</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	Text k = <span class="keyword">new</span> Text();</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 获取一行</span></span><br><span class="line">		String line = value.toString();</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 2 截取</span></span><br><span class="line">		String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 3 获取产品id</span></span><br><span class="line">		String pId = fields[<span class="number">1</span>];</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 4 获取商品名称</span></span><br><span class="line">		String pdName = pdMap.get(pId);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 5 拼接</span></span><br><span class="line">		k.set(line + <span class="string">"\t"</span>+ pdName);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 6 写出</span></span><br><span class="line">		context.write(k, NullWritable.get());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="8-计数器应用与数据清洗（ETL）"><a href="#8-计数器应用与数据清洗（ETL）" class="headerlink" title="8 计数器应用与数据清洗（ETL）"></a>8 计数器应用与数据清洗（ETL）</h2><p>​ Hadoop为每个作业维护若干内置计数器，以描述多项指标。例如，某些计数器记录已处理的字节数和记录数，使用户可监控已处理的输入数据量和已产生的输出数据量。</p><p>使用方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">context.getCounter(<span class="string">"map"</span>,<span class="string">"失败"</span>).increment(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><h2 id="9-MapRedece开发总结"><a href="#9-MapRedece开发总结" class="headerlink" title="9 MapRedece开发总结"></a>9 MapRedece开发总结</h2><h4 id="1-输入数据接口：InputFormat"><a href="#1-输入数据接口：InputFormat" class="headerlink" title="1. 输入数据接口：InputFormat"></a>1. 输入数据接口：InputFormat</h4><p>（1）默认使用的实现类是：TextInputFormat</p><p>（2）TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回。</p><p>（3）KeyValueTextlnputFomat每一行均为一条记录，被分隔符分割为key，value。默认分隔符是tab（\t）。<br>（4）NlinelnputFormat按照指定的行数N来划分切片。</p><p>（5）CombineTextlnputFormat可以把多个小文件合并成一个切片处理，提高处理效率。</p><p>（6）用户还可以自定义ImputFormat。</p><h4 id="2-逻辑处理接口：Mapper"><a href="#2-逻辑处理接口：Mapper" class="headerlink" title="2.逻辑处理接口：Mapper"></a>2.逻辑处理接口：Mapper</h4><p>用户根据业务需求实现其中三个方法：map）setup）cleanup）</p><h4 id="3-Partitioner分区"><a href="#3-Partitioner分区" class="headerlink" title="3.Partitioner分区"></a>3.Partitioner分区</h4><p>（1）有默认实现HashPartitioner，逻辑是根据key的哈希值和numReduces来返回一个分区号；key hashCode（）&amp;Integer.MAXVALUE%<br>numReduces</p><p>（2）如果业务上有特别的需求，可以自定义分区。</p><h4 id="4-Comparable排序"><a href="#4-Comparable排序" class="headerlink" title="4.Comparable排序"></a>4.Comparable排序</h4><p>（1）当我们用自定义的对象作为key来输出时，就必须要实现WritableComparable接口，重写其中的compareTo0方法。</p><p>（2）部分排序：对最终输出的每一个文件进行内部排序。</p><p>（3）全排序：对所有数据进行排序，通常只有一个Reduce。</p><p>（4）二次排序：排序的条件有两个。</p><h4 id="5-Combiner合并"><a href="#5-Combiner合并" class="headerlink" title="5.Combiner合并"></a>5.Combiner合并</h4><p>Combiner合并可以提高程序执行效率，减少I0传输。但是使用时必须不能影响原有的业务处理结果。</p><h4 id="6-Reduce端分组：GroupingComparator"><a href="#6-Reduce端分组：GroupingComparator" class="headerlink" title="6.Reduce端分组：GroupingComparator"></a>6.Reduce端分组：GroupingComparator</h4><p>在Reduce端对key进行分组。应用于：在接收的key为bean对象时，想让一个或几个字段相同（全部字段比较不相同）的key进入到同一个reduce方法时，可以采用分组排序。</p><h4 id="7-逻辑处理接口："><a href="#7-逻辑处理接口：" class="headerlink" title="7.逻辑处理接口："></a>7.逻辑处理接口：</h4><p>Reducer用户根据业务需求实现其中三个方法：reduce();setup();cleanup()</p><h4 id="8-输出数据接口：OutputFormat"><a href="#8-输出数据接口：OutputFormat" class="headerlink" title="8.输出数据接口：OutputFormat"></a>8.输出数据接口：OutputFormat</h4><p>（1）默认实现类是TextOutputFormat，功能逻辑是：将每一个KV对，向目标文本文件输出一行。</p><p>（2）将SequenceFileOutputFormat输出作为后续MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑，很容易被压缩。</p><p>（3）用户还可以自定义OutputFormat。</p></div><blockquote class="post-copyright"><div class="content"><span class="post-time">最后更新时间：<time datetime="2020-02-29T13:57:55.988Z" itemprop="dateUpdated">2020-02-29 21:57:55</time></span><br>有任何问题可以qq联系我</div><footer><a href="http://kiedeng.github.io"><img src="../../../../img/kieden.jpg" alt="康栋"> 康栋</a></footer></blockquote><div class="page-reward"><a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a></div><div class="post-footer"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="../../../../tags/%E9%BB%98%E8%AE%A4/" rel="tag">默认</a></li></ul><div class="page-share-wrap"><div class="page-share" id="pageShare"><ul class="reset share-icons"><li><a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://kiedeng.github.io/2020/02/29/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86/&title=《MapReduce框架原理》 — 学习印记&pic=http://kiedeng.github.io/img/kieden.jpg" data-title="微博"><i class="icon icon-weibo"></i></a></li><li><a class="weixin share-sns wxFab" href="javascript:;" data-title="微信"><i class="icon icon-weixin"></i></a></li><li><a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://kiedeng.github.io/2020/02/29/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86/&title=《MapReduce框架原理》 — 学习印记&source=" data-title=" QQ"><i class="icon icon-qq"></i></a></li><li><a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://kiedeng.github.io/2020/02/29/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86/" data-title=" Facebook"><i class="icon icon-facebook"></i></a></li><li><a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《MapReduce框架原理》 — 学习印记&url=http://kiedeng.github.io/2020/02/29/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86/&via=http://kiedeng.github.io" data-title=" Twitter"><i class="icon icon-twitter"></i></a></li><li><a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://kiedeng.github.io/2020/02/29/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86/" data-title=" Google+"><i class="icon icon-google-plus"></i></a></li></ul></div><a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle"><i class="icon icon-share-alt icon-lg"></i></a></div></div></div><nav class="post-nav flex-row flex-justify-between"><div class="waves-block waves-effect prev"><a href="../../../03/01/%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9/" id="post-prev" class="post-nav-link"><div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div><h4 class="title">数据压缩</h4></a></div><div class="waves-block waves-effect next"><a href="../bean%E5%AF%B9%E8%B1%A1%E6%B2%A1%E6%9C%89%E5%BA%8F%E5%88%97%E5%8C%96/" id="post-next" class="post-nav-link"><div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div><h4 class="title">bean对象没有序列化</h4></a></div></nav></article><div id="reward" class="page-modal reward-lay"><a class="close" href="javascript:;"><i class="icon icon-close"></i></a><h3 class="reward-title"><i class="icon icon-quote-left"></i> 谢谢支持 <i class="icon icon-quote-right"></i></h3><div class="reward-content"><div class="reward-code"><img id="rewardCode" src="../../../../img/wechat.jpg" alt="打赏二维码"></div><label class="reward-toggle"><input id="rewardToggle" type="checkbox" class="reward-toggle-check" data-wechat="../../../../img/wechat.jpg" data-alipay="../../../../img/alipay.jpg"><div class="reward-toggle-ctrol"><span class="reward-toggle-item wechat">微信</span> <span class="reward-toggle-label"></span> <span class="reward-toggle-item alipay">支付宝</span></div></label></div></div></div><footer class="footer"><div class="top"><p><span id="busuanzi_container_site_uv" style="display:none">站点总访客数：<span id="busuanzi_value_site_uv"></span> </span><span id="busuanzi_container_site_pv" style="display:none">站点总访问量：<span id="busuanzi_value_site_pv"></span></span></p><p><span><a href="../../../../atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span> <span>博客内容遵循 <a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank">知识共享 署名 - 非商业性 - 相同方式共享 4.0 国际协议</a></span></p></div><div class="bottom"><p><span>康栋 &copy; 2018 - 2021</span> <span>Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a></span></p></div></footer></main><div class="mask" id="mask"></div><a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a><div class="global-share" id="globalShare"><ul class="reset share-icons"><li><a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://kiedeng.github.io/2020/02/29/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86/&title=《MapReduce框架原理》 — 学习印记&pic=http://kiedeng.github.io/img/kieden.jpg" data-title="微博"><i class="icon icon-weibo"></i></a></li><li><a class="weixin share-sns wxFab" href="javascript:;" data-title="微信"><i class="icon icon-weixin"></i></a></li><li><a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://kiedeng.github.io/2020/02/29/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86/&title=《MapReduce框架原理》 — 学习印记&source=" data-title=" QQ"><i class="icon icon-qq"></i></a></li><li><a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://kiedeng.github.io/2020/02/29/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86/" data-title=" Facebook"><i class="icon icon-facebook"></i></a></li><li><a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《MapReduce框架原理》 — 学习印记&url=http://kiedeng.github.io/2020/02/29/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86/&via=http://kiedeng.github.io" data-title=" Twitter"><i class="icon icon-twitter"></i></a></li><li><a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://kiedeng.github.io/2020/02/29/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86/" data-title=" Google+"><i class="icon icon-google-plus"></i></a></li></ul></div><div class="page-modal wx-share" id="wxShare"><a class="close" href="javascript:;"><i class="icon icon-close"></i></a><p>扫一扫，分享到微信</p><img src="//api.qrserver.com/v1/create-qr-code/?data=http://kiedeng.github.io/2020/02/29/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86/" alt="微信分享二维码"></div><script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script><script>var BLOG={ROOT:"/",SHARE:!0,REWARD:!0}</script><script src="../../../../js/main.min.js?v=1.7.0"></script><div class="search-panel" id="search-panel"><ul class="search-result" id="search-result"></ul></div><template id="search-tpl"><li class="item"><a href="{path}" class="waves-block waves-effect"><div class="title ellipsis" title="{title}">{title}</div><div class="flex-row flex-middle"><div class="tags ellipsis">{tags}</div><time class="flex-col time">{date}</time></div></a></li></template><script src="../../../../js/search.min.js?v=1.7.0" async></script><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script></body></html><!-- rebuild by neat -->