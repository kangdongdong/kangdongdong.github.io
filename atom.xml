<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://kiedeng.github.io/"/>
  <updated>2020-02-27T09:47:40.129Z</updated>
  <id>http://kiedeng.github.io/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Combiner合并</title>
    <link href="http://kiedeng.github.io/2020/02/27/Combiner%E5%90%88%E5%B9%B6/"/>
    <id>http://kiedeng.github.io/2020/02/27/Combiner%E5%90%88%E5%B9%B6/</id>
    <published>2020-02-27T09:47:40.000Z</published>
    <updated>2020-02-27T09:47:40.129Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Feb 27 2020 18:40:43 GMT+0800 (GMT+08:00) --><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Thu Feb 27 2020 18:40:43 GMT+0800 (GMT+08:00) --&gt;&lt;!-- rebuild by neat --&gt;
      
    
    </summary>
    
    
      <category term="未分类" scheme="http://kiedeng.github.io/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/"/>
    
    
      <category term="默认" scheme="http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"/>
    
  </entry>
  
  <entry>
    <title>map值类型不匹配</title>
    <link href="http://kiedeng.github.io/2020/02/27/map%E5%80%BC%E7%B1%BB%E5%9E%8B%E4%B8%8D%E5%8C%B9%E9%85%8D/"/>
    <id>http://kiedeng.github.io/2020/02/27/map%E5%80%BC%E7%B1%BB%E5%9E%8B%E4%B8%8D%E5%8C%B9%E9%85%8D/</id>
    <published>2020-02-27T07:48:50.000Z</published>
    <updated>2020-02-27T08:47:02.567Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Feb 27 2020 18:40:43 GMT+0800 (GMT+08:00) --><p>map值类型不匹配</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">java.lang.Exception: java.io.IOException: Type mismatch in value from map: expected com.atguigu.mr.sort.FlowBean, received org.apache.hadoop.io.Text</span><br><span class="line">at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:<span class="number">462</span>)</span><br><span class="line">at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:<span class="number">522</span>)</span><br></pre></td></tr></table></figure><p>错误位置：进行排序案例的时候发生的错误</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">job.setMapOutputKeyClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">job.setMapOutputValueClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;<span class="comment">//应该为Text.class</span></span><br></pre></td></tr></table></figure><blockquote><p>注：写Driver驱动的时候，要特别注意类型错误问题</p><p>因为不太懂这方面的错，所以</p></blockquote><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Thu Feb 27 2020 18:40:43 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;map值类型不匹配&lt;/p&gt;&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter
      
    
    </summary>
    
    
      <category term="常见错误" scheme="http://kiedeng.github.io/categories/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"/>
    
    
      <category term="默认" scheme="http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"/>
    
  </entry>
  
  <entry>
    <title>WritableComparable排序</title>
    <link href="http://kiedeng.github.io/2020/02/27/WritableComparable%E6%8E%92%E5%BA%8F/"/>
    <id>http://kiedeng.github.io/2020/02/27/WritableComparable%E6%8E%92%E5%BA%8F/</id>
    <published>2020-02-27T04:37:15.000Z</published>
    <updated>2020-02-27T10:40:34.388Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Feb 27 2020 18:40:43 GMT+0800 (GMT+08:00) --><h3 id="排序的分类"><a href="#排序的分类" class="headerlink" title="排序的分类"></a>排序的分类</h3><ol><li><p>部分排序</p><p>MapReduce根据输入记录的键对数据集排序，保证输出的每个文件内部有序</p></li><li><p>全排序</p><p>最终输出结果为一个文件，且文件内部有序</p></li><li><p>辅助排序：（GroupingComparator分组)</p><p>​ 在Reduece端对key进行分组。应用于key为bean对象时，想让一个或几个字段相同的key 进入到同一个reduce方法时，可以采用分组排序</p></li><li><p>二次排序</p><p>在自定义排序过程中，如果compareTo中的判断添加为两个即为二次排序</p></li></ol><h3 id="自定义排序WritableComparable"><a href="#自定义排序WritableComparable" class="headerlink" title="自定义排序WritableComparable"></a>自定义排序WritableComparable</h3><ol><li><p>原理分析</p><p>​ bean对象作为key传输，需要实现WritableComparable接口重写compareTo方法，就可以实现排序</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public int compareTo(FlowBean o) &#123;</span><br><span class="line">int result;</span><br><span class="line">&#x2F;&#x2F; 按照总流量大小，倒序排列</span><br><span class="line">if (sumFlow &gt; bean.getSumFlow()) &#123;</span><br><span class="line">result &#x3D; -1;</span><br><span class="line">&#125;else if (sumFlow &lt; bean.getSumFlow()) &#123;</span><br><span class="line">result &#x3D; 1;</span><br><span class="line">&#125;else &#123;</span><br><span class="line">result &#x3D; 0;</span><br><span class="line">&#125; </span><br><span class="line">return result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h3 id="WritableComparable排序案例实操（全排序）"><a href="#WritableComparable排序案例实操（全排序）" class="headerlink" title="WritableComparable排序案例实操（全排序）"></a>WritableComparable排序案例实操（全排序）</h3><ol><li><p>需求：对总流量进行排序</p></li><li><p>代码实现</p><p>使用以前的Flowcount就可以实现，代码就可以实现，略</p></li></ol><h3 id="排序案例（区内排序）"><a href="#排序案例（区内排序）" class="headerlink" title="排序案例（区内排序）"></a>排序案例（区内排序）</h3><p>需求：要求每个省份手机号输出的文件中按照总流量内部排序</p><p>注意点：区间排序中，需要添加的是自定义的Patitioner分区类与在驱动类中添加分区类，设置Reducetask个数</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce.sort;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProvincePartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">FlowBean</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(FlowBean key, Text value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1 获取手机号码前三位</span></span><br><span class="line">String preNum = value.toString().substring(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> partition = <span class="number">4</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 根据手机号归属地设置分区</span></span><br><span class="line"><span class="keyword">if</span> (<span class="string">"136"</span>.equals(preNum)) &#123;</span><br><span class="line">partition = <span class="number">0</span>;</span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"137"</span>.equals(preNum)) &#123;</span><br><span class="line">partition = <span class="number">1</span>;</span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"138"</span>.equals(preNum)) &#123;</span><br><span class="line">partition = <span class="number">2</span>;</span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"139"</span>.equals(preNum)) &#123;</span><br><span class="line">partition = <span class="number">3</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> partition;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 加载自定义分区类</span></span><br><span class="line">job.setPartitionerClass(ProvincePartitioner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">// 设置Reducetask个数</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></figure><h3 id="GroupingComparator分组（辅助排序）"><a href="#GroupingComparator分组（辅助排序）" class="headerlink" title="GroupingComparator分组（辅助排序）"></a>GroupingComparator分组（辅助排序）</h3><p>对Reduce阶段的数据根据某一个或几个字段进行分组</p><ol><li><p>自定义继承WritableComparator</p></li><li><p>重写compare()方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(WritableComparable a, WritableComparable b)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 比较的业务逻辑</span></span><br><span class="line"><span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>创建一个构造将比较对象的类传给父类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="title">OrderGroupingComparator</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">super</span>(OrderBean<span class="class">.<span class="keyword">class</span>, <span class="title">true</span>)</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h3 id="GroupingComparator分组案例实操"><a href="#GroupingComparator分组案例实操" class="headerlink" title="GroupingComparator分组案例实操"></a>GroupingComparator分组案例实操</h3><ol><li><p>需求：有如下订单数据</p><p>表4-2 订单数据</p></li></ol><table><thead><tr><th>订单id</th><th>商品id</th><th>成交金额</th></tr></thead><tbody><tr><td>0000001</td><td>Pdt_01</td><td>222.8</td></tr><tr><td></td><td>Pdt_02</td><td>33.8</td></tr><tr><td>0000002</td><td>Pdt_03</td><td>522.8</td></tr><tr><td></td><td>Pdt_04</td><td>122.4</td></tr><tr><td></td><td>Pdt_05</td><td>722.4</td></tr><tr><td>0000003</td><td>Pdt_06</td><td>232.8</td></tr><tr><td></td><td>Pdt_02</td><td>33.8</td></tr></tbody></table><p>现在需要求出每一个订单中最贵的商品。</p><p><strong>期望输出数据</strong></p><blockquote><p>1 222.8</p><p>2 722.4</p><p>3 232.8</p></blockquote><ol start="2"><li><p>需求分析</p><ol><li>利用“订单id和成交金额”作为key，可以将Map阶段读取到的所有订单数据按照id升序排序，如果id相同按照金额降序排序，发送到Reduce</li><li>在Reduce端利用groupComparator将订单id相同的kv合成为组，然后取第一个即是该订单中最贵商品</li></ol></li><li><p>代码实现</p><ol><li><p>定义订单信息OrderBean类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce.order;</span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderBean</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">OrderBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">int</span> order_id; <span class="comment">// 订单id号</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">double</span> price; <span class="comment">// 价格</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">OrderBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">super</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">OrderBean</span><span class="params">(<span class="keyword">int</span> order_id, <span class="keyword">double</span> price)</span> </span>&#123;</span><br><span class="line"><span class="keyword">super</span>();</span><br><span class="line"><span class="keyword">this</span>.order_id = order_id;</span><br><span class="line"><span class="keyword">this</span>.price = price;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">out.writeInt(order_id);</span><br><span class="line">out.writeDouble(price);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">order_id = in.readInt();</span><br><span class="line">price = in.readDouble();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> order_id + <span class="string">"\t"</span> + price;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getOrder_id</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> order_id;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setOrder_id</span><span class="params">(<span class="keyword">int</span> order_id)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.order_id = order_id;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">double</span> <span class="title">getPrice</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> price;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setPrice</span><span class="params">(<span class="keyword">double</span> price)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.price = price;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 二次排序</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(OrderBean o)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> result;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (order_id &gt; o.getOrder_id()) &#123;</span><br><span class="line">result = <span class="number">1</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (order_id &lt; o.getOrder_id()) &#123;</span><br><span class="line">result = -<span class="number">1</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// 价格倒序排序</span></span><br><span class="line">result = price &gt; o.getPrice() ? -<span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>编写OrderSortMapper类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce.order;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">OrderBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">OrderBean k = <span class="keyword">new</span> OrderBean();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1 获取一行</span></span><br><span class="line">String line = value.toString();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 截取</span></span><br><span class="line">String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3 封装对象</span></span><br><span class="line">k.setOrder_id(Integer.parseInt(fields[<span class="number">0</span>]));</span><br><span class="line">k.setPrice(Double.parseDouble(fields[<span class="number">2</span>]));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4 写出</span></span><br><span class="line">context.write(k, NullWritable.get());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>编写OrderSortGroupingComparator类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce.order;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparator;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderGroupingComparator</span> <span class="keyword">extends</span> <span class="title">WritableComparator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="title">OrderGroupingComparator</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">super</span>(OrderBean<span class="class">.<span class="keyword">class</span>, <span class="title">true</span>)</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(WritableComparable a, WritableComparable b)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">OrderBean aBean = (OrderBean) a;</span><br><span class="line">OrderBean bBean = (OrderBean) b;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> result;</span><br><span class="line"><span class="keyword">if</span> (aBean.getOrder_id() &gt; bBean.getOrder_id()) &#123;</span><br><span class="line">result = <span class="number">1</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (aBean.getOrder_id() &lt; bBean.getOrder_id()) &#123;</span><br><span class="line">result = -<span class="number">1</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">result = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>编写OrderSortReducer类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce.order;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">OrderBean</span>, <span class="title">NullWritable</span>, <span class="title">OrderBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(OrderBean key, Iterable&lt;NullWritable&gt; values, Context context)</span><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">context.write(key, NullWritable.get());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>编写OrderSortDriver类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce.order;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception, IOException </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输入输出路径需要根据自己电脑上实际的输入输出路径设置</span></span><br><span class="line">args  = <span class="keyword">new</span> String[]&#123;<span class="string">"e:/input/inputorder"</span> , <span class="string">"e:/output1"</span>&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1 获取配置信息</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 设置jar包加载路径</span></span><br><span class="line">job.setJarByClass(OrderDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3 加载map/reduce类</span></span><br><span class="line">job.setMapperClass(OrderMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">job.setReducerClass(OrderReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4 设置map输出数据key和value类型</span></span><br><span class="line">job.setMapOutputKeyClass(OrderBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">job.setMapOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5 设置最终输出数据的key和value类型</span></span><br><span class="line">job.setOutputKeyClass(OrderBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 6 设置输入数据和输出数据路径</span></span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 8 设置reduce端的分组</span></span><br><span class="line">job.setGroupingComparatorClass(OrderGroupingComparator<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 7 提交</span></span><br><span class="line"><span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol></li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Thu Feb 27 2020 18:40:43 GMT+0800 (GMT+08:00) --&gt;&lt;h3 id=&quot;排序的分类&quot;&gt;&lt;a href=&quot;#排序的分类&quot; class=&quot;headerlink&quot; title=&quot;排序的分类&quot;&gt;&lt;/a&gt;排序的分类&lt;
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="http://kiedeng.github.io/categories/Hadoop/"/>
    
    
      <category term="默认" scheme="http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"/>
    
  </entry>
  
  <entry>
    <title>Partition分区</title>
    <link href="http://kiedeng.github.io/2020/02/26/Partition%E5%88%86%E5%8C%BA/"/>
    <id>http://kiedeng.github.io/2020/02/26/Partition%E5%88%86%E5%8C%BA/</id>
    <published>2020-02-26T15:51:54.000Z</published>
    <updated>2020-02-26T16:46:55.670Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Feb 27 2020 18:40:43 GMT+0800 (GMT+08:00) --><p><strong>问题引出</strong>：将结果按照条件输出到不同文件（分区）中</p><p>默认的分区是根据key的hashCode对ReduceTasks个数取模得到的。</p><h3 id="自定义Partitioner步骤"><a href="#自定义Partitioner步骤" class="headerlink" title="自定义Partitioner步骤"></a>自定义Partitioner步骤</h3><ol><li><p>自定义继承Partitioner,重写getPartition()方法</p></li><li><p>在job驱动中，设置自定义Partitioner</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setPatitionerClass(CustonPartitioner.class)</span><br></pre></td></tr></table></figure></li><li><p>自定义Partition后，要根据自定义Partition的逻辑设置相对应的ReduceTask</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setNumReduceTasd();</span><br></pre></td></tr></table></figure></li><li><p>分区总结</p><blockquote><p>（1）如果Reduce Task的数量&gt;getPartition的结果数，则会多产生几个空的输出文件part-r-000xx；</p><p>（2）如果1&lt;ReduceTask的数量&lt;getPartition的结果数，则有一部分分区数据无处安放，会Exception；<br>（3）如果ReduceTask的数量=1，则不管MapTask端输出多少个分区文件，最终结果都交给这一个Reduce Task，最终也就只会产生一个结果文件part-r-00000；<br>（4）分区号必须从零开始，逐一累加。</p></blockquote></li></ol><h3 id="Partition案例实操"><a href="#Partition案例实操" class="headerlink" title="Partition案例实操"></a>Partition案例实操</h3><p>题目：将统计结果按照手机归属地不同省份输出到不同文件中（分区）</p><ul><li><p>添加分区类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce.flowsum;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProvincePartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text key, FlowBean value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1 获取电话号码的前三位</span></span><br><span class="line">String preNum = key.toString().substring(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> partition = <span class="number">4</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 判断是哪个省</span></span><br><span class="line"><span class="keyword">if</span> (<span class="string">"136"</span>.equals(preNum)) &#123;</span><br><span class="line">partition = <span class="number">0</span>;</span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"137"</span>.equals(preNum)) &#123;</span><br><span class="line">partition = <span class="number">1</span>;</span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"138"</span>.equals(preNum)) &#123;</span><br><span class="line">partition = <span class="number">2</span>;</span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"139"</span>.equals(preNum)) &#123;</span><br><span class="line">partition = <span class="number">3</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> partition;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>在驱动函数中增加自定义数据分区设置和ReduceTask设置</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 8 指定自定义数据分区</span></span><br><span class="line">job.setPartitionerClass(ProvincePartitioner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">// 9 同时指定相应数量的reduce task</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></figure></li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Thu Feb 27 2020 18:40:43 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;strong&gt;问题引出&lt;/strong&gt;：将结果按照条件输出到不同文件（分区）中&lt;/p&gt;&lt;p&gt;默认的分区是根据key的hashCode对Red
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="http://kiedeng.github.io/categories/Hadoop/"/>
    
    
      <category term="默认" scheme="http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"/>
    
  </entry>
  
  <entry>
    <title>背诵知识点</title>
    <link href="http://kiedeng.github.io/2020/02/26/%E8%83%8C%E8%AF%B5%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
    <id>http://kiedeng.github.io/2020/02/26/%E8%83%8C%E8%AF%B5%E7%9F%A5%E8%AF%86%E7%82%B9/</id>
    <published>2020-02-26T15:21:28.000Z</published>
    <updated>2020-02-26T15:26:40.986Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Feb 27 2020 18:40:43 GMT+0800 (GMT+08:00) --><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 键值对修改切割值</span><br><span class="line">conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, &quot; &quot;);</span><br><span class="line">&#x2F;&#x2F; 7设置每个切片InputSplit中划分三条记录</span><br><span class="line">NLineInputFormat.setNumLinesPerSplit(job, 3);</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Thu Feb 27 2020 18:40:43 GMT+0800 (GMT+08:00) --&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span cl
      
    
    </summary>
    
    
      <category term="未分类" scheme="http://kiedeng.github.io/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/"/>
    
    
      <category term="默认" scheme="http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"/>
    
  </entry>
  
  <entry>
    <title>Text导包错误</title>
    <link href="http://kiedeng.github.io/2020/02/26/Text%E5%AF%BC%E5%8C%85%E9%94%99%E8%AF%AF/"/>
    <id>http://kiedeng.github.io/2020/02/26/Text%E5%AF%BC%E5%8C%85%E9%94%99%E8%AF%AF/</id>
    <published>2020-02-26T05:52:34.000Z</published>
    <updated>2020-02-26T05:58:21.071Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Feb 27 2020 18:40:43 GMT+0800 (GMT+08:00) --><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">java.lang.ClassCastException: class com.sun.jersey.core.impl.provider.entity.XMLJAXBElementProvider$Text</span><br><span class="line">at java.lang.Class.asSubclass(Class.java:3404)</span><br><span class="line">at org.apache.hadoop.mapred.JobConf.getOutputKeyComparator(JobConf.java:887)</span><br><span class="line">at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.init(MapTask.java:1004)</span><br><span class="line">at org.apache.hadoop.mapred.MapTask.createSortingCollector(MapTask.java:402)</span><br><span class="line">at org.apache.hadoop.mapred.MapTask.access$100(MapTask.java:81)</span><br><span class="line">at org.apache.hadoop.mapred.MapTask$NewOutputCollector.<span class="tag">&lt;<span class="name">init</span>&gt;</span>(MapTask.java:698)</span><br><span class="line">at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:770)</span><br><span class="line">at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)</span><br><span class="line">at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)</span><br><span class="line">at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)</span><br><span class="line">at java.util.concurrent.FutureTask.run(FutureTask.java:266)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)</span><br><span class="line">at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure><p>错误的导入了Text包，</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">错误：</span><br><span class="line"><span class="keyword">import</span> com.sun.jersey.core.impl.provider.entity.XMLJAXBElementProvider.Text;</span><br><span class="line">正确的包：</span><br><span class="line">    <span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Thu Feb 27 2020 18:40:43 GMT+0800 (GMT+08:00) --&gt;&lt;figure class=&quot;highlight xml&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span clas
      
    
    </summary>
    
    
      <category term="常见错误" scheme="http://kiedeng.github.io/categories/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"/>
    
    
      <category term="默认" scheme="http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode计划表</title>
    <link href="http://kiedeng.github.io/2020/02/21/LeetCode%E8%AE%A1%E5%88%92%E8%A1%A8/"/>
    <id>http://kiedeng.github.io/2020/02/21/LeetCode%E8%AE%A1%E5%88%92%E8%A1%A8/</id>
    <published>2020-02-21T05:26:42.000Z</published>
    <updated>2020-02-21T05:33:01.684Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Feb 27 2020 18:40:43 GMT+0800 (GMT+08:00) --><table><thead><tr><th>题目序号</th><th>题目名称</th><th>总结</th></tr></thead><tbody><tr><td>1</td><td>Two Sum</td><td></td></tr><tr><td>4</td><td>Median of Two Sorted Arrays</td><td></td></tr><tr><td>11</td><td>Container With Most Water</td><td></td></tr><tr><td>17</td><td>Letter Combinations of a Phone Number</td><td></td></tr><tr><td>21</td><td>Merge Two Sorted Lists</td><td></td></tr><tr><td>23</td><td>Merge k Sorted Lists</td><td></td></tr><tr><td>37</td><td>Sudoku Solver</td><td></td></tr><tr><td>39</td><td>Combination Sum</td><td></td></tr><tr><td>40</td><td>Combination Sum II</td><td></td></tr><tr><td>51</td><td>N-Queens</td><td></td></tr><tr><td>53</td><td>Maximum Subarray</td><td></td></tr><tr><td>56</td><td>Merge Intervals</td><td></td></tr><tr><td>57</td><td>Insert Interval</td><td></td></tr><tr><td>62</td><td>Unique Paths</td><td></td></tr><tr><td>63</td><td>Unique Paths II</td><td></td></tr><tr><td>64</td><td>Minimum Path Sum</td><td></td></tr><tr><td>69</td><td>sqrt(x)</td><td></td></tr><tr><td>70</td><td>Climbing Stairs</td><td></td></tr><tr><td>72</td><td>Edit Distance</td><td></td></tr><tr><td>78</td><td>Subsets</td><td></td></tr><tr><td>79</td><td>Word Search</td><td></td></tr><tr><td>91</td><td>Decode Ways</td><td></td></tr><tr><td>98</td><td>Validate Binary Search Tree</td><td></td></tr><tr><td>100</td><td>Same Tree</td><td></td></tr><tr><td>102</td><td>Binary Tree Level Order Traversal</td><td></td></tr><tr><td>110</td><td>Balanced Binary Tree</td><td></td></tr><tr><td>112</td><td>Path Sum</td><td></td></tr><tr><td>113</td><td>Path Sum II</td><td></td></tr><tr><td>115</td><td>Distinct Subsequences</td><td></td></tr><tr><td>120</td><td>Triangle</td><td></td></tr><tr><td>121</td><td>Best Time to Buy and Sell Stock</td><td></td></tr><tr><td>124</td><td>Binary Tree Maximum Path Sum</td><td></td></tr><tr><td>126</td><td>Word Ladder II</td><td></td></tr><tr><td>127</td><td>Word Ladder</td><td></td></tr><tr><td>128</td><td>Longest Consecutive Sequence</td><td></td></tr><tr><td>139</td><td>Word Break (revisit)</td><td></td></tr><tr><td>140</td><td>Word Break II</td><td></td></tr><tr><td>141</td><td>Linked List Cycle</td><td></td></tr><tr><td>145</td><td>Binary Tree Postorder Traversal</td><td></td></tr><tr><td>146</td><td>LRU Cache</td><td></td></tr><tr><td>148</td><td>Sort List</td><td></td></tr><tr><td>149</td><td>Max Points on a Line</td><td></td></tr><tr><td>153</td><td>Find Minimum in Rotated Sorted Array</td><td></td></tr><tr><td>154</td><td>Find Minimum in Rotated Sorted Array II</td><td></td></tr><tr><td>169</td><td>Majority Element</td><td></td></tr><tr><td>174</td><td>Dungeon Game</td><td></td></tr><tr><td>198</td><td>House Robber</td><td></td></tr><tr><td>200</td><td>Number of Islands</td><td></td></tr><tr><td>204</td><td>Count Primes</td><td></td></tr><tr><td>207</td><td>Course Schedule</td><td></td></tr><tr><td>208</td><td>Implement Trie (Prefix Tree)</td><td></td></tr><tr><td>210</td><td>Course Schedule II</td><td></td></tr><tr><td>216</td><td>Combination Sum III</td><td></td></tr><tr><td>218</td><td>The Skyline Problem</td><td></td></tr><tr><td>221</td><td>Maximal Square</td><td></td></tr><tr><td>239</td><td>Sliding Window Maximum</td><td></td></tr><tr><td>241</td><td>Different Ways to Add Parentheses</td><td></td></tr><tr><td>263</td><td>Ugly Number</td><td></td></tr><tr><td>264</td><td>Ugly Number II</td><td></td></tr><tr><td>268</td><td>Missing Number</td><td></td></tr><tr><td>282</td><td>Expression Add Operators</td><td></td></tr><tr><td>289</td><td>Game of Life</td><td></td></tr><tr><td>295</td><td>Find Median from Data Stream</td><td></td></tr><tr><td>297</td><td>Serialize and Deserialize Binary Tree</td><td></td></tr><tr><td>300</td><td>Longest Increasing Subsequence</td><td></td></tr><tr><td>301</td><td>Remove Invalid Parentheses</td><td></td></tr><tr><td>303</td><td>Range Sum Query - Immutable</td><td></td></tr><tr><td>304</td><td>Range Sum Query 2D - Immutable</td><td></td></tr><tr><td>309</td><td>Best Time to Buy and Sell Stock with Cooldown</td><td></td></tr><tr><td>312</td><td>Burst Balloons</td><td></td></tr><tr><td>315</td><td>Count of Smaller Numbers After Self</td><td></td></tr><tr><td>321</td><td>Create Maximum Number</td><td></td></tr><tr><td>322</td><td>Coin Change</td><td></td></tr><tr><td>329</td><td>Longest Increasing Path in a Matrix</td><td></td></tr><tr><td>332</td><td>Reconstruct Itinerary</td><td></td></tr><tr><td>347</td><td>Top K Frequent Elements</td><td></td></tr><tr><td>377</td><td>Combination Sum IV</td><td></td></tr><tr><td>380</td><td>Insert Delete GetRandom O(1)</td><td></td></tr><tr><td>381</td><td>Insert Delete GetRandom O(1) - Duplicates allowed</td><td></td></tr><tr><td>391</td><td>Perfect Rectangle</td><td></td></tr><tr><td>399</td><td>Evaluate Division</td><td></td></tr><tr><td>404</td><td>Sum of Left Leaves</td><td></td></tr><tr><td>409</td><td>Longest Palindrome</td><td></td></tr><tr><td>410</td><td>Split Array Largest Sum</td><td></td></tr><tr><td>416</td><td>Partition Equal Subset Sum</td><td></td></tr><tr><td>417</td><td>Pacific Atlantic Water Flow</td><td></td></tr><tr><td>432</td><td>All O`one Data Structure</td><td></td></tr><tr><td>438</td><td>Find All Anagrams in a String</td><td></td></tr><tr><td>449</td><td>Serialize and Deserialize BST</td><td></td></tr><tr><td>450</td><td>Delete Node in a BST</td><td></td></tr><tr><td>451</td><td>Sort Characters By Frequency</td><td></td></tr><tr><td>452</td><td>Minimum Number of Arrows to Burst Balloons</td><td></td></tr><tr><td>455</td><td>Assign Cookies</td><td></td></tr><tr><td>460</td><td>LFU Cache</td><td></td></tr><tr><td>461</td><td>Hamming Distance</td><td></td></tr><tr><td>463</td><td>Island Perimeter</td><td></td></tr><tr><td>464</td><td>Can I Win</td><td></td></tr><tr><td>470</td><td>Implement Rand10() Using Rand7()</td><td></td></tr><tr><td>476</td><td>Number Complement</td><td></td></tr><tr><td>477</td><td>Total Hamming Distance</td><td></td></tr><tr><td>480</td><td>Sliding Window Median</td><td></td></tr><tr><td>486</td><td>Predict the Winner</td><td></td></tr><tr><td>488</td><td>Zuma Game</td><td></td></tr><tr><td>494</td><td>Target Sum2</td><td></td></tr><tr><td>494</td><td>Target Sum</td><td></td></tr><tr><td>504</td><td>Base 7</td><td></td></tr><tr><td>516</td><td>Longest Palindromic Subsequence</td><td></td></tr><tr><td>525</td><td>Contiguous Array</td><td></td></tr><tr><td>530</td><td>Minimum Absolute Difference in BST</td><td></td></tr><tr><td>540</td><td>Single Element in a Sorted Array</td><td></td></tr><tr><td>543</td><td>Diameter of Binary Tree</td><td></td></tr><tr><td>546</td><td>Remove Boxes</td><td></td></tr><tr><td>547</td><td>Friend Circles</td><td></td></tr><tr><td>551</td><td>Student Attendance Record I</td><td></td></tr><tr><td>560</td><td>Subarray Sum Equals K</td><td></td></tr><tr><td>561</td><td>Array Partition I</td><td></td></tr><tr><td>566</td><td>Reshape the Matrix</td><td></td></tr><tr><td>567</td><td>Permutation in String</td><td></td></tr><tr><td>576</td><td>Out of Boundary Paths</td><td></td></tr><tr><td>606</td><td>Construct String from Binary Tree</td><td></td></tr><tr><td>611</td><td>Valid Triangle Number</td><td></td></tr><tr><td>617</td><td>Merge Two Binary Trees</td><td></td></tr><tr><td>621</td><td>Task Scheduler</td><td></td></tr><tr><td>628</td><td>Maximum Product of Three Numbers</td><td></td></tr><tr><td>633</td><td>Sum of Square Numbers</td><td></td></tr><tr><td>636</td><td>Exclusive Time of Functions</td><td></td></tr><tr><td>637</td><td>Average of Levels in Binary Tree</td><td></td></tr><tr><td>639</td><td>Decode Ways II</td><td></td></tr><tr><td>652</td><td>Find Duplicate Subtrees</td><td></td></tr><tr><td>654</td><td>Maximum Binary Tree</td><td></td></tr><tr><td>655</td><td>Print Binary Tree</td><td></td></tr><tr><td>657</td><td>Judge Route Circle</td><td></td></tr><tr><td>664</td><td>Strange Printer</td><td></td></tr><tr><td>668</td><td>Kth Smallest Number in Multiplication Table</td><td></td></tr><tr><td>669</td><td>Trim a Binary Search Tree</td><td></td></tr><tr><td>671</td><td>Second Minimum Node In a Binary Tree</td><td></td></tr><tr><td>673</td><td>Number of Longest Increasing Subsequence</td><td></td></tr><tr><td>674</td><td>Longest Continuous Increasing Subsequence</td><td></td></tr><tr><td>675</td><td>Cut Off Trees for Golf Event</td><td></td></tr><tr><td>676</td><td>Implement Magic Dictionary</td><td></td></tr><tr><td>677</td><td>Map Sum Pairs</td><td></td></tr><tr><td>678</td><td>Valid Parenthesis String</td><td></td></tr><tr><td>680</td><td>Valid Palindrome II</td><td></td></tr><tr><td>681</td><td>Next Closest Time</td><td></td></tr><tr><td>682</td><td>Baseball Game</td><td></td></tr><tr><td>683</td><td>K Empty Slots</td><td></td></tr><tr><td>684</td><td>Redundant Connection</td><td></td></tr><tr><td>685</td><td>Redundant Connection II</td><td></td></tr><tr><td>687</td><td>Longest Univalue Path</td><td></td></tr><tr><td>688</td><td>Knight Probability in Chessboard</td><td></td></tr><tr><td>690</td><td>Employee Importance</td><td></td></tr><tr><td>692</td><td>Top K Frequent Words</td><td></td></tr><tr><td>699</td><td>Falling Squares</td><td></td></tr><tr><td>707</td><td>Design Linked List</td><td></td></tr><tr><td>712</td><td>Minimum ASCII Delete Sum for Two Strings</td><td></td></tr><tr><td>715</td><td>Range Module</td><td></td></tr><tr><td>719</td><td>Find K-th Smallest Pair Distance</td><td></td></tr><tr><td>720</td><td>Longest Word in Dictionary</td><td></td></tr><tr><td>724</td><td>Find Pivot Index</td><td></td></tr><tr><td>725</td><td>Split Linked List in Parts</td><td></td></tr><tr><td>726</td><td>Number of Atoms</td><td></td></tr><tr><td>728</td><td>Self Dividing Numbers</td><td></td></tr><tr><td>729</td><td>My Calendar I</td><td></td></tr><tr><td>730</td><td>Count Different Palindromic Subsequences</td><td></td></tr><tr><td>731</td><td>My Calendar II</td><td></td></tr><tr><td>732</td><td>My Calendar III</td><td></td></tr><tr><td>733</td><td>Flood Fill</td><td></td></tr><tr><td>734</td><td>Sentence Similarity</td><td></td></tr><tr><td>735</td><td>Asteroid Collision</td><td></td></tr><tr><td>736</td><td>Parse Lisp Expression</td><td></td></tr><tr><td>737</td><td>Sentence Similarity II</td><td></td></tr><tr><td>740</td><td>Delete and Earn</td><td></td></tr><tr><td>741</td><td>Cherry Pickup</td><td></td></tr><tr><td>742</td><td>Closest Leaf in a Binary Tree</td><td></td></tr><tr><td>743</td><td>Network Delay Time</td><td></td></tr><tr><td>744</td><td>Find Smallest Letter Greater Than Target</td><td></td></tr><tr><td>745</td><td>Prefix and Suffix Search</td><td></td></tr><tr><td>746</td><td>Min Cost Climbing Stairs</td><td></td></tr><tr><td>748</td><td>Shortest Completing Word</td><td></td></tr><tr><td>749</td><td>Contain Virus</td><td></td></tr><tr><td>752</td><td>Open the Lock</td><td></td></tr><tr><td>753</td><td>Cracking the Safe</td><td></td></tr><tr><td>754</td><td>Reach a Number</td><td></td></tr><tr><td>755</td><td>Pour Water</td><td></td></tr><tr><td>758</td><td>Bold Words in String</td><td></td></tr><tr><td>759</td><td>Employee Free Time</td><td></td></tr><tr><td>762</td><td>Prime Number of Set Bits in Binary Representation</td><td></td></tr><tr><td>763</td><td>Partition Labels</td><td></td></tr><tr><td>769</td><td>Max Chunks To Make Sorted</td><td></td></tr><tr><td>773</td><td>Sliding Puzzle</td><td></td></tr><tr><td>775</td><td>Global and Local Inversions</td><td></td></tr><tr><td>778</td><td>Swim in Rising Water</td><td></td></tr><tr><td>784</td><td>Letter Case Permutation</td><td></td></tr><tr><td>786</td><td>K-th Smallest Prime Fraction</td><td></td></tr><tr><td>787</td><td>Cheapest Flights Within K Stops</td><td></td></tr><tr><td>790</td><td>Domino and Tromino Tiling</td><td></td></tr><tr><td>792</td><td>Number of Matching Subsequences</td><td></td></tr><tr><td>799</td><td>Champagne Tower</td><td></td></tr><tr><td>801</td><td>Minimum Swaps To Make Sequences Increasing</td><td></td></tr><tr><td>802</td><td>Find Eventual Safe States</td><td></td></tr><tr><td>803</td><td>Bricks Falling When Hit</td><td></td></tr><tr><td>813</td><td>Largest Sum of Averages</td><td></td></tr><tr><td>815</td><td>Bus Routes</td><td></td></tr><tr><td>817</td><td>Linked List Components</td><td></td></tr><tr><td>818</td><td>Race Car2</td><td></td></tr><tr><td>818</td><td>Race Car</td><td></td></tr><tr><td>823</td><td>Binary Trees With Factors</td><td></td></tr><tr><td>826</td><td>Most Profit Assigning Work</td><td></td></tr><tr><td>827</td><td>Making A Large Island</td><td></td></tr><tr><td>841</td><td>Keys and Rooms</td><td></td></tr><tr><td>847</td><td>Shortest Path Visiting All Nodes</td><td></td></tr><tr><td>848</td><td>Shifting Letters</td><td></td></tr><tr><td>856</td><td>Score of Parentheses</td><td></td></tr><tr><td>863</td><td>All Nodes Distance K in Binary Tree</td><td></td></tr><tr><td>864</td><td>Shortest Path to Get All Keys</td><td></td></tr><tr><td>865</td><td>Smallest Subtree with all the Deepest Nodes</td><td></td></tr><tr><td>871</td><td>Minimum Number of Refueling Stops</td><td></td></tr><tr><td>873</td><td>Length of Longest Fibonacci Subsequence</td><td></td></tr><tr><td>877</td><td>Stone Game</td><td></td></tr><tr><td>879</td><td>Profitable Schemes</td><td></td></tr><tr><td>882</td><td>Reachable Nodes In Subdivided Graph</td><td></td></tr><tr><td>886</td><td>Possible Bipartition</td><td></td></tr><tr><td>889</td><td>Construct Binary Tree from Preorder and Postorder Traversal</td><td></td></tr><tr><td>891</td><td>Sum of Subsequence Widths</td><td></td></tr><tr><td>894</td><td>All Possible Full Binary Trees</td><td></td></tr><tr><td>895</td><td>Maximum Frequency Stack</td><td></td></tr><tr><td>898</td><td>Bitwise ORs of Subarrays</td><td></td></tr><tr><td>901</td><td>Online Stock Span</td><td></td></tr><tr><td>902</td><td>Numbers At Most N Given Digit Set</td><td></td></tr><tr><td>923</td><td>3Sum With Multiplicity</td><td></td></tr><tr><td>926</td><td>Flip String to Monotone Increasing</td><td></td></tr><tr><td>934</td><td>Shortest Bridge</td><td></td></tr><tr><td>935</td><td>Knight Dialer</td><td></td></tr><tr><td>936</td><td>Stamping The Sequence</td><td></td></tr><tr><td>943</td><td>Find the Shortest Superstring</td><td></td></tr><tr><td>952</td><td>Largest Component Size by Common Factor</td><td></td></tr><tr><td>956</td><td>Tallest Billboard</td><td></td></tr><tr><td>959</td><td>Regions Cut By Slashes</td><td></td></tr><tr><td>964</td><td>Least Operators to Express Number</td><td></td></tr><tr><td>967</td><td>Numbers With Same Consecutive Differences</td><td></td></tr><tr><td>972</td><td>Equal Rational Numbers</td><td></td></tr><tr><td>973</td><td>K Closest Points to Origin</td><td></td></tr><tr><td>975</td><td>Odd Even Jump</td><td></td></tr><tr><td>979</td><td>Distribute Coins in Binary Tree</td><td></td></tr><tr><td>980</td><td>Unique Paths III</td><td></td></tr><tr><td>1000</td><td>Minimum Cost to Merge Stones</td><td></td></tr><tr><td>1017</td><td>Convert to Base -2</td><td></td></tr><tr><td>1019</td><td>Next Greater Node In Linked List</td><td></td></tr><tr><td>1024</td><td>Video Stitching</td><td></td></tr><tr><td>1043</td><td>Partition Array for Maximum Sum</td><td></td></tr><tr><td>1092</td><td>Shortest Common Supersequenc</td><td></td></tr><tr><td>1105</td><td>Filling Bookcase Shelves</td><td></td></tr><tr><td>1106</td><td>Parsing A Boolean Expression</td><td></td></tr><tr><td>1124</td><td>Longest Well-Performing Interval</td><td></td></tr><tr><td>1125</td><td>Smallest Sufficient Team</td><td></td></tr><tr><td>1129</td><td>Shortest Path with Alternating Colors</td><td></td></tr></tbody></table><p>的</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Thu Feb 27 2020 18:40:43 GMT+0800 (GMT+08:00) --&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;题目序号&lt;/th&gt;&lt;th&gt;题目名称&lt;/th&gt;&lt;th&gt;总结&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;
      
    
    </summary>
    
    
      <category term="算法" scheme="http://kiedeng.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="LeetCode" scheme="http://kiedeng.github.io/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce工作流程</title>
    <link href="http://kiedeng.github.io/2020/02/20/MapReduce%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B/"/>
    <id>http://kiedeng.github.io/2020/02/20/MapReduce%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B/</id>
    <published>2020-02-20T13:39:47.000Z</published>
    <updated>2020-02-20T13:39:47.432Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Feb 21 2020 01:03:29 GMT+0800 (GMT+08:00) --><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri Feb 21 2020 01:03:29 GMT+0800 (GMT+08:00) --&gt;&lt;!-- rebuild by neat --&gt;
      
    
    </summary>
    
    
      <category term="未分类" scheme="http://kiedeng.github.io/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/"/>
    
    
      <category term="默认" scheme="http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"/>
    
  </entry>
  
  <entry>
    <title>Shuffle机制</title>
    <link href="http://kiedeng.github.io/2020/02/20/Shuffle%E6%9C%BA%E5%88%B6/"/>
    <id>http://kiedeng.github.io/2020/02/20/Shuffle%E6%9C%BA%E5%88%B6/</id>
    <published>2020-02-20T13:38:49.000Z</published>
    <updated>2020-02-20T13:38:49.220Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Feb 21 2020 01:03:29 GMT+0800 (GMT+08:00) --><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri Feb 21 2020 01:03:29 GMT+0800 (GMT+08:00) --&gt;&lt;!-- rebuild by neat --&gt;
      
    
    </summary>
    
    
      <category term="未分类" scheme="http://kiedeng.github.io/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/"/>
    
    
      <category term="默认" scheme="http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"/>
    
  </entry>
  
  <entry>
    <title>InputFormat数据输入</title>
    <link href="http://kiedeng.github.io/2020/02/20/InputFormat%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5/"/>
    <id>http://kiedeng.github.io/2020/02/20/InputFormat%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5/</id>
    <published>2020-02-20T13:38:16.000Z</published>
    <updated>2020-02-26T03:34:10.043Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Feb 27 2020 18:40:43 GMT+0800 (GMT+08:00) --><h2 id="切片与MapTask并行度决定机制"><a href="#切片与MapTask并行度决定机制" class="headerlink" title="切片与MapTask并行度决定机制"></a>切片与MapTask并行度决定机制</h2><p><strong>数据块</strong>：Block是HDFS物理上把数据分成一块一块。</p><p><strong>数据切片</strong>：数据切片只是逻辑上对输入进行分片，并不会再磁盘上将其切分成片进行存储。</p><h2 id="Job提交流程源码和切片源码详解"><a href="#Job提交流程源码和切片源码详解" class="headerlink" title="Job提交流程源码和切片源码详解"></a>Job提交流程源码和切片源码详解</h2><ol><li><p>Job提交流程源码详解</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">waitForCompletion()</span><br><span class="line"></span><br><span class="line">submit();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1建立连接</span></span><br><span class="line">connect();</span><br><span class="line"><span class="comment">// 1）创建提交Job的代理</span></span><br><span class="line"><span class="keyword">new</span> Cluster(getConfiguration());</span><br><span class="line"><span class="comment">// （1）判断是本地yarn还是远程</span></span><br><span class="line">initialize(jobTrackAddr, conf); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 提交job</span></span><br><span class="line">submitter.submitJobInternal(Job.<span class="keyword">this</span>, cluster)</span><br><span class="line"><span class="comment">// 1）创建给集群提交数据的Stag路径</span></span><br><span class="line">Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2）获取jobid ，并创建Job路径</span></span><br><span class="line">JobID jobId = submitClient.getNewJobID();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3）拷贝jar包到集群</span></span><br><span class="line">copyAndConfigureFiles(job, submitJobDir);</span><br><span class="line">rUploader.uploadFiles(job, jobSubmitDir);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4）计算切片，生成切片规划文件</span></span><br><span class="line">writeSplits(job, submitJobDir);</span><br><span class="line">maps = writeNewSplits(job, jobSubmitDir);</span><br><span class="line">input.getSplits(job);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5）向Stag路径写XML配置文件</span></span><br><span class="line">writeConf(conf, submitJobFile);</span><br><span class="line">conf.writeXml(out);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 6）提交Job,返回提交状态</span></span><br><span class="line">status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());</span><br></pre></td></tr></table></figure></li></ol><p><img src="https://kiedeng.site/usr/uploads/2020/02/2136348625.png" alt="Snipaste_2020-02-20_22-05-42.png"></p><h2 id="FileInputFormat切片机制"><a href="#FileInputFormat切片机制" class="headerlink" title="FileInputFormat切片机制"></a>FileInputFormat切片机制</h2><ol><li><p>切片机制</p><ul><li>简单的按照文件的内容长度进行切片</li><li>切片大小，默认等于Block大小</li><li>切片时不考虑数据集整体，而是逐个针对每一个文件单独切片</li></ul><p><strong>注</strong>：每次切片，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片</p></li><li><p>源码中计算机切片大小的公式</p><ul><li>Math. max(minSize, Math.min(maxSize, blockSize));</li><li>maprecduce.input.fileinputformat.split.minsize=1默认值为1</li><li>mapreduce.input.fileinputformat.split maxsize=LongMAXValue 默认值Long.MAXValue,因此，默认情况下，切片大小=blocksize</li></ul></li><li><p>获取切片信息API</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//获取切片的文件名称</span></span><br><span class="line">String name=inputSplit.getPath（）.getName（）：</span><br><span class="line"><span class="comment">//根据文件类型获取切片信息</span></span><br><span class="line">FileSplit inputSplit =（FileSplit）context.get InputSplit（）；</span><br></pre></td></tr></table></figure></li></ol><h2 id="CombineTextInputFormat切片机制"><a href="#CombineTextInputFormat切片机制" class="headerlink" title="CombineTextInputFormat切片机制"></a>CombineTextInputFormat切片机制</h2><p>​ 默认的TextInputFormat切片机制是对任务按文件规划切片，会产生大量小文件，产生大量的MapTask，处理效率极其低下，故CombineTextInputFormat来处理大量小文件的情况。</p><ol><li><p>虚拟存储切片最大值设置</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CombineTextInputFormat. setMaxInputSplitSize(job,<span class="number">4194304</span>):<span class="comment">//4m-</span></span><br></pre></td></tr></table></figure></li><li><p>切片机制</p><p>生成切片过程包括：虚拟存储过程和切片过程二部分</p><ul><li><p>虚拟存储过程</p><blockquote><p>将目录下的所有文件与setMaxInputSize比较，小于则切分为一块，大于且小于两倍，则平分这一块，大于两倍，则切出一块setMaxInputSize的块</p></blockquote></li><li><p>切片过程</p><blockquote><p>判断虚拟存储文件大小是否大于setMaxInputSplitSize值，大于或等于则单独形成一个切片，否则和下一个切片进行合并，形成一个切片</p></blockquote></li></ul></li></ol><h2 id="CombineTextInputFormat案例实操"><a href="#CombineTextInputFormat案例实操" class="headerlink" title="CombineTextInputFormat案例实操"></a>CombineTextInputFormat案例实操</h2><p>需求：将输入的大量小文件合并成一个切片统一处理（以WordCount为基础），准备四个文件。</p><p>只需要在Driver中添加输入格式即可：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 如果不设置InputFormat，它默认用的是TextInputFormat.class</span></span><br><span class="line">job.setInputFormatClass(CombineTextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//虚拟存储切片最大值设置4m</span></span><br><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">4194304</span>);</span><br></pre></td></tr></table></figure><h2 id="FileInputFormat实现类"><a href="#FileInputFormat实现类" class="headerlink" title="FileInputFormat实现类"></a>FileInputFormat实现类</h2><p>​ <strong>FileInputFormat</strong> 常见的接口实现类包括：<strong>TextinputFormat、KeyValue TextInputFormat、NLinelnputFormat、CombineTextinputFormat，自定义InputFormat</strong>等。</p><ol><li><p>TextInputFormat</p><p>FileInputFile默认的实现类，按行读，键为字节偏移量，LongWritable类型</p></li><li><p>KeyValueTextInputFormat</p><p>通过分隔符，分为key，value，课通过设置</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置切割符</span></span><br><span class="line">conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, <span class="string">" "</span>);</span><br><span class="line"><span class="comment">// 设置输入格式</span></span><br><span class="line">job.setInputFormatClass(KeyValueTextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure></li><li><p>NlineInputFormat</p><p>按照行数N来划分，即切片数=输入文件总行数/N</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 7设置每个切片InputSplit中划分三条记录</span><br><span class="line">NLineInputFormat.setNumLinesPerSplit(job, 3);   </span><br><span class="line">&#x2F;&#x2F; 8使用NLineInputFormat处理记录数  </span><br><span class="line">job.setInputFormatClass(NLineInputFormat.class);</span><br></pre></td></tr></table></figure></li><li><p>自定义InputFormat</p><blockquote><p>需要自定义一个类继承FileInputFormat</p><p>改写RecordReader，实现封装为KV</p><p>输出是使用SequenceFileOutFormat输出合并文件</p></blockquote></li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Thu Feb 27 2020 18:40:43 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;切片与MapTask并行度决定机制&quot;&gt;&lt;a href=&quot;#切片与MapTask并行度决定机制&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="http://kiedeng.github.io/categories/Hadoop/"/>
    
    
      <category term="MapReduce" scheme="http://kiedeng.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>序列化案例</title>
    <link href="http://kiedeng.github.io/2020/02/16/%E5%BA%8F%E5%88%97%E5%8C%96%E6%A1%88%E4%BE%8B/"/>
    <id>http://kiedeng.github.io/2020/02/16/%E5%BA%8F%E5%88%97%E5%8C%96%E6%A1%88%E4%BE%8B/</id>
    <published>2020-02-16T07:08:00.000Z</published>
    <updated>2020-02-20T11:27:43.180Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Feb 21 2020 01:03:29 GMT+0800 (GMT+08:00) --><p><strong>需求</strong>：统计每一个手机号耗费的总上行流量，下行流量，总流量</p><p><strong>编写MapReduce程序</strong>：</p><ol><li><p>编写流量统计的Bean对象</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce.flowsum;</span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1 实现writable接口</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBean</span> <span class="keyword">implements</span> <span class="title">Writable</span></span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">long</span> upFlow;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">long</span> downFlow;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">long</span> sumFlow;</span><br><span class="line"></span><br><span class="line"><span class="comment">//2  反序列化时，需要反射调用空参构造函数，所以必须有</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">super</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">(<span class="keyword">long</span> upFlow, <span class="keyword">long</span> downFlow)</span> </span>&#123;</span><br><span class="line"><span class="keyword">super</span>();</span><br><span class="line"><span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line"><span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line"><span class="keyword">this</span>.sumFlow = upFlow + downFlow;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//3  写序列化方法</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">out.writeLong(upFlow);</span><br><span class="line">out.writeLong(downFlow);</span><br><span class="line">out.writeLong(sumFlow);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//4 反序列化方法</span></span><br><span class="line"><span class="comment">//5 反序列化方法读顺序必须和写序列化方法的写顺序必须一致</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.upFlow  = in.readLong();</span><br><span class="line"><span class="keyword">this</span>.downFlow = in.readLong();</span><br><span class="line"><span class="keyword">this</span>.sumFlow = in.readLong();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 6 编写toString方法，方便后续打印到文本</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> upFlow + <span class="string">"\t"</span> + downFlow + <span class="string">"\t"</span> + sumFlow;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getUpFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> upFlow;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUpFlow</span><span class="params">(<span class="keyword">long</span> upFlow)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getDownFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> downFlow;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setDownFlow</span><span class="params">(<span class="keyword">long</span> downFlow)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getSumFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> sumFlow;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSumFlow</span><span class="params">(<span class="keyword">long</span> sumFlow)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.sumFlow = sumFlow;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>编写Mapper类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce.flowsum;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">FlowBean v = <span class="keyword">new</span> FlowBean();</span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1 获取一行</span></span><br><span class="line">String line = value.toString();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 切割字段</span></span><br><span class="line">String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3 封装对象</span></span><br><span class="line"><span class="comment">// 取出手机号码</span></span><br><span class="line">String phoneNum = fields[<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line"><span class="comment">// 取出上行流量和下行流量</span></span><br><span class="line"><span class="keyword">long</span> upFlow = Long.parseLong(fields[fields.length - <span class="number">3</span>]);</span><br><span class="line"><span class="keyword">long</span> downFlow = Long.parseLong(fields[fields.length - <span class="number">2</span>]);</span><br><span class="line"></span><br><span class="line">k.set(phoneNum);</span><br><span class="line">v.set(downFlow, upFlow);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4 写出</span></span><br><span class="line">context.write(k, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>编写Reducer类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce.flowsum;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowCountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;FlowBean&gt; values, Context context)</span><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">long</span> sum_upFlow = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">long</span> sum_downFlow = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1 遍历所用bean，将其中的上行流量，下行流量分别累加</span></span><br><span class="line"><span class="keyword">for</span> (FlowBean flowBean : values) &#123;</span><br><span class="line">sum_upFlow += flowBean.getUpFlow();</span><br><span class="line">sum_downFlow += flowBean.getDownFlow();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 封装对象</span></span><br><span class="line">FlowBean resultBean = <span class="keyword">new</span> FlowBean(sum_upFlow, sum_downFlow);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3 写出</span></span><br><span class="line">context.write(key, resultBean);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>编写Driver驱动类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce.flowsum;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowsumDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IllegalArgumentException, IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输入输出路径需要根据自己电脑上实际的输入输出路径设置</span></span><br><span class="line">args = <span class="keyword">new</span> String[] &#123; <span class="string">"e:/input/inputflow"</span>, <span class="string">"e:/output1"</span> &#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1 获取配置信息，或者job对象实例</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 6 指定本程序的jar包所在的本地路径</span></span><br><span class="line">job.setJarByClass(FlowsumDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 指定本业务job要使用的mapper/Reducer业务类</span></span><br><span class="line">job.setMapperClass(FlowCountMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">job.setReducerClass(FlowCountReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3 指定mapper输出数据的kv类型</span></span><br><span class="line">job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">job.setMapOutputValueClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4 指定最终输出的数据的kv类型</span></span><br><span class="line">job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">job.setOutputValueClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5 指定job的输入原始文件所在目录</span></span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行</span></span><br><span class="line"><span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri Feb 21 2020 01:03:29 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;strong&gt;需求&lt;/strong&gt;：统计每一个手机号耗费的总上行流量，下行流量，总流量&lt;/p&gt;&lt;p&gt;&lt;strong&gt;编写MapReduce程
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="http://kiedeng.github.io/categories/Hadoop/"/>
    
    
      <category term="MapReduce" scheme="http://kiedeng.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>序列化概述</title>
    <link href="http://kiedeng.github.io/2020/02/16/%E5%BA%8F%E5%88%97%E5%8C%96%E6%A6%82%E8%BF%B0/"/>
    <id>http://kiedeng.github.io/2020/02/16/%E5%BA%8F%E5%88%97%E5%8C%96%E6%A6%82%E8%BF%B0/</id>
    <published>2020-02-16T07:06:59.000Z</published>
    <updated>2020-02-20T10:58:32.980Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Feb 21 2020 01:03:29 GMT+0800 (GMT+08:00) --><h3 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h3><p>​ 序列化就是把内存中的对象，转换为字节序列（或者其他数据传输协议）以便于存储到磁盘（持久化）和网络传输。</p><p>​ 反序列化就是将收到字节序列或者是磁盘的持久化数据，转换为内存中的对象</p><p><strong>原因：</strong>一般来说，“活的对象”只生产在内存里，关机断电就没有了，并且不能发送，然而序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机。</p><p>由于java序列化是一个重量级序列化框架（Serializable），序列化的时候，会附带很多额外的信息（各种校验信息，Header，继承体系等），不便于在网络中高效传输。故Hadoop开发了一套序列化机制(Writable)。</p><p>Hadoop序列化特点：</p><blockquote><p>紧凑：高效实用存储空间</p><p>快速：读写数据的额外开销小</p><p>可扩展：随着通信协议的升级而升级</p><p>互操作：支持多语言的交互</p></blockquote><h3 id="实现序列化接口（Writable）"><a href="#实现序列化接口（Writable）" class="headerlink" title="实现序列化接口（Writable）"></a>实现序列化接口（Writable）</h3><ol><li><p>必须实现Writable接口</p></li><li><p>反序列化时，需要反射调用空参构造函数，所以必须要有空参构造</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span></span>&#123;</span><br><span class="line"><span class="keyword">super</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>重写序列化方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">out.writeLong(upFlow);</span><br><span class="line">out.writeLong(downFlow);</span><br><span class="line">out.writeLong(sumFlow);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>重写反序列化方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">upFlow = in.readLong();</span><br><span class="line">downFlow = in.readLong();</span><br><span class="line">sumFlow = in.readLong();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>注意反序列化的顺序和序列化的顺序完全一致</p></li><li><p>重写toString（），可用“\t”分开，方便后续用</p></li><li><p>如果需要将自定义的bean放在key中传输，则还需要实现Comparable接口，因为MapReduce框中的Shuffle过程要求对key必须能排序。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean o)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 倒序排列，从大到小</span></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>.sumFlow &gt; o.getSumFlow() ? -<span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri Feb 21 2020 01:03:29 GMT+0800 (GMT+08:00) --&gt;&lt;h3 id=&quot;序列化&quot;&gt;&lt;a href=&quot;#序列化&quot; class=&quot;headerlink&quot; title=&quot;序列化&quot;&gt;&lt;/a&gt;序列化&lt;/h3&gt;&lt;p&gt;​
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="http://kiedeng.github.io/categories/Hadoop/"/>
    
    
      <category term="默认" scheme="http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"/>
    
  </entry>
  
  <entry>
    <title>centos安装MySQL</title>
    <link href="http://kiedeng.github.io/2020/02/16/centos%E5%AE%89%E8%A3%85MySQL/"/>
    <id>http://kiedeng.github.io/2020/02/16/centos%E5%AE%89%E8%A3%85MySQL/</id>
    <published>2020-02-16T02:14:41.000Z</published>
    <updated>2020-02-16T02:15:28.523Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Feb 21 2020 01:03:29 GMT+0800 (GMT+08:00) --><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri Feb 21 2020 01:03:29 GMT+0800 (GMT+08:00) --&gt;&lt;!-- rebuild by neat --&gt;
      
    
    </summary>
    
    
      <category term="MySQL" scheme="http://kiedeng.github.io/categories/MySQL/"/>
    
    
      <category term="部署" scheme="http://kiedeng.github.io/tags/%E9%83%A8%E7%BD%B2/"/>
    
  </entry>
  
  <entry>
    <title>WordCount案例</title>
    <link href="http://kiedeng.github.io/2020/02/15/WordCount%E6%A1%88%E4%BE%8B/"/>
    <id>http://kiedeng.github.io/2020/02/15/WordCount%E6%A1%88%E4%BE%8B/</id>
    <published>2020-02-15T07:23:09.000Z</published>
    <updated>2020-02-15T07:24:46.955Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Feb 15 2020 15:24:52 GMT+0800 (GMT+08:00) --><p><strong>需求：</strong>在给定的文本文件中输出每个单词出现的总次数</p><ol><li><p>创建maven工程</p></li><li><p>pom.xml文件中添加一下依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>2.8.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>在项目的资源目录下/src/main/resources目录下，新建一个文件，命名为“log4j.properties”</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">log4j.rootLogger=INFO, stdout</span><br><span class="line">log4j.appender.stdout=org.apache.log4j.ConsoleAppender</span><br><span class="line">log4j.appender.stdout.layout=org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n</span><br><span class="line">log4j.appender.logfile=org.apache.log4j.FileAppender</span><br><span class="line">log4j.appender.logfile.File=target/spring.log</span><br><span class="line">log4j.appender.logfile.layout=org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n</span><br></pre></td></tr></table></figure></li><li><p>编写程序</p><ol><li><p>编写mapper</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordcountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">Text k = <span class="keyword">new</span> Text();</span><br><span class="line">IntWritable v = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1 获取一行</span></span><br><span class="line">String line = value.toString();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 切割</span></span><br><span class="line">String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3 输出</span></span><br><span class="line"><span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line"></span><br><span class="line">k.set(word);</span><br><span class="line">context.write(k, v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>编写Reduce类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce.wordcount;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordcountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> sum;</span><br><span class="line">IntWritable v = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values,Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1 累加求和</span></span><br><span class="line">sum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (IntWritable count : values) &#123;</span><br><span class="line">sum += count.get();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 输出</span></span><br><span class="line">       v.set(sum);</span><br><span class="line">context.write(key,v);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>编写Driver驱动类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.mapreduce.wordcount;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordcountDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1 获取配置信息以及封装任务</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 设置jar加载路径</span></span><br><span class="line">job.setJarByClass(WordcountDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3 设置map和reduce类</span></span><br><span class="line">job.setMapperClass(WordcountMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">job.setReducerClass(WordcountReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4 设置map输出</span></span><br><span class="line">job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">job.setMapOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5 设置最终输出kv类型</span></span><br><span class="line">job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 6 设置输入和输出路径</span></span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 7 提交</span></span><br><span class="line"><span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>测试</p><ol><li><p>直接在eclipse/Idea上测试</p></li><li><p>集群上测试</p><ol><li><p>maven打jar包，需要添加打包插件依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin <span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">archive</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">manifest</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>com.atguigu.mr.WordcountDriver<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">manifest</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">archive</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如果工程显示红叉，项目右键-&gt;maven-&gt;update project即可</p></li><li><p>maven install，将jar包放到Hadoop集群</p></li><li><p>执行WordCount</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar  wc.jar</span><br><span class="line"> com.atguigu.wordcount.WordcountDriver /user/atguigu/input /user/atguigu/output</span><br></pre></td></tr></table></figure></li></ol></li></ol></li></ol></li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Feb 15 2020 15:24:52 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;strong&gt;需求：&lt;/strong&gt;在给定的文本文件中输出每个单词出现的总次数&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;创建maven工程&lt;/p&gt;&lt;/l
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="http://kiedeng.github.io/categories/Hadoop/"/>
    
      <category term="Mapreduce" scheme="http://kiedeng.github.io/categories/Hadoop/Mapreduce/"/>
    
    
      <category term="案例" scheme="http://kiedeng.github.io/tags/%E6%A1%88%E4%BE%8B/"/>
    
  </entry>
  
  <entry>
    <title>HDFS_2.X新特性</title>
    <link href="http://kiedeng.github.io/2020/02/14/HDFS-2-X%E6%96%B0%E7%89%B9%E6%80%A7/"/>
    <id>http://kiedeng.github.io/2020/02/14/HDFS-2-X%E6%96%B0%E7%89%B9%E6%80%A7/</id>
    <published>2020-02-13T17:46:00.000Z</published>
    <updated>2020-02-14T17:34:24.151Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Feb 15 2020 15:22:16 GMT+0800 (GMT+08:00) --><h3 id="集群间的数据拷贝"><a href="#集群间的数据拷贝" class="headerlink" title="集群间的数据拷贝"></a>集群间的数据拷贝</h3><ol><li><p>scp实现两个远程主机之间的文件复制</p><p>​ scp -r hello.txt <a href="mailto:root@hadoop103:/user/atguigu/hello.txt">root@hadoop103:/user/atguigu/hello.txt</a> // 推 push</p><p>​ scp -r [root@hadoop103:/user/atguigu/hello.txt hello.txt](mailto:root@hadoop103:/user/atguigu/hello.txt hello.txt) // 拉 pull</p><p>​ scp -r <a href="mailto:root@hadoop103:/user/atguigu/hello.txt">root@hadoop103:/user/atguigu/hello.txt</a> root@hadoop104:/user/atguigu //是通过本地主机中转实现两个远程主机的文件复制；如果在两个远程主机之间ssh没有配置的情况下可以使用该方式。</p></li><li><p>采用distcp命令实现两个Hadoop集群之间的递归数据复制</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> bin/hadoop distcp</span><br><span class="line">hdfs://haoop102:9000/user/atguigu/hello.txt hdfs://hadoop103:9000/user/atguigu/hello.txt</span><br></pre></td></tr></table></figure></li></ol><h3 id="小文件存档"><a href="#小文件存档" class="headerlink" title="小文件存档"></a>小文件存档</h3><ol><li><p>弊端</p><p>​ 每个文件按块存储，每个块的数据在NameNode的内存中，因此HDFS存储小文件会非常低效。因为大量小文件会用尽NameNode的大部分内存。</p></li><li><p>解决小文件的办法之一</p><p>​ HDFS存档文件或Har文件，是一个更高效的文件存档工具。HDFS存档文件对内还是一个一个独立文件，对NameNode而言是一个整体，减少NameNode内存</p></li><li><p>实例</p><ol><li><p>需要启动YARN进程</p></li><li><p>归档文件</p><p>​ 把/user/atguigu/input目录里面的所有文件归档成一个叫input.har的归档文件，并把归档后文件存储到/user/atguigu/output路径下。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hadoop archive -archiveName input.har –p  /user/atguigu/input   /user/atguigu/output</span><br></pre></td></tr></table></figure></li><li><p>查看归档</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -lsr /user/atguigu/output/input.har</span><br><span class="line">hadoop fs -lsr har:///user/atguigu/output/input.har</span><br></pre></td></tr></table></figure></li><li><p>解归档文件</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -cp har:/// user/atguigu/output/input.har/*    /user/atguigu</span><br></pre></td></tr></table></figure></li></ol></li></ol><h3 id="回收站"><a href="#回收站" class="headerlink" title="回收站"></a>回收站</h3><ol><li><p>开启回收站功能参数说明</p><ol><li>默认值fs.trash.interval=0,0表示禁用回收站</li><li>默认值fs.trash.checkpoint.interval=0,检查回收站的间隔时间。如果为0，则该值和fs.tarsh.interval的参数相等</li><li>要求fs.trash.checkpoint.interval&lt;=fs.trash.interval</li></ol></li><li><p>启动回收站</p><p>修改core-site.xml,配置垃圾回收时间为1分钟</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.trash.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>查看回收站</p><p>回收站在集群中的路径：/user/atguigu/.Trash/….</p></li><li><p>修改访问垃圾回收站用户名称</p><p>进入垃圾回收站用户名称，默认是dr.who，修改为atguigu用户</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>atguigu<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>调用moveToTrash（）才进入回收站</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Trash trash = New Trash(conf);</span><br><span class="line">trash.moveToTrash(path);</span><br></pre></td></tr></table></figure></li><li><p>恢复回收站数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mv</span><br><span class="line">/user/atguigu/.Trash/Current/user/atguigu/input    /user/atguigu/input</span><br></pre></td></tr></table></figure></li><li><p>清空回收站</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -expunge</span><br></pre></td></tr></table></figure></li></ol><h3 id="快照管理"><a href="#快照管理" class="headerlink" title="快照管理"></a>快照管理</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -allowSnapshot 路径（功能描述：开启指定目录的快照功能）</span><br><span class="line">hdfs dfsadmin -disallowSnapshot 路径（功能描述：禁用指定目录的快照功能，默认是禁用）</span><br><span class="line">hdfs dfs -createSnapshot 路径（功能描述：对目录创建快照）</span><br><span class="line">hdfs dfs -createSnapshot 路径名称（功能描述：指定名称创建快照）</span><br><span class="line">hdfs dfs -renameSnapshot 略径旧名称新名称（功能描述：重命名快照）</span><br><span class="line">hdfs lsSnapshottableDir（功能描述：列出当前用户所有可快照目录）</span><br><span class="line">hdfs snapshotDiff 路径1 路径2（功能描述：比较两个快照目录的不同之处）</span><br><span class="line">hdfs dfs -deleteSnapshot&lt;path&gt;&lt;snapshotName&gt;（功能描述：删除快照）</span><br></pre></td></tr></table></figure><ol><li><p>开启/禁用指定目录的快照功能</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -allowSnapshot /user/atguigu/input</span><br><span class="line">hdfs dfsadmin -disallowSnapshot /user/atguigu/input</span><br></pre></td></tr></table></figure></li><li><p>对目录创建快照</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -createSnapshot /user/atguigu/input</span><br><span class="line">hdfs dfs -lsr /user/atguigu/input/.snapshot/</span><br></pre></td></tr></table></figure></li><li><p>指定名称创建快照</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -createSnapshot /user/atguigu/input  miao170508</span><br></pre></td></tr></table></figure></li><li><p>快照重命名</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -renameSnapshot /user/atguigu/input/  miao170508 atguigu170508</span><br></pre></td></tr></table></figure></li><li><p>列出当前用户所有可快照目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs lsSnapshottableDir</span><br></pre></td></tr></table></figure></li><li><p>比较两个快照目录的不同之处</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs snapshotDiff /user/atguigu/input/  .  .snapshot/atguigu170508</span><br></pre></td></tr></table></figure></li><li><p>恢复快照</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -cp /user/atguigu/input/.snapshot/s20170708-134303.027 /user</span><br></pre></td></tr></table></figure></li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Feb 15 2020 15:22:16 GMT+0800 (GMT+08:00) --&gt;&lt;h3 id=&quot;集群间的数据拷贝&quot;&gt;&lt;a href=&quot;#集群间的数据拷贝&quot; class=&quot;headerlink&quot; title=&quot;集群间的数据拷贝&quot;&gt;&lt;
      
    
    </summary>
    
    
      <category term="HDFS" scheme="http://kiedeng.github.io/categories/HDFS/"/>
    
      <category term="Hadoop" scheme="http://kiedeng.github.io/categories/HDFS/Hadoop/"/>
    
    
      <category term="原理" scheme="http://kiedeng.github.io/tags/%E5%8E%9F%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>NN与DataNode</title>
    <link href="http://kiedeng.github.io/2020/02/14/NN%E4%B8%8EDataNode/"/>
    <id>http://kiedeng.github.io/2020/02/14/NN%E4%B8%8EDataNode/</id>
    <published>2020-02-13T17:21:19.000Z</published>
    <updated>2020-02-14T01:01:48.935Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Feb 15 2020 15:22:16 GMT+0800 (GMT+08:00) --><h2 id="NN和2NN工作机制"><a href="#NN和2NN工作机制" class="headerlink" title="NN和2NN工作机制"></a>NN和2NN工作机制</h2><p>FsImage:磁盘中备份元数据的文件</p><p>Edits：每当元数据有更新或者添加元数据时，修改内存中元数据并追加到Edits，为防止该文件数据过大影响效率，因此需要定期进行FsImage和Edits合并，引入一个节点SecondaryNamenode,专门用于FsImage和Edits的合并</p><h5 id="第一阶段：NameNode启动"><a href="#第一阶段：NameNode启动" class="headerlink" title="第一阶段：NameNode启动"></a>第一阶段：NameNode启动</h5><ol><li>第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。</li><li>客户端对元数据进行增删改的请求。</li><li>NameNode记录操作日志，更新滚动日志。</li><li>NameNode在内存中对数据进行增删改。</li></ol><h5 id="第二阶段：Secondary-NameNode工作"><a href="#第二阶段：Secondary-NameNode工作" class="headerlink" title="第二阶段：Secondary NameNode工作"></a>第二阶段：Secondary NameNode工作</h5><ol><li>Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。</li><li>Secondary NameNode请求执行CheckPoint。</li><li>NameNode滚动正在写的Edits日志。</li><li>将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。</li><li>Secondary NameNode加载编辑日志和镜像文件到内存，并合并。</li><li>生成新的镜像文件fsimage.chkpoint。</li><li>拷贝fsimage.chkpoint到NameNode。</li><li>NameNode将fsimage.chkpoint重新命名成fsimage。</li></ol><p><strong>* opt/module/hadoop-2.7.2/data/tmp/dfs/name/current *</strong></p><h4 id="oiv查看Fsimage"><a href="#oiv查看Fsimage" class="headerlink" title="oiv查看Fsimage"></a>oiv查看Fsimage</h4><p>语法：hdfs oiv -p 文件类型 -i镜像文件 -o 转换后文件输出路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs oiv -p XML -i fsimage_0000000000000000025 -o /opt/module/hadoop-2.7.2/fsimage.xml</span><br></pre></td></tr></table></figure><h4 id="oev查看Edits文件"><a href="#oev查看Edits文件" class="headerlink" title="oev查看Edits文件"></a>oev查看Edits文件</h4><p>语法：hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-2.7.2/edits.xml</span><br></pre></td></tr></table></figure><h4 id="CheckPoint时间设置"><a href="#CheckPoint时间设置" class="headerlink" title="CheckPoint时间设置"></a>CheckPoint时间设置</h4><ol><li><p>通常情况下，SecondaryNameNade每隔一个小时执行一次，在【hdfs-default.xml】中设置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>一分钟检查一次操作次数，3当操作次数达到1百万时，<br>SecondaryNameNode执行一次。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>操作动作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span> 1分钟检查一次操作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span> &gt;</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="NameNode故障处理"><a href="#NameNode故障处理" class="headerlink" title="NameNode故障处理"></a>NameNode故障处理</h2><p><strong>方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录</strong></p><ol><li><p>kill -9 NameNode过程</p></li><li><p>删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name）</p></li><li><p>拷贝SecondaryNameNode中数据到原NameNode存储数据目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -r atguigu@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* ./name/</span><br></pre></td></tr></table></figure></li><li><p>重新启动NameNode</p></li></ol><p><strong>方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中。</strong></p><ol><li><p>修改hdfs-site.xml中的</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>120<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data/tmp/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>kill -9 NameNode进程</p></li><li><p>删除NameNode存储的数据</p></li><li><p>将2NN存储数据的目录拷贝到NN存储数据的评级目录，并删除in_use.lock文件</p></li><li><p>导入检查点数据</p><blockquote><p>sbin/hadoop-daemon.sh start namenode</p></blockquote></li><li><p>启动NameNode</p><blockquote><p>sbin/hadoop-daemon.sh start namenode</p></blockquote></li></ol><h4 id="安全模式"><a href="#安全模式" class="headerlink" title="安全模式"></a>安全模式</h4><p>（1）bin/hdfs dfsadmin -safemode get （功能描述：查看安全模式状态）</p><p>（2）bin/hdfs dfsadmin -safemode enter （功能描述：进入安全模式状态）</p><p>（3）bin/hdfs dfsadmin -safemode leave （功能描述：离开安全模式状态）</p><p>（4）bin/hdfs dfsadmin -safemode wait （功能描述：等待安全模式状态）</p><h2 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h2><h3 id="DataNode工作机制"><a href="#DataNode工作机制" class="headerlink" title="DataNode工作机制"></a>DataNode工作机制</h3><ol><li>一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。</li><li>DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。</li><li>心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。</li><li>集群运行中可以安全加入和退出一些机器。</li></ol><h3 id="保证DataNode的数据完整性"><a href="#保证DataNode的数据完整性" class="headerlink" title="保证DataNode的数据完整性"></a>保证DataNode的数据完整性</h3><p>1）当DataNode读取Block的时候，它会计算CheckSum。</p><p>2）如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。</p><p>3）Client读取其他DataNode上的Block。</p><p>4）DataNode在其文件创建后周期验证CheckSum</p><h3 id="掉线时限参数设置"><a href="#掉线时限参数设置" class="headerlink" title="掉线时限参数设置"></a>掉线时限参数设置</h3><blockquote><p>如果定义超时时间为TimeOut，则超时时长的计算公式为：<br>TimeOut =2<em>dfs.namenode.heartbeat.recheck-interval+10</em>dfs.heartbeat interval。而默认的dfs.namenode.heartbeat.recheck-interval 大小为5分钟，dfs.heartbeatinterval默认为3秒。</p></blockquote><p>配置位置：hdfs-site.xml，配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.heartbeat.recheck-interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>300000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.heartbeat.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="服役新节点"><a href="#服役新节点" class="headerlink" title="服役新节点"></a>服役新节点</h3><ol><li>环境准备</li></ol><p>​ （1）在hadoop104主机上再克隆一台hadoop105主机</p><p>​ （2）修改IP地址和主机名称</p><p>​ （3）删除原来HDFS文件系统留存的文件（/opt/module/hadoop-2.7.2/data和log）</p><p>​ （4）source一下配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sbin&#x2F;hadoop-daemon.sh start datanode</span><br><span class="line">sbin&#x2F;yarn-daemon.sh start nodemanager</span><br></pre></td></tr></table></figure><ol start="2"><li><p>添加白名单</p><ol><li><p>在NameNode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts文件，将允许的集群IP写入</p></li><li><p>在NameNode的hdfs-site.xml配置文件中增加dfs.hosts属性</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>配置文件分发</p></li><li><p>刷新namenode：hdfs dfsadmin -refreshNodes</p></li><li><p>更新ResourceManager节点：yarn rmadmin -refreshNodes</p></li><li><p>数据如果不平衡，则：start-balancer.sh</p></li></ol></li><li><p>添加黑名单（不能同时有节点出现在两个中）</p><ol><li><p>在NameNode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts.exclude文件</p></li><li><p>在NameNode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts.exclude<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>刷新namenode和Resourcemanager</p></li></ol></li></ol><h3 id="Datanode多目录配置"><a href="#Datanode多目录配置" class="headerlink" title="Datanode多目录配置"></a>Datanode多目录配置</h3><ol><li><p>DataNode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本</p></li><li><p>hdfs-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Feb 15 2020 15:22:16 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;NN和2NN工作机制&quot;&gt;&lt;a href=&quot;#NN和2NN工作机制&quot; class=&quot;headerlink&quot; title=&quot;NN和2NN工
      
    
    </summary>
    
    
      <category term="HDFS" scheme="http://kiedeng.github.io/categories/HDFS/"/>
    
      <category term="Hadoop" scheme="http://kiedeng.github.io/categories/HDFS/Hadoop/"/>
    
    
      <category term="原理" scheme="http://kiedeng.github.io/tags/%E5%8E%9F%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>HDFS的数据流</title>
    <link href="http://kiedeng.github.io/2020/02/13/HDFS%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B5%81/"/>
    <id>http://kiedeng.github.io/2020/02/13/HDFS%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B5%81/</id>
    <published>2020-02-13T10:03:05.000Z</published>
    <updated>2020-02-13T17:20:15.945Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Feb 15 2020 15:22:16 GMT+0800 (GMT+08:00) --><h3 id="HDFS写数据流程"><a href="#HDFS写数据流程" class="headerlink" title="HDFS写数据流程"></a>HDFS写数据流程</h3><ol><li>客户端通过Distributed FileSysetm模块向NameNode请求上传文件，NameNode检查文件是否已存在，父目录是否存在</li><li>NameNode返回是否可以上传</li><li>客户端请求第一个Block上传哪几个DataNode服务器上</li><li>NameNode返回三个DataNode节点，分别为dn1，dn2，dn3</li><li>客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成</li><li>dn1，dn2，dn3逐级应答客户端</li><li>客户端开始往dn1上传第一个Block（先从磁盘读取数据收到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传个dn3，dn1没传一个packet会放入一个应答队列等待应答</li><li>当一个Bloc传输完成之后，客户端再次请求NameNode上传第二个Block服务器</li></ol><h3 id="节点距离：两个节点到达最近的共同祖先的距离总和。"><a href="#节点距离：两个节点到达最近的共同祖先的距离总和。" class="headerlink" title="节点距离：两个节点到达最近的共同祖先的距离总和。"></a>节点距离：两个节点到达最近的共同祖先的距离总和。</h3><h3 id="机架感知"><a href="#机架感知" class="headerlink" title="机架感知"></a>机架感知</h3><blockquote><p>对于常见情况，当复制因子为3时，HDFS的放置策略是将一个副本放在本地机架中的一个节点上，另一个放在本地机架中的另一个节点上，最后一个放在不同机架中的另一个节点上</p></blockquote><h3 id="HDFS读数据流程"><a href="#HDFS读数据流程" class="headerlink" title="HDFS读数据流程"></a>HDFS读数据流程</h3><ol><li>客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。</li><li>挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据</li><li>DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。</li><li>客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。</li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Feb 15 2020 15:22:16 GMT+0800 (GMT+08:00) --&gt;&lt;h3 id=&quot;HDFS写数据流程&quot;&gt;&lt;a href=&quot;#HDFS写数据流程&quot; class=&quot;headerlink&quot; title=&quot;HDFS写数据流程
      
    
    </summary>
    
    
      <category term="HDFS" scheme="http://kiedeng.github.io/categories/HDFS/"/>
    
      <category term="Hadoop" scheme="http://kiedeng.github.io/categories/HDFS/Hadoop/"/>
    
    
      <category term="原理" scheme="http://kiedeng.github.io/tags/%E5%8E%9F%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>HDFS客户端操作</title>
    <link href="http://kiedeng.github.io/2020/02/13/HDFS%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%93%8D%E4%BD%9C/"/>
    <id>http://kiedeng.github.io/2020/02/13/HDFS%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%93%8D%E4%BD%9C/</id>
    <published>2020-02-13T03:16:18.000Z</published>
    <updated>2020-02-13T10:05:11.257Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Feb 15 2020 15:22:16 GMT+0800 (GMT+08:00) --><h2 id="客户端环境准备"><a href="#客户端环境准备" class="headerlink" title="客户端环境准备"></a>客户端环境准备</h2><ol><li><p>将编译hadoop jar包放在路径下（比如：D:\environment\hadoop-2.7.2）</p></li><li><p>配置HADOOP_HOME环境变量</p><blockquote><p>HADOOP_HOME : D:\environment\hadoop-2.7.2</p></blockquote></li><li><p>配置Path环境变量</p><blockquote><p>%HADOOP_HOME%\bin</p></blockquote></li><li><p>创建一个Maven工程</p></li><li><p>导入相应的依赖坐标+日志添加</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span> <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span> <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.atguigu<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>HDFS-0529<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.0.1-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>2.8.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>jdk.tools<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jdk.tools<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">scope</span>&gt;</span>system<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">systemPath</span>&gt;</span>$&#123;JAVA_HOME&#125;/lib/tools.jar<span class="tag">&lt;/<span class="name">systemPath</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line">  </span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ol><p>需要再项目的src/main/resources目录下，新建一个文件，命令为”log4j.properties”,在文件中填写</p><blockquote><p>log4j.rootLogger=INFO, stdout</p><p>log4j.appender.stdout=org.apache.log4j.ConsoleAppender</p><p>log4j.appender.stdout.layout=org.apache.log4j.PatternLayout</p><p>log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n</p><p>log4j.appender.logfile=org.apache.log4j.FileAppender</p><p>log4j.appender.logfile.File=target/spring.log</p><p>log4j.appender.logfile.layout=org.apache.log4j.PatternLayout</p><p>log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n</p></blockquote><ol start="6"><li><p>创建包名：com.atguigu.hdfs</p></li><li><p>创建HdfsClient类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HdfsClient</span></span>&#123;</span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testMkdirs</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1 获取文件系统</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line"><span class="comment">// 配置在集群上运行</span></span><br><span class="line"><span class="comment">// configuration.set("fs.defaultFS", "hdfs://hadoop102:9000");</span></span><br><span class="line"><span class="comment">// FileSystem fs = FileSystem.get(configuration);</span></span><br><span class="line"></span><br><span class="line">FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:9000"</span>), configuration, <span class="string">"atguigu"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 创建目录</span></span><br><span class="line">fs.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/1108/daxian/banzhang"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3 关闭资源</span></span><br><span class="line">fs.close();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>执行程序</p><blockquote><p>客户端去操作HDFS时，是有一个用户身份的。</p><p>默认情况下，HDFS客户端API会从JVM中获取一个参数来作为自己的用户身份：-DHADOOP_USER_NAME=atguigu，atguigu为用户名称。</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// 配置文件</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://hadoop102:9000"</span>);</span><br><span class="line"><span class="comment">// 获取客户端对象</span></span><br><span class="line">FileSystem fs = FileSystem.get(conf);</span><br><span class="line">fs.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/kangdong"</span>));</span><br><span class="line"><span class="comment">// 关闭客户端</span></span><br><span class="line">fs.close();</span><br><span class="line">System.out.println(<span class="string">"Over!"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h2 id="HDFS的API操作"><a href="#HDFS的API操作" class="headerlink" title="HDFS的API操作"></a>HDFS的API操作</h2><h3 id="HDFS文件上传（测试参数优先级）"><a href="#HDFS文件上传（测试参数优先级）" class="headerlink" title="HDFS文件上传（测试参数优先级）"></a>HDFS文件上传（测试参数优先级）</h3><ol><li><p>编写源代码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// 配置文件</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://hadoop102:9000"</span>);</span><br><span class="line"><span class="comment">// 获取客户端对象</span></span><br><span class="line">FileSystem fs = FileSystem.get(conf);</span><br><span class="line">fs.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/kangdong"</span>));</span><br><span class="line"><span class="comment">// 关闭客户端</span></span><br><span class="line">fs.close();</span><br><span class="line">System.out.println(<span class="string">"Over!"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>将hdfs-site.xml拷贝到项目的根目录（即资源目录下）</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>参数优先级</p><blockquote><p>参数优先级排序：（1）客户端代码中设置的值 &gt;（2）ClassPath下的用户自定义配置文件 &gt;（3）然后是服务器的默认配置</p></blockquote></li></ol><h3 id="HDFS文件的下载"><a href="#HDFS文件的下载" class="headerlink" title="HDFS文件的下载"></a>HDFS文件的下载</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// boolean delSrc 指是否将原文件删除</span></span><br><span class="line"><span class="comment">// Path src 指要下载的文件路径</span></span><br><span class="line"><span class="comment">// Path dst 指将文件下载到的路径</span></span><br><span class="line"><span class="comment">// boolean useRawLocalFileSystem 是否开启文件校验</span></span><br><span class="line">fs.copyToLocalFile(<span class="keyword">false</span>, <span class="keyword">new</span> Path(<span class="string">"/banzhang.txt"</span>), <span class="keyword">new</span> Path(<span class="string">"e:/banhua.txt"</span>), <span class="keyword">true</span>);</span><br></pre></td></tr></table></figure><h3 id="HDFS文件的删除"><a href="#HDFS文件的删除" class="headerlink" title="HDFS文件的删除"></a>HDFS文件的删除</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fs.delete(<span class="keyword">new</span> Path(<span class="string">"/0508/"</span>), <span class="keyword">true</span>);<span class="comment">//第一参数是路径，第二个参数是判断是否递归删除</span></span><br></pre></td></tr></table></figure><h3 id="HDFS文件的改名"><a href="#HDFS文件的改名" class="headerlink" title="HDFS文件的改名"></a>HDFS文件的改名</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fs.rename(<span class="keyword">new</span> Path(<span class="string">"/banzhang.txt"</span>), <span class="keyword">new</span> Path(<span class="string">"/banhua.txt"</span>));</span><br></pre></td></tr></table></figure><h3 id="HDFS文件详情的查看"><a href="#HDFS文件详情的查看" class="headerlink" title="HDFS文件详情的查看"></a>HDFS文件详情的查看</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testListFiles</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1获取文件系统</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:9000"</span>), configuration, <span class="string">"atguigu"</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 获取文件详情</span></span><br><span class="line">RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(<span class="keyword">new</span> Path(<span class="string">"/"</span>), <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span>(listFiles.hasNext())&#123;</span><br><span class="line">LocatedFileStatus status = listFiles.next();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出详情</span></span><br><span class="line"><span class="comment">// 文件名称</span></span><br><span class="line">System.out.println(status.getPath().getName());</span><br><span class="line"><span class="comment">// 长度</span></span><br><span class="line">System.out.println(status.getLen());</span><br><span class="line"><span class="comment">// 权限</span></span><br><span class="line">System.out.println(status.getPermission());</span><br><span class="line"><span class="comment">// 分组</span></span><br><span class="line">System.out.println(status.getGroup());</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取存储的块信息</span></span><br><span class="line">BlockLocation[] blockLocations = status.getBlockLocations();</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (BlockLocation blockLocation : blockLocations) &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取块存储的主机节点</span></span><br><span class="line">String[] hosts = blockLocation.getHosts();</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (String host : hosts) &#123;</span><br><span class="line">System.out.println(host);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">System.out.println(<span class="string">"-----------班长的分割线----------"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3 关闭资源</span></span><br><span class="line">fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="HDFS的I-O流操作"><a href="#HDFS的I-O流操作" class="headerlink" title="HDFS的I/O流操作"></a>HDFS的I/O流操作</h2><h3 id="HDFS文件上传"><a href="#HDFS文件上传" class="headerlink" title="HDFS文件上传"></a>HDFS文件上传</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">IOcopyFromLocal</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:9000"</span>), conf, <span class="string">"atguigu"</span>);</span><br><span class="line">FileInputStream fis = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(<span class="string">"d:/kiedeng.txt"</span>));</span><br><span class="line">FSDataOutputStream fout = fs.create(<span class="keyword">new</span> Path(<span class="string">"/kg.txt"</span>));</span><br><span class="line">IOUtils.copyBytes(fis, fout, conf);</span><br><span class="line"></span><br><span class="line">IOUtils.closeStream(fis);</span><br><span class="line">IOUtils.closeStream(fout);</span><br><span class="line">fs.close();</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="HDFS文件下载"><a href="#HDFS文件下载" class="headerlink" title="HDFS文件下载"></a>HDFS文件下载</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 文件下载</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getFileFromHDFS</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1 获取文件系统</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:9000"</span>), configuration, <span class="string">"atguigu"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 获取输入流</span></span><br><span class="line">FSDataInputStream fis = fs.open(<span class="keyword">new</span> Path(<span class="string">"/banhua.txt"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3 获取输出流</span></span><br><span class="line">FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">"e:/banhua.txt"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4 流的对拷</span></span><br><span class="line">IOUtils.copyBytes(fis, fos, configuration);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5 关闭资源</span></span><br><span class="line">IOUtils.closeStream(fos);</span><br><span class="line">IOUtils.closeStream(fis);</span><br><span class="line">fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="定位文件读取"><a href="#定位文件读取" class="headerlink" title="定位文件读取"></a>定位文件读取</h3><ol><li><p>下载第一块</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFileSeek1</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1 获取文件系统</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:9000"</span>), configuration, <span class="string">"atguigu"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 获取输入流</span></span><br><span class="line">FSDataInputStream fis = fs.open(<span class="keyword">new</span> Path(<span class="string">"/hadoop-2.7.2.tar.gz"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3 创建输出流</span></span><br><span class="line">FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">"e:/hadoop-2.7.2.tar.gz.part1"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4 流的拷贝</span></span><br><span class="line"><span class="keyword">byte</span>[] buf = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1024</span>];</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i =<span class="number">0</span> ; i &lt; <span class="number">1024</span> * <span class="number">128</span>; i++)&#123;</span><br><span class="line">fis.read(buf);</span><br><span class="line">fos.write(buf);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5关闭资源</span></span><br><span class="line">IOUtils.closeStream(fis);</span><br><span class="line">IOUtils.closeStream(fos);</span><br><span class="line">fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>下载第二块</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFileSeek2</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1 获取文件系统</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:9000"</span>), configuration, <span class="string">"atguigu"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 打开输入流</span></span><br><span class="line">FSDataInputStream fis = fs.open(<span class="keyword">new</span> Path(<span class="string">"/hadoop-2.7.2.tar.gz"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3 定位输入数据位置</span></span><br><span class="line">fis.seek(<span class="number">1024</span>*<span class="number">1024</span>*<span class="number">128</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4 创建输出流</span></span><br><span class="line">FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">"e:/hadoop-2.7.2.tar.gz.part2"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5 流的对拷</span></span><br><span class="line">IOUtils.copyBytes(fis, fos, configuration);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 6 关闭资源</span></span><br><span class="line">IOUtils.closeStream(fis);</span><br><span class="line">IOUtils.closeStream(fos);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="3"><li><p>合并文件</p><blockquote><p>​ 在Window命令窗口中进入到目录E:\，然后执行如下命令，对数据进行合并</p><p>​ type hadoop-2.7.2.tar.gz.part2 &gt;&gt; hadoop-2.7.2.tar.gz.part1</p><p>合并完成后，将hadoop-2.7.2.tar.gz.part1重新命名为hadoop-2.7.2.tar.gz。解压发现该tar包非常完整。</p></blockquote></li></ol></li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Feb 15 2020 15:22:16 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;客户端环境准备&quot;&gt;&lt;a href=&quot;#客户端环境准备&quot; class=&quot;headerlink&quot; title=&quot;客户端环境准备&quot;&gt;&lt;/a&gt;
      
    
    </summary>
    
    
      <category term="HDFS" scheme="http://kiedeng.github.io/categories/HDFS/"/>
    
      <category term="Hadoop" scheme="http://kiedeng.github.io/categories/HDFS/Hadoop/"/>
    
    
      <category term="代码" scheme="http://kiedeng.github.io/tags/%E4%BB%A3%E7%A0%81/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop编译源码</title>
    <link href="http://kiedeng.github.io/2020/02/12/Hadoop%E7%BC%96%E8%AF%91%E6%BA%90%E7%A0%81/"/>
    <id>http://kiedeng.github.io/2020/02/12/Hadoop%E7%BC%96%E8%AF%91%E6%BA%90%E7%A0%81/</id>
    <published>2020-02-12T03:29:53.000Z</published>
    <updated>2020-02-13T02:02:58.820Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Feb 15 2020 15:22:16 GMT+0800 (GMT+08:00) --><h2 id="jar包准备"><a href="#jar包准备" class="headerlink" title="jar包准备"></a>jar包准备</h2><ol><li>hadoop-2.7.2-src.tar.gz</li><li>jdk-8u144-linux-x64.tar.gz</li><li>apache-ant-1.9.9-bin.tar.gz（build工具，打包用的）</li><li>apache-maven-3.0.5-bin.tar.gz</li><li>protobuf-2.5.0.tar.gz（序列化的框架）</li></ol><h2 id="jar包安装"><a href="#jar包安装" class="headerlink" title="jar包安装"></a>jar包安装</h2><ol><li><p>JDK解压，配置环境变量JAVA_HOME和PATH</p><blockquote><p>#JAVA_HOME：</p><p>export JAVA_HOME=/opt/module/jdk1.8.0_144</p><p>export PATH=$PATH:$JAVA_HOME/bin</p></blockquote><p>source /etc/profile进行生效</p></li><li><p>Maven解压，配置MAVEN_HOME和PATH</p><blockquote><p>[root@hadoop101 software]# tar -zxvf apache-maven-3.0.5-bin.tar.gz -C /opt/module/</p><p>[root@hadoop101 apache-maven-3.0.5]# vi conf/settings.xml</p><mirrors><p>​<mirror></mirror></p><p>​<id>nexus-aliyun</id></p><p>​<mirrorof>central</mirrorof></p><p>​<name>Nexus aliyun</name></p><p>​<url><a href="http://maven.aliyun.com/nexus/content/groups/public" target="_blank" rel="noopener">http://maven.aliyun.com/nexus/content/groups/public</a></url></p><p>​</p></mirrors><p>[root@hadoop101 apache-maven-3.0.5]# vi /etc/profile</p><p>#MAVEN_HOME</p><p>export MAVEN_HOME=/opt/module/apache-maven-3.0.5</p><p>export PATH=$PATH:$MAVEN_HOME/bin</p><p>[root@hadoop101 software]#source /etc/profile</p></blockquote><p><strong>验证命令：mvn-version</strong></p></li><li><p>ant解压、配置 ANT _HOME和PATH</p><blockquote><p>[root@hadoop101 software]# tar -zxvf apache-ant-1.9.9-bin.tar.gz -C /opt/module/</p><p>[root@hadoop101 apache-ant-1.9.9]# vi /etc/profile</p><p>#ANT_HOME</p><p>export ANT_HOME=/opt/module/apache-ant-1.9.9</p><p>export PATH=$PATH:$ANT_HOME/bi</p><p>[root@hadoop101 software]#source /etc/profile</p></blockquote></li><li><p>安装 glibc-headers 和 g++ 命令如下</p><blockquote><p>[root@hadoop101 apache-ant-1.9.9]# yum install glibc-headers</p><p>[root@hadoop101 apache-ant-1.9.9]# yum install gcc-c++</p></blockquote></li><li><p>安装make和cmake</p><blockquote><p>[root@hadoop101 apache-ant-1.9.9]# yum install make</p><p>[root@hadoop101 apache-ant-1.9.9]# yum install cmake</p></blockquote></li><li><p>解压protobuf ，进入到解压后protobuf主目录，/opt/module/protobuf-2.5.0，然后相继执行命令</p><blockquote><p>[root@hadoop101 software]# tar -zxvf protobuf-2.5.0.tar.gz -C /opt/module/</p><p>[root@hadoop101 opt]# cd /opt/module/protobuf-2.5.0/</p><p>[root@hadoop101 protobuf-2.5.0]#./configure</p><p>[root@hadoop101 protobuf-2.5.0]# make</p><p>[root@hadoop101 protobuf-2.5.0]# make check</p><p>[root@hadoop101 protobuf-2.5.0]# make install</p><p>[root@hadoop101 protobuf-2.5.0]# ldconfig</p><p>[root@hadoop101 hadoop-dist]# vi /etc/profile</p><p>#LD_LIBRARY_PATH</p><p>export LD_LIBRARY_PATH=/opt/module/protobuf-2.5.0</p><p>export PATH=$PATH:$LD_LIBRARY_PATH</p><p>[root@hadoop101 software]#source /etc/profile</p></blockquote></li><li><p>安装openssl库</p><blockquote><p>[root@hadoop101 software]#yum install openssl-devel</p></blockquote></li><li><p>安装 ncurses-devel库</p><blockquote><p>[root@hadoop101 software]#yum install ncurses-devel</p></blockquote></li></ol><h2 id="编译源码"><a href="#编译源码" class="headerlink" title="编译源码"></a>编译源码</h2><ol><li><p>解压源码到/opt/目录</p><blockquote><p>tar -zxvf hadoop-2.7.2-src.tar.gz -C /opt/</p></blockquote></li><li><p>进入源码主目录</p></li><li><p>通过maven执行编译命令</p><blockquote><p>mvn package -Pdist,native -DskipTests -Dtar</p></blockquote></li><li><p>成功的64位hadoop包在/opt/hadoop-2.7.2-src/hadoop-dist/target下</p></li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Feb 15 2020 15:22:16 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;jar包准备&quot;&gt;&lt;a href=&quot;#jar包准备&quot; class=&quot;headerlink&quot; title=&quot;jar包准备&quot;&gt;&lt;/a&gt;jar
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="http://kiedeng.github.io/categories/Hadoop/"/>
    
    
      <category term="默认" scheme="http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"/>
    
  </entry>
  
  <entry>
    <title>软件推荐</title>
    <link href="http://kiedeng.github.io/2020/02/11/%E8%BD%AF%E4%BB%B6%E6%8E%A8%E8%8D%90/"/>
    <id>http://kiedeng.github.io/2020/02/11/%E8%BD%AF%E4%BB%B6%E6%8E%A8%E8%8D%90/</id>
    <published>2020-02-11T10:48:17.000Z</published>
    <updated>2020-02-11T11:25:52.268Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sat Feb 15 2020 15:22:16 GMT+0800 (GMT+08:00) --><h3 id="TinyTask"><a href="#TinyTask" class="headerlink" title="TinyTask"></a>TinyTask</h3><p>自动化操作工具</p><h3 id="Ditto"><a href="#Ditto" class="headerlink" title="Ditto"></a>Ditto</h3><p>复制神器</p><h3 id="SecureCRT"><a href="#SecureCRT" class="headerlink" title="SecureCRT"></a>SecureCRT</h3><p>一般大数据使用的远程工具</p><h3 id="PanDownload"><a href="#PanDownload" class="headerlink" title="PanDownload"></a>PanDownload</h3><p>网盘下载</p><h3 id="天若OCR文字识别"><a href="#天若OCR文字识别" class="headerlink" title="天若OCR文字识别"></a>天若OCR文字识别</h3><p>用于文字识别</p><h3 id="Snipaste"><a href="#Snipaste" class="headerlink" title="Snipaste"></a>Snipaste</h3><p>截图贴图工具</p><h3 id="Typora"><a href="#Typora" class="headerlink" title="Typora"></a>Typora</h3><p>markdown编辑器</p><h3 id="Notepad"><a href="#Notepad" class="headerlink" title="Notepad++"></a>Notepad++</h3><p>编辑器，可以进行文件夹文字查找</p><h3 id="Sublime"><a href="#Sublime" class="headerlink" title="Sublime"></a>Sublime</h3><p>编辑器，可以远程服务器</p><h3 id="Bandizip"><a href="#Bandizip" class="headerlink" title="Bandizip"></a>Bandizip</h3><p>解压工具</p><h3 id="VMware"><a href="#VMware" class="headerlink" title="VMware"></a>VMware</h3><p>虚拟机</p><h3 id="Xshell-Xftp"><a href="#Xshell-Xftp" class="headerlink" title="Xshell/Xftp"></a>Xshell/Xftp</h3><p>远程连接工具</p><h3 id="Adobe-Acrobat-DC"><a href="#Adobe-Acrobat-DC" class="headerlink" title="Adobe Acrobat DC"></a>Adobe Acrobat DC</h3><p>pdf阅读，编辑</p><h3 id="Anki"><a href="#Anki" class="headerlink" title="Anki"></a>Anki</h3><p>记忆工具</p><h3 id="Everything"><a href="#Everything" class="headerlink" title="Everything"></a>Everything</h3><p>查看文件</p><h3 id="PotPlayer"><a href="#PotPlayer" class="headerlink" title="PotPlayer"></a>PotPlayer</h3><p>播放器</p><h3 id="Chrome"><a href="#Chrome" class="headerlink" title="Chrome"></a>Chrome</h3><p>浏览器</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sat Feb 15 2020 15:22:16 GMT+0800 (GMT+08:00) --&gt;&lt;h3 id=&quot;TinyTask&quot;&gt;&lt;a href=&quot;#TinyTask&quot; class=&quot;headerlink&quot; title=&quot;TinyTask&quot;&gt;&lt;
      
    
    </summary>
    
    
      <category term="工具" scheme="http://kiedeng.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="自动化" scheme="http://kiedeng.github.io/tags/%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    
  </entry>
  
</feed>
