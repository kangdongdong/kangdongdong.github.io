<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://kiedeng.github.io/"/>
  <updated>2020-02-13T17:46:00.368Z</updated>
  <id>http://kiedeng.github.io/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>HDFS_2.X新特性</title>
    <link href="http://kiedeng.github.io/2020/02/14/HDFS-2-X%E6%96%B0%E7%89%B9%E6%80%A7/"/>
    <id>http://kiedeng.github.io/2020/02/14/HDFS-2-X%E6%96%B0%E7%89%B9%E6%80%A7/</id>
    <published>2020-02-13T17:46:00.000Z</published>
    <updated>2020-02-13T17:46:00.368Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Feb 14 2020 01:46:27 GMT+0800 (GMT+08:00) --><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri Feb 14 2020 01:46:27 GMT+0800 (GMT+08:00) --&gt;&lt;!-- rebuild by neat --&gt;
      
    
    </summary>
    
    
      <category term="未分类" scheme="http://kiedeng.github.io/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/"/>
    
    
      <category term="默认" scheme="http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"/>
    
  </entry>
  
  <entry>
    <title>NN与DataNode</title>
    <link href="http://kiedeng.github.io/2020/02/14/NN%E4%B8%8EDataNode/"/>
    <id>http://kiedeng.github.io/2020/02/14/NN%E4%B8%8EDataNode/</id>
    <published>2020-02-13T17:21:19.000Z</published>
    <updated>2020-02-14T01:01:48.935Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Feb 14 2020 09:02:06 GMT+0800 (GMT+08:00) --><h2 id="NN和2NN工作机制"><a href="#NN和2NN工作机制" class="headerlink" title="NN和2NN工作机制"></a>NN和2NN工作机制</h2><p>FsImage:磁盘中备份元数据的文件</p><p>Edits：每当元数据有更新或者添加元数据时，修改内存中元数据并追加到Edits，为防止该文件数据过大影响效率，因此需要定期进行FsImage和Edits合并，引入一个节点SecondaryNamenode,专门用于FsImage和Edits的合并</p><h5 id="第一阶段：NameNode启动"><a href="#第一阶段：NameNode启动" class="headerlink" title="第一阶段：NameNode启动"></a>第一阶段：NameNode启动</h5><ol><li>第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。</li><li>客户端对元数据进行增删改的请求。</li><li>NameNode记录操作日志，更新滚动日志。</li><li>NameNode在内存中对数据进行增删改。</li></ol><h5 id="第二阶段：Secondary-NameNode工作"><a href="#第二阶段：Secondary-NameNode工作" class="headerlink" title="第二阶段：Secondary NameNode工作"></a>第二阶段：Secondary NameNode工作</h5><ol><li>Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。</li><li>Secondary NameNode请求执行CheckPoint。</li><li>NameNode滚动正在写的Edits日志。</li><li>将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。</li><li>Secondary NameNode加载编辑日志和镜像文件到内存，并合并。</li><li>生成新的镜像文件fsimage.chkpoint。</li><li>拷贝fsimage.chkpoint到NameNode。</li><li>NameNode将fsimage.chkpoint重新命名成fsimage。</li></ol><p><strong>* opt/module/hadoop-2.7.2/data/tmp/dfs/name/current *</strong></p><h4 id="oiv查看Fsimage"><a href="#oiv查看Fsimage" class="headerlink" title="oiv查看Fsimage"></a>oiv查看Fsimage</h4><p>语法：hdfs oiv -p 文件类型 -i镜像文件 -o 转换后文件输出路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs oiv -p XML -i fsimage_0000000000000000025 -o /opt/module/hadoop-2.7.2/fsimage.xml</span><br></pre></td></tr></table></figure><h4 id="oev查看Edits文件"><a href="#oev查看Edits文件" class="headerlink" title="oev查看Edits文件"></a>oev查看Edits文件</h4><p>语法：hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-2.7.2/edits.xml</span><br></pre></td></tr></table></figure><h4 id="CheckPoint时间设置"><a href="#CheckPoint时间设置" class="headerlink" title="CheckPoint时间设置"></a>CheckPoint时间设置</h4><ol><li><p>通常情况下，SecondaryNameNade每隔一个小时执行一次，在【hdfs-default.xml】中设置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>一分钟检查一次操作次数，3当操作次数达到1百万时，<br>SecondaryNameNode执行一次。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>操作动作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span> 1分钟检查一次操作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span> &gt;</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="NameNode故障处理"><a href="#NameNode故障处理" class="headerlink" title="NameNode故障处理"></a>NameNode故障处理</h2><p><strong>方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录</strong></p><ol><li><p>kill -9 NameNode过程</p></li><li><p>删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name）</p></li><li><p>拷贝SecondaryNameNode中数据到原NameNode存储数据目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -r atguigu@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* ./name/</span><br></pre></td></tr></table></figure></li><li><p>重新启动NameNode</p></li></ol><p><strong>方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中。</strong></p><ol><li><p>修改hdfs-site.xml中的</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>120<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data/tmp/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>kill -9 NameNode进程</p></li><li><p>删除NameNode存储的数据</p></li><li><p>将2NN存储数据的目录拷贝到NN存储数据的评级目录，并删除in_use.lock文件</p></li><li><p>导入检查点数据</p><blockquote><p>sbin/hadoop-daemon.sh start namenode</p></blockquote></li><li><p>启动NameNode</p><blockquote><p>sbin/hadoop-daemon.sh start namenode</p></blockquote></li></ol><h4 id="安全模式"><a href="#安全模式" class="headerlink" title="安全模式"></a>安全模式</h4><p>（1）bin/hdfs dfsadmin -safemode get （功能描述：查看安全模式状态）</p><p>（2）bin/hdfs dfsadmin -safemode enter （功能描述：进入安全模式状态）</p><p>（3）bin/hdfs dfsadmin -safemode leave （功能描述：离开安全模式状态）</p><p>（4）bin/hdfs dfsadmin -safemode wait （功能描述：等待安全模式状态）</p><h2 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h2><h3 id="DataNode工作机制"><a href="#DataNode工作机制" class="headerlink" title="DataNode工作机制"></a>DataNode工作机制</h3><ol><li>一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。</li><li>DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。</li><li>心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。</li><li>集群运行中可以安全加入和退出一些机器。</li></ol><h3 id="保证DataNode的数据完整性"><a href="#保证DataNode的数据完整性" class="headerlink" title="保证DataNode的数据完整性"></a>保证DataNode的数据完整性</h3><p>1）当DataNode读取Block的时候，它会计算CheckSum。</p><p>2）如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。</p><p>3）Client读取其他DataNode上的Block。</p><p>4）DataNode在其文件创建后周期验证CheckSum</p><h3 id="掉线时限参数设置"><a href="#掉线时限参数设置" class="headerlink" title="掉线时限参数设置"></a>掉线时限参数设置</h3><blockquote><p>如果定义超时时间为TimeOut，则超时时长的计算公式为：<br>TimeOut =2<em>dfs.namenode.heartbeat.recheck-interval+10</em>dfs.heartbeat interval。而默认的dfs.namenode.heartbeat.recheck-interval 大小为5分钟，dfs.heartbeatinterval默认为3秒。</p></blockquote><p>配置位置：hdfs-site.xml，配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.heartbeat.recheck-interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>300000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.heartbeat.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="服役新节点"><a href="#服役新节点" class="headerlink" title="服役新节点"></a>服役新节点</h3><ol><li>环境准备</li></ol><p>​ （1）在hadoop104主机上再克隆一台hadoop105主机</p><p>​ （2）修改IP地址和主机名称</p><p>​ （3）删除原来HDFS文件系统留存的文件（/opt/module/hadoop-2.7.2/data和log）</p><p>​ （4）source一下配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sbin&#x2F;hadoop-daemon.sh start datanode</span><br><span class="line">sbin&#x2F;yarn-daemon.sh start nodemanager</span><br></pre></td></tr></table></figure><ol start="2"><li><p>添加白名单</p><ol><li><p>在NameNode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts文件，将允许的集群IP写入</p></li><li><p>在NameNode的hdfs-site.xml配置文件中增加dfs.hosts属性</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>配置文件分发</p></li><li><p>刷新namenode：hdfs dfsadmin -refreshNodes</p></li><li><p>更新ResourceManager节点：yarn rmadmin -refreshNodes</p></li><li><p>数据如果不平衡，则：start-balancer.sh</p></li></ol></li><li><p>添加黑名单（不能同时有节点出现在两个中）</p><ol><li><p>在NameNode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts.exclude文件</p></li><li><p>在NameNode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts.exclude<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>刷新namenode和Resourcemanager</p></li></ol></li></ol><h3 id="Datanode多目录配置"><a href="#Datanode多目录配置" class="headerlink" title="Datanode多目录配置"></a>Datanode多目录配置</h3><ol><li><p>DataNode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本</p></li><li><p>hdfs-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri Feb 14 2020 09:02:06 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;NN和2NN工作机制&quot;&gt;&lt;a href=&quot;#NN和2NN工作机制&quot; class=&quot;headerlink&quot; title=&quot;NN和2NN工
      
    
    </summary>
    
    
      <category term="HDFS" scheme="http://kiedeng.github.io/categories/HDFS/"/>
    
      <category term="Hadoop" scheme="http://kiedeng.github.io/categories/HDFS/Hadoop/"/>
    
    
      <category term="原理" scheme="http://kiedeng.github.io/tags/%E5%8E%9F%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>HDFS的数据流</title>
    <link href="http://kiedeng.github.io/2020/02/13/HDFS%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B5%81/"/>
    <id>http://kiedeng.github.io/2020/02/13/HDFS%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B5%81/</id>
    <published>2020-02-13T10:03:05.000Z</published>
    <updated>2020-02-13T17:20:15.945Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Feb 14 2020 01:46:27 GMT+0800 (GMT+08:00) --><h3 id="HDFS写数据流程"><a href="#HDFS写数据流程" class="headerlink" title="HDFS写数据流程"></a>HDFS写数据流程</h3><ol><li>客户端通过Distributed FileSysetm模块向NameNode请求上传文件，NameNode检查文件是否已存在，父目录是否存在</li><li>NameNode返回是否可以上传</li><li>客户端请求第一个Block上传哪几个DataNode服务器上</li><li>NameNode返回三个DataNode节点，分别为dn1，dn2，dn3</li><li>客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成</li><li>dn1，dn2，dn3逐级应答客户端</li><li>客户端开始往dn1上传第一个Block（先从磁盘读取数据收到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传个dn3，dn1没传一个packet会放入一个应答队列等待应答</li><li>当一个Bloc传输完成之后，客户端再次请求NameNode上传第二个Block服务器</li></ol><h3 id="节点距离：两个节点到达最近的共同祖先的距离总和。"><a href="#节点距离：两个节点到达最近的共同祖先的距离总和。" class="headerlink" title="节点距离：两个节点到达最近的共同祖先的距离总和。"></a>节点距离：两个节点到达最近的共同祖先的距离总和。</h3><h3 id="机架感知"><a href="#机架感知" class="headerlink" title="机架感知"></a>机架感知</h3><blockquote><p>对于常见情况，当复制因子为3时，HDFS的放置策略是将一个副本放在本地机架中的一个节点上，另一个放在本地机架中的另一个节点上，最后一个放在不同机架中的另一个节点上</p></blockquote><h3 id="HDFS读数据流程"><a href="#HDFS读数据流程" class="headerlink" title="HDFS读数据流程"></a>HDFS读数据流程</h3><ol><li>客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。</li><li>挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据</li><li>DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。</li><li>客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。</li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri Feb 14 2020 01:46:27 GMT+0800 (GMT+08:00) --&gt;&lt;h3 id=&quot;HDFS写数据流程&quot;&gt;&lt;a href=&quot;#HDFS写数据流程&quot; class=&quot;headerlink&quot; title=&quot;HDFS写数据流程
      
    
    </summary>
    
    
      <category term="HDFS" scheme="http://kiedeng.github.io/categories/HDFS/"/>
    
      <category term="Hadoop" scheme="http://kiedeng.github.io/categories/HDFS/Hadoop/"/>
    
    
      <category term="原理" scheme="http://kiedeng.github.io/tags/%E5%8E%9F%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>HDFS客户端操作</title>
    <link href="http://kiedeng.github.io/2020/02/13/HDFS%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%93%8D%E4%BD%9C/"/>
    <id>http://kiedeng.github.io/2020/02/13/HDFS%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%93%8D%E4%BD%9C/</id>
    <published>2020-02-13T03:16:18.000Z</published>
    <updated>2020-02-13T10:05:11.257Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Feb 14 2020 01:46:27 GMT+0800 (GMT+08:00) --><h2 id="客户端环境准备"><a href="#客户端环境准备" class="headerlink" title="客户端环境准备"></a>客户端环境准备</h2><ol><li><p>将编译hadoop jar包放在路径下（比如：D:\environment\hadoop-2.7.2）</p></li><li><p>配置HADOOP_HOME环境变量</p><blockquote><p>HADOOP_HOME : D:\environment\hadoop-2.7.2</p></blockquote></li><li><p>配置Path环境变量</p><blockquote><p>%HADOOP_HOME%\bin</p></blockquote></li><li><p>创建一个Maven工程</p></li><li><p>导入相应的依赖坐标+日志添加</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span> <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span> <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.atguigu<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>HDFS-0529<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.0.1-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>2.8.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>jdk.tools<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jdk.tools<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">scope</span>&gt;</span>system<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">systemPath</span>&gt;</span>$&#123;JAVA_HOME&#125;/lib/tools.jar<span class="tag">&lt;/<span class="name">systemPath</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line">  </span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ol><p>需要再项目的src/main/resources目录下，新建一个文件，命令为”log4j.properties”,在文件中填写</p><blockquote><p>log4j.rootLogger=INFO, stdout</p><p>log4j.appender.stdout=org.apache.log4j.ConsoleAppender</p><p>log4j.appender.stdout.layout=org.apache.log4j.PatternLayout</p><p>log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n</p><p>log4j.appender.logfile=org.apache.log4j.FileAppender</p><p>log4j.appender.logfile.File=target/spring.log</p><p>log4j.appender.logfile.layout=org.apache.log4j.PatternLayout</p><p>log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n</p></blockquote><ol start="6"><li><p>创建包名：com.atguigu.hdfs</p></li><li><p>创建HdfsClient类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HdfsClient</span></span>&#123;</span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testMkdirs</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1 获取文件系统</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line"><span class="comment">// 配置在集群上运行</span></span><br><span class="line"><span class="comment">// configuration.set("fs.defaultFS", "hdfs://hadoop102:9000");</span></span><br><span class="line"><span class="comment">// FileSystem fs = FileSystem.get(configuration);</span></span><br><span class="line"></span><br><span class="line">FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:9000"</span>), configuration, <span class="string">"atguigu"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 创建目录</span></span><br><span class="line">fs.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/1108/daxian/banzhang"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3 关闭资源</span></span><br><span class="line">fs.close();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>执行程序</p><blockquote><p>客户端去操作HDFS时，是有一个用户身份的。</p><p>默认情况下，HDFS客户端API会从JVM中获取一个参数来作为自己的用户身份：-DHADOOP_USER_NAME=atguigu，atguigu为用户名称。</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// 配置文件</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://hadoop102:9000"</span>);</span><br><span class="line"><span class="comment">// 获取客户端对象</span></span><br><span class="line">FileSystem fs = FileSystem.get(conf);</span><br><span class="line">fs.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/kangdong"</span>));</span><br><span class="line"><span class="comment">// 关闭客户端</span></span><br><span class="line">fs.close();</span><br><span class="line">System.out.println(<span class="string">"Over!"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h2 id="HDFS的API操作"><a href="#HDFS的API操作" class="headerlink" title="HDFS的API操作"></a>HDFS的API操作</h2><h3 id="HDFS文件上传（测试参数优先级）"><a href="#HDFS文件上传（测试参数优先级）" class="headerlink" title="HDFS文件上传（测试参数优先级）"></a>HDFS文件上传（测试参数优先级）</h3><ol><li><p>编写源代码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// 配置文件</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://hadoop102:9000"</span>);</span><br><span class="line"><span class="comment">// 获取客户端对象</span></span><br><span class="line">FileSystem fs = FileSystem.get(conf);</span><br><span class="line">fs.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/kangdong"</span>));</span><br><span class="line"><span class="comment">// 关闭客户端</span></span><br><span class="line">fs.close();</span><br><span class="line">System.out.println(<span class="string">"Over!"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>将hdfs-site.xml拷贝到项目的根目录（即资源目录下）</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>参数优先级</p><blockquote><p>参数优先级排序：（1）客户端代码中设置的值 &gt;（2）ClassPath下的用户自定义配置文件 &gt;（3）然后是服务器的默认配置</p></blockquote></li></ol><h3 id="HDFS文件的下载"><a href="#HDFS文件的下载" class="headerlink" title="HDFS文件的下载"></a>HDFS文件的下载</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// boolean delSrc 指是否将原文件删除</span></span><br><span class="line"><span class="comment">// Path src 指要下载的文件路径</span></span><br><span class="line"><span class="comment">// Path dst 指将文件下载到的路径</span></span><br><span class="line"><span class="comment">// boolean useRawLocalFileSystem 是否开启文件校验</span></span><br><span class="line">fs.copyToLocalFile(<span class="keyword">false</span>, <span class="keyword">new</span> Path(<span class="string">"/banzhang.txt"</span>), <span class="keyword">new</span> Path(<span class="string">"e:/banhua.txt"</span>), <span class="keyword">true</span>);</span><br></pre></td></tr></table></figure><h3 id="HDFS文件的删除"><a href="#HDFS文件的删除" class="headerlink" title="HDFS文件的删除"></a>HDFS文件的删除</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fs.delete(<span class="keyword">new</span> Path(<span class="string">"/0508/"</span>), <span class="keyword">true</span>);<span class="comment">//第一参数是路径，第二个参数是判断是否递归删除</span></span><br></pre></td></tr></table></figure><h3 id="HDFS文件的改名"><a href="#HDFS文件的改名" class="headerlink" title="HDFS文件的改名"></a>HDFS文件的改名</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fs.rename(<span class="keyword">new</span> Path(<span class="string">"/banzhang.txt"</span>), <span class="keyword">new</span> Path(<span class="string">"/banhua.txt"</span>));</span><br></pre></td></tr></table></figure><h3 id="HDFS文件详情的查看"><a href="#HDFS文件详情的查看" class="headerlink" title="HDFS文件详情的查看"></a>HDFS文件详情的查看</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testListFiles</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1获取文件系统</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:9000"</span>), configuration, <span class="string">"atguigu"</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 获取文件详情</span></span><br><span class="line">RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(<span class="keyword">new</span> Path(<span class="string">"/"</span>), <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span>(listFiles.hasNext())&#123;</span><br><span class="line">LocatedFileStatus status = listFiles.next();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出详情</span></span><br><span class="line"><span class="comment">// 文件名称</span></span><br><span class="line">System.out.println(status.getPath().getName());</span><br><span class="line"><span class="comment">// 长度</span></span><br><span class="line">System.out.println(status.getLen());</span><br><span class="line"><span class="comment">// 权限</span></span><br><span class="line">System.out.println(status.getPermission());</span><br><span class="line"><span class="comment">// 分组</span></span><br><span class="line">System.out.println(status.getGroup());</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取存储的块信息</span></span><br><span class="line">BlockLocation[] blockLocations = status.getBlockLocations();</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (BlockLocation blockLocation : blockLocations) &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取块存储的主机节点</span></span><br><span class="line">String[] hosts = blockLocation.getHosts();</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (String host : hosts) &#123;</span><br><span class="line">System.out.println(host);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">System.out.println(<span class="string">"-----------班长的分割线----------"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3 关闭资源</span></span><br><span class="line">fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="HDFS的I-O流操作"><a href="#HDFS的I-O流操作" class="headerlink" title="HDFS的I/O流操作"></a>HDFS的I/O流操作</h2><h3 id="HDFS文件上传"><a href="#HDFS文件上传" class="headerlink" title="HDFS文件上传"></a>HDFS文件上传</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">IOcopyFromLocal</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:9000"</span>), conf, <span class="string">"atguigu"</span>);</span><br><span class="line">FileInputStream fis = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(<span class="string">"d:/kiedeng.txt"</span>));</span><br><span class="line">FSDataOutputStream fout = fs.create(<span class="keyword">new</span> Path(<span class="string">"/kg.txt"</span>));</span><br><span class="line">IOUtils.copyBytes(fis, fout, conf);</span><br><span class="line"></span><br><span class="line">IOUtils.closeStream(fis);</span><br><span class="line">IOUtils.closeStream(fout);</span><br><span class="line">fs.close();</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="HDFS文件下载"><a href="#HDFS文件下载" class="headerlink" title="HDFS文件下载"></a>HDFS文件下载</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 文件下载</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getFileFromHDFS</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1 获取文件系统</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:9000"</span>), configuration, <span class="string">"atguigu"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 获取输入流</span></span><br><span class="line">FSDataInputStream fis = fs.open(<span class="keyword">new</span> Path(<span class="string">"/banhua.txt"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3 获取输出流</span></span><br><span class="line">FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">"e:/banhua.txt"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4 流的对拷</span></span><br><span class="line">IOUtils.copyBytes(fis, fos, configuration);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5 关闭资源</span></span><br><span class="line">IOUtils.closeStream(fos);</span><br><span class="line">IOUtils.closeStream(fis);</span><br><span class="line">fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="定位文件读取"><a href="#定位文件读取" class="headerlink" title="定位文件读取"></a>定位文件读取</h3><ol><li><p>下载第一块</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFileSeek1</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1 获取文件系统</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:9000"</span>), configuration, <span class="string">"atguigu"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 获取输入流</span></span><br><span class="line">FSDataInputStream fis = fs.open(<span class="keyword">new</span> Path(<span class="string">"/hadoop-2.7.2.tar.gz"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3 创建输出流</span></span><br><span class="line">FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">"e:/hadoop-2.7.2.tar.gz.part1"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4 流的拷贝</span></span><br><span class="line"><span class="keyword">byte</span>[] buf = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1024</span>];</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i =<span class="number">0</span> ; i &lt; <span class="number">1024</span> * <span class="number">128</span>; i++)&#123;</span><br><span class="line">fis.read(buf);</span><br><span class="line">fos.write(buf);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5关闭资源</span></span><br><span class="line">IOUtils.closeStream(fis);</span><br><span class="line">IOUtils.closeStream(fos);</span><br><span class="line">fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>下载第二块</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFileSeek2</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1 获取文件系统</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:9000"</span>), configuration, <span class="string">"atguigu"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 打开输入流</span></span><br><span class="line">FSDataInputStream fis = fs.open(<span class="keyword">new</span> Path(<span class="string">"/hadoop-2.7.2.tar.gz"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3 定位输入数据位置</span></span><br><span class="line">fis.seek(<span class="number">1024</span>*<span class="number">1024</span>*<span class="number">128</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4 创建输出流</span></span><br><span class="line">FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">"e:/hadoop-2.7.2.tar.gz.part2"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5 流的对拷</span></span><br><span class="line">IOUtils.copyBytes(fis, fos, configuration);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 6 关闭资源</span></span><br><span class="line">IOUtils.closeStream(fis);</span><br><span class="line">IOUtils.closeStream(fos);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="3"><li><p>合并文件</p><blockquote><p>​ 在Window命令窗口中进入到目录E:\，然后执行如下命令，对数据进行合并</p><p>​ type hadoop-2.7.2.tar.gz.part2 &gt;&gt; hadoop-2.7.2.tar.gz.part1</p><p>合并完成后，将hadoop-2.7.2.tar.gz.part1重新命名为hadoop-2.7.2.tar.gz。解压发现该tar包非常完整。</p></blockquote></li></ol></li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri Feb 14 2020 01:46:27 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;客户端环境准备&quot;&gt;&lt;a href=&quot;#客户端环境准备&quot; class=&quot;headerlink&quot; title=&quot;客户端环境准备&quot;&gt;&lt;/a&gt;
      
    
    </summary>
    
    
      <category term="HDFS" scheme="http://kiedeng.github.io/categories/HDFS/"/>
    
      <category term="Hadoop" scheme="http://kiedeng.github.io/categories/HDFS/Hadoop/"/>
    
    
      <category term="代码" scheme="http://kiedeng.github.io/tags/%E4%BB%A3%E7%A0%81/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop编译源码</title>
    <link href="http://kiedeng.github.io/2020/02/12/Hadoop%E7%BC%96%E8%AF%91%E6%BA%90%E7%A0%81/"/>
    <id>http://kiedeng.github.io/2020/02/12/Hadoop%E7%BC%96%E8%AF%91%E6%BA%90%E7%A0%81/</id>
    <published>2020-02-12T03:29:53.000Z</published>
    <updated>2020-02-13T02:02:58.820Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Feb 14 2020 01:46:27 GMT+0800 (GMT+08:00) --><h2 id="jar包准备"><a href="#jar包准备" class="headerlink" title="jar包准备"></a>jar包准备</h2><ol><li>hadoop-2.7.2-src.tar.gz</li><li>jdk-8u144-linux-x64.tar.gz</li><li>apache-ant-1.9.9-bin.tar.gz（build工具，打包用的）</li><li>apache-maven-3.0.5-bin.tar.gz</li><li>protobuf-2.5.0.tar.gz（序列化的框架）</li></ol><h2 id="jar包安装"><a href="#jar包安装" class="headerlink" title="jar包安装"></a>jar包安装</h2><ol><li><p>JDK解压，配置环境变量JAVA_HOME和PATH</p><blockquote><p>#JAVA_HOME：</p><p>export JAVA_HOME=/opt/module/jdk1.8.0_144</p><p>export PATH=$PATH:$JAVA_HOME/bin</p></blockquote><p>source /etc/profile进行生效</p></li><li><p>Maven解压，配置MAVEN_HOME和PATH</p><blockquote><p>[root@hadoop101 software]# tar -zxvf apache-maven-3.0.5-bin.tar.gz -C /opt/module/</p><p>[root@hadoop101 apache-maven-3.0.5]# vi conf/settings.xml</p><mirrors><p>​<mirror></mirror></p><p>​<id>nexus-aliyun</id></p><p>​<mirrorof>central</mirrorof></p><p>​<name>Nexus aliyun</name></p><p>​<url><a href="http://maven.aliyun.com/nexus/content/groups/public" target="_blank" rel="noopener">http://maven.aliyun.com/nexus/content/groups/public</a></url></p><p>​</p></mirrors><p>[root@hadoop101 apache-maven-3.0.5]# vi /etc/profile</p><p>#MAVEN_HOME</p><p>export MAVEN_HOME=/opt/module/apache-maven-3.0.5</p><p>export PATH=$PATH:$MAVEN_HOME/bin</p><p>[root@hadoop101 software]#source /etc/profile</p></blockquote><p><strong>验证命令：mvn-version</strong></p></li><li><p>ant解压、配置 ANT _HOME和PATH</p><blockquote><p>[root@hadoop101 software]# tar -zxvf apache-ant-1.9.9-bin.tar.gz -C /opt/module/</p><p>[root@hadoop101 apache-ant-1.9.9]# vi /etc/profile</p><p>#ANT_HOME</p><p>export ANT_HOME=/opt/module/apache-ant-1.9.9</p><p>export PATH=$PATH:$ANT_HOME/bi</p><p>[root@hadoop101 software]#source /etc/profile</p></blockquote></li><li><p>安装 glibc-headers 和 g++ 命令如下</p><blockquote><p>[root@hadoop101 apache-ant-1.9.9]# yum install glibc-headers</p><p>[root@hadoop101 apache-ant-1.9.9]# yum install gcc-c++</p></blockquote></li><li><p>安装make和cmake</p><blockquote><p>[root@hadoop101 apache-ant-1.9.9]# yum install make</p><p>[root@hadoop101 apache-ant-1.9.9]# yum install cmake</p></blockquote></li><li><p>解压protobuf ，进入到解压后protobuf主目录，/opt/module/protobuf-2.5.0，然后相继执行命令</p><blockquote><p>[root@hadoop101 software]# tar -zxvf protobuf-2.5.0.tar.gz -C /opt/module/</p><p>[root@hadoop101 opt]# cd /opt/module/protobuf-2.5.0/</p><p>[root@hadoop101 protobuf-2.5.0]#./configure</p><p>[root@hadoop101 protobuf-2.5.0]# make</p><p>[root@hadoop101 protobuf-2.5.0]# make check</p><p>[root@hadoop101 protobuf-2.5.0]# make install</p><p>[root@hadoop101 protobuf-2.5.0]# ldconfig</p><p>[root@hadoop101 hadoop-dist]# vi /etc/profile</p><p>#LD_LIBRARY_PATH</p><p>export LD_LIBRARY_PATH=/opt/module/protobuf-2.5.0</p><p>export PATH=$PATH:$LD_LIBRARY_PATH</p><p>[root@hadoop101 software]#source /etc/profile</p></blockquote></li><li><p>安装openssl库</p><blockquote><p>[root@hadoop101 software]#yum install openssl-devel</p></blockquote></li><li><p>安装 ncurses-devel库</p><blockquote><p>[root@hadoop101 software]#yum install ncurses-devel</p></blockquote></li></ol><h2 id="编译源码"><a href="#编译源码" class="headerlink" title="编译源码"></a>编译源码</h2><ol><li><p>解压源码到/opt/目录</p><blockquote><p>tar -zxvf hadoop-2.7.2-src.tar.gz -C /opt/</p></blockquote></li><li><p>进入源码主目录</p></li><li><p>通过maven执行编译命令</p><blockquote><p>mvn package -Pdist,native -DskipTests -Dtar</p></blockquote></li><li><p>成功的64位hadoop包在/opt/hadoop-2.7.2-src/hadoop-dist/target下</p></li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri Feb 14 2020 01:46:27 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;jar包准备&quot;&gt;&lt;a href=&quot;#jar包准备&quot; class=&quot;headerlink&quot; title=&quot;jar包准备&quot;&gt;&lt;/a&gt;jar
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="http://kiedeng.github.io/categories/Hadoop/"/>
    
    
      <category term="默认" scheme="http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"/>
    
  </entry>
  
  <entry>
    <title>软件推荐</title>
    <link href="http://kiedeng.github.io/2020/02/11/%E8%BD%AF%E4%BB%B6%E6%8E%A8%E8%8D%90/"/>
    <id>http://kiedeng.github.io/2020/02/11/%E8%BD%AF%E4%BB%B6%E6%8E%A8%E8%8D%90/</id>
    <published>2020-02-11T10:48:17.000Z</published>
    <updated>2020-02-11T11:25:52.268Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Feb 14 2020 01:46:27 GMT+0800 (GMT+08:00) --><h3 id="TinyTask"><a href="#TinyTask" class="headerlink" title="TinyTask"></a>TinyTask</h3><p>自动化操作工具</p><h3 id="Ditto"><a href="#Ditto" class="headerlink" title="Ditto"></a>Ditto</h3><p>复制神器</p><h3 id="SecureCRT"><a href="#SecureCRT" class="headerlink" title="SecureCRT"></a>SecureCRT</h3><p>一般大数据使用的远程工具</p><h3 id="PanDownload"><a href="#PanDownload" class="headerlink" title="PanDownload"></a>PanDownload</h3><p>网盘下载</p><h3 id="天若OCR文字识别"><a href="#天若OCR文字识别" class="headerlink" title="天若OCR文字识别"></a>天若OCR文字识别</h3><p>用于文字识别</p><h3 id="Snipaste"><a href="#Snipaste" class="headerlink" title="Snipaste"></a>Snipaste</h3><p>截图贴图工具</p><h3 id="Typora"><a href="#Typora" class="headerlink" title="Typora"></a>Typora</h3><p>markdown编辑器</p><h3 id="Notepad"><a href="#Notepad" class="headerlink" title="Notepad++"></a>Notepad++</h3><p>编辑器，可以进行文件夹文字查找</p><h3 id="Sublime"><a href="#Sublime" class="headerlink" title="Sublime"></a>Sublime</h3><p>编辑器，可以远程服务器</p><h3 id="Bandizip"><a href="#Bandizip" class="headerlink" title="Bandizip"></a>Bandizip</h3><p>解压工具</p><h3 id="VMware"><a href="#VMware" class="headerlink" title="VMware"></a>VMware</h3><p>虚拟机</p><h3 id="Xshell-Xftp"><a href="#Xshell-Xftp" class="headerlink" title="Xshell/Xftp"></a>Xshell/Xftp</h3><p>远程连接工具</p><h3 id="Adobe-Acrobat-DC"><a href="#Adobe-Acrobat-DC" class="headerlink" title="Adobe Acrobat DC"></a>Adobe Acrobat DC</h3><p>pdf阅读，编辑</p><h3 id="Anki"><a href="#Anki" class="headerlink" title="Anki"></a>Anki</h3><p>记忆工具</p><h3 id="Everything"><a href="#Everything" class="headerlink" title="Everything"></a>Everything</h3><p>查看文件</p><h3 id="PotPlayer"><a href="#PotPlayer" class="headerlink" title="PotPlayer"></a>PotPlayer</h3><p>播放器</p><h3 id="Chrome"><a href="#Chrome" class="headerlink" title="Chrome"></a>Chrome</h3><p>浏览器</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri Feb 14 2020 01:46:27 GMT+0800 (GMT+08:00) --&gt;&lt;h3 id=&quot;TinyTask&quot;&gt;&lt;a href=&quot;#TinyTask&quot; class=&quot;headerlink&quot; title=&quot;TinyTask&quot;&gt;&lt;
      
    
    </summary>
    
    
      <category term="工具" scheme="http://kiedeng.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="自动化" scheme="http://kiedeng.github.io/tags/%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Linux使用手册</title>
    <link href="http://kiedeng.github.io/2020/02/10/Linux%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/"/>
    <id>http://kiedeng.github.io/2020/02/10/Linux%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/</id>
    <published>2020-02-10T08:57:11.000Z</published>
    <updated>2020-02-11T04:56:13.605Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Feb 14 2020 01:46:27 GMT+0800 (GMT+08:00) --><h2 id="用户管理命令"><a href="#用户管理命令" class="headerlink" title="用户管理命令"></a>用户管理命令</h2><blockquote><p><strong>#添加</strong></p><p>useradd kiedeng 或者 useradd -g [组名] 用户名</p><p><strong>#设置密码</strong></p><p>useradd 用户名</p><p><strong>#查看用户是否存在</strong></p><p>id 用户名</p><p><strong>#查看创建的用户</strong></p><p>cat /etc/passwd</p><p><strong>#切换用户</strong></p><p>su 用户名（没有获得环境变量）</p><p>su - 用户名 （获得环境变量）</p><p><strong>#删除用户</strong></p><p>userdel 用户名</p><p>userdel -r 用户名 （用户和用户目录全部删除）</p><p><strong>#设置root权限</strong></p><p>vim /etc/sudoers</p><p>在root下中添加一行</p><p>用户名 ALL=(ALL) ALL</p><p>用户名 ALL=(ALL) NOPASSWDALL （不需要密码）</p><p><strong>修改usermod 修改用户</strong></p><p>usemod -g 用户组 用户名</p></blockquote><h2 id="用户组管理"><a href="#用户组管理" class="headerlink" title="用户组管理"></a>用户组管理</h2><blockquote><p>添加：groupadd 组名</p><p>删除：groupdel 组名</p><p>修改组：groupmod -n 新组名 老组名</p><p>查看组：cat /etc/group</p></blockquote><h2 id="文件权限类"><a href="#文件权限类" class="headerlink" title="文件权限类"></a>文件权限类</h2><blockquote><p>总共10位：0~9</p><p>0位：-表示文件，d代表文件，l代表链接文档</p><p>1-3位确定属主的该文件权限</p><p>4-6位确定属组的该文件的权限</p><p>7-9位确定其他用户改文件的权限</p><p><strong>修改文件权限</strong></p><p>chmod 777 a.txt</p><p>chmod -R 777 xiyou （递归删除）</p><p>改变所有者</p><p>chown [选项] [最终用户] [文件或者目录] 选项为-R （递归操作）</p><p>改变所属组</p><p>chgrp [最终用户组] [文件或者目录]</p></blockquote><h2 id="搜索查找类"><a href="#搜索查找类" class="headerlink" title="搜索查找类"></a>搜索查找类</h2><p><strong>find 查找文件或者目录</strong></p><p>基本语法：find [搜索范围] [选项]</p><blockquote><p>选项</p><p>-name&lt;查询方式&gt; 按照指定文件名查找</p><p>-user&lt;用户名&gt; 指定用户查找</p><p>-size&lt;文件大小&gt; 按照文件大小查找 （+为大于，-为小于）</p><p>比如：find /home -size +20458</p></blockquote><p><strong>locate快速定位文件路径</strong></p><p>更新：updatedb</p><p>基本语法：locate 搜索文件</p><p><strong>grep 过滤查找及“|”管道符</strong></p><p>基本语法：grep 选项 查找内容 源文件</p><h2 id="压缩和解压缩"><a href="#压缩和解压缩" class="headerlink" title="压缩和解压缩"></a>压缩和解压缩</h2><p><strong>gzip/gunzip压缩</strong></p><ul><li>只能压缩文件，不能压缩目录</li></ul><p>命令：gzip 文件；gunzip 文件. gz</p><p><strong>zip/unzip压缩</strong></p><p>基本语法：</p><blockquote><p>zip [选项] xxx.zip 将要压缩的内容 （目录或文件）</p><p>unzip [选项] xxx.zip 解压文件</p><p>选项：-d&lt;目录&gt; 指定压缩后文件存放的目录</p></blockquote><p><strong>tar 打包</strong></p><blockquote><p>tar [选项] xxx.tar.gz 将要打包进去的内容</p><p>选项</p><p>-c 产生tar打包文件</p><p>-v 显示详细信息</p><p>-f 指定压缩后的文件名</p><p>-z 打包同时压缩</p><p>-x 解包.tar文件</p></blockquote><p>压缩：tar -zcvf kie.tar.gz a.txt b.txt</p><p>解压：tar -zxvf kie.tar.gz -C /opt</p><h2 id="磁盘分区类"><a href="#磁盘分区类" class="headerlink" title="磁盘分区类"></a>磁盘分区类</h2><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri Feb 14 2020 01:46:27 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;用户管理命令&quot;&gt;&lt;a href=&quot;#用户管理命令&quot; class=&quot;headerlink&quot; title=&quot;用户管理命令&quot;&gt;&lt;/a&gt;用户管
      
    
    </summary>
    
    
      <category term="使用手册" scheme="http://kiedeng.github.io/categories/%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/"/>
    
    
      <category term="Linux" scheme="http://kiedeng.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>hadoop集群搭建</title>
    <link href="http://kiedeng.github.io/2020/02/10/hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"/>
    <id>http://kiedeng.github.io/2020/02/10/hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/</id>
    <published>2020-02-10T06:11:15.000Z</published>
    <updated>2020-02-13T15:23:05.775Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Feb 14 2020 01:46:27 GMT+0800 (GMT+08:00) --><h2 id="克隆虚拟机"><a href="#克隆虚拟机" class="headerlink" title="克隆虚拟机"></a>克隆虚拟机</h2><ul><li>配置好的Linux虚拟机-&gt; 管理 -&gt; 克隆</li></ul><p><strong>删除网卡，复制物理地址</strong>：vim /etc/udev/rules.d/70-persistent-net.rules</p><blockquote><p>删除eht0的那一行</p><p>将下一行的eth0改为eth1</p><p>复制address物理地址</p></blockquote><p><strong>配置网络：</strong>vim /etc/sysconfig/network-scripts/ifcfg-eth0</p><blockquote><p>HWADDR:粘贴物理地址</p><p>IPADDR=192.168.1.101 设置ip</p><p>ONBOOT=yes<br>NM_CONTROLLED=yes<br>B00TPROTO=static</p><p>GATEWAY=192.168.1.2<br>DNS1=192.168.1.2 和网关一致</p></blockquote><p><strong>修改主机名称：</strong>vim /etc/sysconfig/network</p><blockquote><p>NETWORKING=yes<br>HOSTNAME=hadoopl 01</p></blockquote><p><strong>修改主机映射：</strong>vim /etc/hosts</p><h2 id="安装JDK"><a href="#安装JDK" class="headerlink" title="安装JDK"></a>安装JDK</h2><ol><li><p>通过sftp将jdk放在/opt/software下</p><blockquote><p>tar -zxvf [gz文件名] -C [解压的路径]</p></blockquote></li><li><p>设置环境路径</p><blockquote><p>#JAVA_HOME</p><p>export JAVA_HOME=/opt/module/jdk1.8.0_144</p><p>export PATH=$PATH:$JAVA_HOME/bin</p></blockquote></li><li><p>让修改以后的文件生效</p><blockquote><p>source /etc/profile</p><p>测试是否成功指令</p><p>java -version</p></blockquote></li></ol><h2 id="安装Hadoop"><a href="#安装Hadoop" class="headerlink" title="安装Hadoop"></a>安装Hadoop</h2><ol><li><p>将hadoop-2.7.2.tar.gz传入，解压</p></li><li><p>在/etc/profile添加环境变量</p><blockquote><p>##HADOOP_HOME</p><p>export HADOOP_HOME=/opt/module/hadoop-2.7.2</p><p>export PATH=$PATH:$HADOOP_HOME/bin</p><p>export PATH=$PATH:$HADOOP_HOME/sbin</p></blockquote></li><li><p>生效文件：source /etc/profile</p></li></ol><h2 id="Hadoop的目录结构"><a href="#Hadoop的目录结构" class="headerlink" title="Hadoop的目录结构"></a>Hadoop的目录结构</h2><ol><li>bin目录：存放对Hadoop相关服务（HDFS和YARN）进行操作的脚本</li><li>etc目录：Hadoop的配置目录，存放Hadoop的配置文件</li><li>lib目录：存放Hadoop的本地库（对数据进行压缩解压功能）</li><li>sbin目录：存放启动或者停止Hadoop相关服务的脚本</li><li>share目录：存放hadoop的jar包，官方文档和案例</li></ol><h2 id="Hadoop的运行模式"><a href="#Hadoop的运行模式" class="headerlink" title="Hadoop的运行模式"></a><strong>Hadoop的运行模式</strong></h2><p>​ 本地模式，伪分布式模式，完成分布式模式</p><h3 id="一，本地模式"><a href="#一，本地模式" class="headerlink" title="一，本地模式"></a>一，本地模式</h3><h4 id="官方Grep案例"><a href="#官方Grep案例" class="headerlink" title="官方Grep案例"></a>官方Grep案例</h4><ol><li><p>在hadoop-2.7.2文件下面创建一个input文件夹</p></li><li><p>复制文件到input文件夹里面</p></li><li><p>执行share文件夹下的MapReduce程序</p><blockquote><p>bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output ‘dfs[a-z.]+’</p></blockquote></li><li><p>查看输出结果</p><blockquote><p>cat output/*</p></blockquote></li></ol><h4 id="官方WordCount案例（计算单词个数）"><a href="#官方WordCount案例（计算单词个数）" class="headerlink" title="官方WordCount案例（计算单词个数）"></a>官方WordCount案例（计算单词个数）</h4><ol><li><p>创建文件夹（wcinput）</p></li><li><p>在该文件夹创建并编辑文件内容</p></li><li><p>执行</p><blockquote><p>hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount wcinput wcoutput</p></blockquote></li><li><p>查看结果</p><blockquote><p>hadoop-2.7.2]$ cat wcoutput/part-r-00000</p></blockquote></li></ol><h3 id="二，伪分布式运行模式"><a href="#二，伪分布式运行模式" class="headerlink" title="二，伪分布式运行模式"></a>二，伪分布式运行模式</h3><h4 id="启动HDFS并运行MapReduce程序"><a href="#启动HDFS并运行MapReduce程序" class="headerlink" title="启动HDFS并运行MapReduce程序"></a>启动HDFS并运行MapReduce程序</h4><ol><li><p>配置集群</p><ul><li><p>hadoop-env.sh(添加jdk路径)</p><blockquote><p>export JAVA_HOME=/opt/module/jdk1.8.0_144</p></blockquote></li><li><p>core-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定HDFS中NameNode的地址 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop101:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul></li></ol><ul><li><p>hdfs-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定HDFS副本的数量 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul><ol start="2"><li><p>启动集群</p><ul><li><p>格式化NameNode（第一次启动时格式化，以后就不要格式化</p><blockquote><p>bin/hdfs namenode -format</p></blockquote></li><li><p>启动NameNode</p><blockquote><p>sbin/hadoop-daemon.sh start namenode</p></blockquote></li><li><p>启动DataNode</p><blockquote><p>sbin/hadoop-daemon.sh start datanode</p></blockquote></li></ul></li><li><p>查看集群</p><ul><li><p>web访问默认端口：50070</p></li><li><p>打开失败：<a href="https://www.cnblogs.com/zlslch/p/6604189.html" target="_blank" rel="noopener">https://www.cnblogs.com/zlslch/p/6604189.html</a></p></li><li><p>log日志：/opt/module/hadoop-2.7.2/logs</p></li><li><p>不能一直格式化NameNode原因</p><blockquote><p>会产生新的集群id，导致NameNode和DataNode的集群id不一致，集群找不到数据，所以格式化前先删除data和log日志</p></blockquote></li></ul></li><li><p>操作集群</p><ul><li>（wordcount）执行操作和本地模式一样</li></ul></li></ol><h4 id="启动YARN并运行MapReduce程序"><a href="#启动YARN并运行MapReduce程序" class="headerlink" title="启动YARN并运行MapReduce程序"></a>启动YARN并运行MapReduce程序</h4><ol><li><p>配置集群</p><ul><li><p>yarn-env.sh(添加路径)</p><blockquote><p>export JAVA_HOME=/opt/module/jdk1.8.0_144</p></blockquote></li><li><p>配置yarn-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Reducer获取数据的方式 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定YARN的ResourceManager的地址 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop101<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul></li></ol><ul><li><p>mapred-env.sh(添加jdk路径)</p></li><li><p>(对mapred-site.xml.template重新命名为) mapred-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定MR运行在YARN上 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul><ol start="2"><li><p>启动集群</p><ol><li><p>启动前必须先启动NameNode和DataNode</p></li><li><p>启动ResourceManager</p><blockquote><p>sbin/yarn-daemon.sh start resourcemanager</p></blockquote></li><li><p>启动NodeManager</p><blockquote><p>sbin/yarn-daemon.sh start nodemanager</p></blockquote></li></ol></li><li><p>集群操作</p><ol><li><p>yarn访问端口：8088</p></li><li><p>删除文件系统的ouput文件</p><blockquote><p>bin/hdfs dfs -rm -R /user/kiedong/output</p></blockquote></li><li><p>执行MapReduce程序</p><blockquote><p>bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/atguigu/input /user/atguigu/output</p></blockquote></li></ol></li></ol><h4 id="配置历史服务器"><a href="#配置历史服务器" class="headerlink" title="配置历史服务器"></a>配置历史服务器</h4><ul><li><p>配置mapred-site.xml,添加</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 历史服务器端地址 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop101:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop101:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul><ul><li><p>启动历史服务器</p><blockquote><p>sbin/mr-jobhistory-daemon.sh start historyserver</p></blockquote></li><li><p>端口：19888</p></li></ul><h4 id="配置日志的聚集"><a href="#配置日志的聚集" class="headerlink" title="配置日志的聚集"></a>配置日志的聚集</h4><p>日志聚集的概念：应用程序运行完成以后，程序运行日志上传到HDFS系统上</p><p>日志聚集的好处：方便开发调试</p><p><strong>注意</strong>：开启此功能，需要重新启动NodeManager，ResourceManager和HistoryManager</p><ul><li><p>yarn-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 日志聚集功能使能 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 日志保留时间设置7天 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul><ul><li><p>关闭,然后NodeManager 、ResourceManager和HistoryManager</p></li><li><p>执行WordCount案例</p></li><li><p>查看日志端口：19888</p></li></ul><h3 id="完全分布式运行模式（重点）"><a href="#完全分布式运行模式（重点）" class="headerlink" title="完全分布式运行模式（重点）"></a>完全分布式运行模式（重点）</h3><h4 id="主要步骤："><a href="#主要步骤：" class="headerlink" title="主要步骤："></a><strong>主要步骤：</strong></h4><ol><li>准备三台客户机（关闭防火墙，静态ip，主机名称）</li><li>安装JDK</li><li>配置环境变量</li><li>安装Hadoop</li><li>配置环境变量</li><li>配置集群</li><li>单点启动</li><li>配置ssh</li><li>群起并测试集群</li></ol><h4 id="编写集群分发脚本xsync"><a href="#编写集群分发脚本xsync" class="headerlink" title="编写集群分发脚本xsync"></a>编写集群分发脚本xsync</h4><ol><li><p>scp（secure copy）安全拷贝</p><ol><li><p>scp可以实现服务器与服务器之间的数据拷贝</p></li><li><p>语法:将hadoop101传给hadoop102</p><blockquote><p>scp -r /opt/module root@hadoop102:/opt/module</p></blockquote></li></ol></li><li><p>rsync远程同步工具</p><ol><li>主要用于备份和镜像。</li><li>与scp区别：rsync只对差异性文件做更新。scp是复制所有文件</li><li>rsync -rvl /opt/software root@hadoop102:/opt/software</li><li>-r 递归 -v显示复制过程 -l拷贝符号连接</li></ol></li><li><p>xsync集群分发脚本（本集群使用）</p><ol><li><p>需求：循环复制文件到所有节点的相同目录下</p></li><li><p>在home/用户名/bin这个目录下存放脚本，这样次用户可以在系统任何地方直接执行</p></li><li><p>编写代码：vim xsync</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">1 获取输入参数个数，如果没有参数，直接退出</span></span><br><span class="line">pcount=$#</span><br><span class="line">if((pcount==0)); then</span><br><span class="line">echo no args;</span><br><span class="line">exit;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2 获取文件名称</span></span><br><span class="line">p1=$1</span><br><span class="line">fname=`basename $p1`</span><br><span class="line">echo fname=$fname</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">3 获取上级目录到绝对路径</span></span><br><span class="line">pdir=`cd -P $(dirname $p1); pwd`</span><br><span class="line">echo pdir=$pdir</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">4 获取当前用户名称</span></span><br><span class="line">user=`whoami`</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">5 循环</span></span><br><span class="line">for((host=103; host&lt;105; host++)); do</span><br><span class="line">        echo ------------------- hadoop$host --------------</span><br><span class="line">        rsync -rvl $pdir/$fname $user@hadoop$host:$pdir</span><br><span class="line">done</span><br></pre></td></tr></table></figure></li><li><p>修改脚本xsync具有执行权限</p><blockquote><p>chomod 777 xsync</p></blockquote></li></ol></li></ol><h4 id="配置集群"><a href="#配置集群" class="headerlink" title="配置集群"></a>配置集群</h4><ol><li><p>核心配置文件</p><ul><li><p>core-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定HDFS中NameNode的地址 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul></li></ol><ol start="2"><li><p>HDFS配置文件</p><ul><li><p>hadoop-en.sh：配置环境变量</p></li><li><p>hdfs-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul></li></ol><ol start="3"><li><p>YARN配置文件</p><ul><li><p>yarn-env.sh：配置环境变量</p></li><li><p>yarn-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Reducer获取数据的方式 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定YARN的ResourceManager的地址 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul></li></ol><ol start="4"><li><p>MapReduce配置文件</p><ul><li><p>mapred-env.sh：配置环境变量</p></li><li><p>配置mapred-env.sh</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cp mapred-site.xml.template mapred-site.xml</span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定MR运行在Yarn上 --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul></li></ol><h4 id="集群单点启动"><a href="#集群单点启动" class="headerlink" title="集群单点启动"></a>集群单点启动</h4><ol><li><p>第一次启动，需要格式化NameNode</p><blockquote><p>hadoop namenode -format</p></blockquote></li><li><p>在hadoop102上启动NameNode</p><blockquote><p>hadoop-daemon.sh start namenode</p></blockquote></li><li><p>在hadoop102,103,104分别启动DataNode</p></li></ol><h4 id="ssh无密码登陆配置"><a href="#ssh无密码登陆配置" class="headerlink" title="ssh无密码登陆配置"></a>ssh无密码登陆配置</h4><ol><li><p>原理</p><blockquote><p>将A服务器生成的公钥拷贝给B服务器，当A将数据用私钥A加密，</p><p>则B用A的公钥解密，并将数据用公钥加密给A</p></blockquote></li><li><p>生成公钥和私钥</p><blockquote><p>ssh-keygen -t rsa</p></blockquote></li><li><p>将公钥拷贝到免密的目标机器上</p><blockquote><p>ssh-copy-id hadoop102</p><p>ssh-copy-id hadoop103</p><p>ssh-copy-id hadoop104</p></blockquote><p>注意：root用户需要再进行一次拷贝，在hadoop103生成公私钥拷贝给其他机器</p></li><li><p>.ssh文件夹下文件功能解释</p><blockquote><p>known_hosts:记录ssh访问过计算机的公钥</p><p>id_rsa : 生成的私钥</p><p>id_rsa.pub:生成的公钥</p><p>authorized_keys ： 存放授权过得无密登录服务器公钥</p></blockquote></li></ol><h4 id="群起集群"><a href="#群起集群" class="headerlink" title="群起集群"></a>群起集群</h4><ol><li><p>配置slaves，并同步slaves</p><blockquote><p>hadoop102</p><p>hadoop103</p><p>hadoop104</p></blockquote></li><li><p>启动集群</p><ul><li><p>第一次先格式化NameNode（先停止以前运行的namenode和datanode，然后删除data和log数据）</p></li><li><p>启动</p><blockquote><p>sbin/start-dfs.sh (hadoop102上)</p><p>sbin/start-yarn.sh（hadoop03上）</p></blockquote></li></ul></li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri Feb 14 2020 01:46:27 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;克隆虚拟机&quot;&gt;&lt;a href=&quot;#克隆虚拟机&quot; class=&quot;headerlink&quot; title=&quot;克隆虚拟机&quot;&gt;&lt;/a&gt;克隆虚拟机&lt;
      
    
    </summary>
    
    
      <category term="部署" scheme="http://kiedeng.github.io/categories/%E9%83%A8%E7%BD%B2/"/>
    
    
      <category term="Hadoop" scheme="http://kiedeng.github.io/tags/Hadoop/"/>
    
      <category term="大数据" scheme="http://kiedeng.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Linux安装与配置</title>
    <link href="http://kiedeng.github.io/2020/02/10/Linux%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/"/>
    <id>http://kiedeng.github.io/2020/02/10/Linux%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/</id>
    <published>2020-02-10T05:35:12.000Z</published>
    <updated>2020-02-11T02:47:13.265Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Feb 14 2020 01:46:27 GMT+0800 (GMT+08:00) --><h3 id="安装时注意事项-用VMware安装centos-大数据虚拟机"><a href="#安装时注意事项-用VMware安装centos-大数据虚拟机" class="headerlink" title="安装时注意事项(用VMware安装centos[大数据虚拟机])"></a>安装时注意事项(用VMware安装centos[大数据虚拟机])</h3><ol><li><p>检查BIOS虚拟化支持</p></li><li><p>内存默认设置为2048MB</p></li><li><p>最大磁盘大小默认为20GB</p></li><li><p>是否对CD媒体进行测试，<strong>直接跳过Skip</strong></p></li><li><p>创建自定义分区(都是标准分区)</p><blockquote><p>boot 默认： 100MB</p><p>wap 默认：2048MB</p><p>/ 默认：15360</p></blockquote></li><li><p>自定义系统软件</p><blockquote><p>基本系统：兼容程序库；基本</p><p>应用程序：互联网浏览器</p><p>桌面：除了KDE,其余都选</p><p>语言支持：中文支持</p></blockquote></li><li><p>Kdump去掉</p></li></ol><h3 id="查看网络IP和网关"><a href="#查看网络IP和网关" class="headerlink" title="查看网络IP和网关"></a>查看网络IP和网关</h3><p>编辑 -&gt; 虚拟网络编辑器 -&gt; NAT模式 即可看到子网IP</p><p>NET设置可以看到网关</p><h3 id="设置IP"><a href="#设置IP" class="headerlink" title="设置IP"></a>设置IP</h3><h4 id="自动获取"><a href="#自动获取" class="headerlink" title="自动获取"></a>自动获取</h4><p>登录后，通过界面来设置自动获取</p><h4 id="指定固定ip"><a href="#指定固定ip" class="headerlink" title="指定固定ip"></a>指定固定ip</h4><p>​ 直接修改配置文件来指定IP,并可以连接到外网(程序员推荐)，编辑 vi /etc/sysconfig/network-scripts/ifcfg-eth0<br>​ 要求：将ip地址配置的静态的，ip地址为192.168.xxx.xxx</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">DEVICE&#x3D;eth0                #接口名（设备,网卡）</span><br><span class="line">HWADDR&#x3D;00:0C:2x:6x:0x:xx   #MAC地址 </span><br><span class="line">TYPE&#x3D;Ethernet               #网络类型（通常是Ethemet）</span><br><span class="line">UUID&#x3D;926a57ba-92c6-4231-bacb-f27e5e6a9f44  #随机id</span><br><span class="line">#系统启动的时候网络接口是否有效（yes&#x2F;no）</span><br><span class="line">ONBOOT&#x3D;yes                </span><br><span class="line"># IP的配置方法[none|static|bootp|dhcp]（引导时不使用协议|静态分配IP|BOOTP协议|DHCP协议）</span><br><span class="line">BOOTPROTO&#x3D;static      </span><br><span class="line">#IP地址</span><br><span class="line">IPADDR&#x3D;192.168.189.130  </span><br><span class="line">#网关  </span><br><span class="line">GATEWAY&#x3D;192.168.189.2      </span><br><span class="line">#域名解析器</span><br><span class="line">DNS1&#x3D;192.168.189.2</span><br></pre></td></tr></table></figure><p><strong>重启网络服务或者重启系统生效</strong>：service network restart 、reboot</p><h3 id="修改主机名"><a href="#修改主机名" class="headerlink" title="修改主机名"></a>修改主机名</h3><p>查看当前主机名：hostname</p><p>修改主机名：/etc/hostname</p><p>修改主机映射文件：vim /etc/sysconfig/network</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">NETWORKING&#x3D;yes</span><br><span class="line">NETWORKING_IPV6&#x3D;no</span><br><span class="line">HOSTNAME&#x3D; hadoop &#x2F;&#x2F;写入新的主机名</span><br><span class="line">注意：主机名称不要有“_”下划线</span><br></pre></td></tr></table></figure><p>修改ip与主机的映射：/etc/hosts</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">192.168.102.130  hadoop</span><br></pre></td></tr></table></figure><h3 id="Windows设置本地dns解析"><a href="#Windows设置本地dns解析" class="headerlink" title="Windows设置本地dns解析"></a>Windows设置本地dns解析</h3><blockquote><p>C:\Windows\System32\drivers\etc\hosts</p><p>添加内容：192.168.102.130 hadoop</p></blockquote><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri Feb 14 2020 01:46:27 GMT+0800 (GMT+08:00) --&gt;&lt;h3 id=&quot;安装时注意事项-用VMware安装centos-大数据虚拟机&quot;&gt;&lt;a href=&quot;#安装时注意事项-用VMware安装centos-大
      
    
    </summary>
    
    
      <category term="部署" scheme="http://kiedeng.github.io/categories/%E9%83%A8%E7%BD%B2/"/>
    
    
      <category term="Linux" scheme="http://kiedeng.github.io/tags/Linux/"/>
    
      <category term="VMware" scheme="http://kiedeng.github.io/tags/VMware/"/>
    
      <category term="centos" scheme="http://kiedeng.github.io/tags/centos/"/>
    
  </entry>
  
  <entry>
    <title>git的使用</title>
    <link href="http://kiedeng.github.io/2020/02/10/git%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
    <id>http://kiedeng.github.io/2020/02/10/git%E7%9A%84%E4%BD%BF%E7%94%A8/</id>
    <published>2020-02-10T03:44:17.000Z</published>
    <updated>2020-02-10T05:13:33.539Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Feb 14 2020 01:46:27 GMT+0800 (GMT+08:00) --><h3 id="Git的优势："><a href="#Git的优势：" class="headerlink" title="Git的优势："></a>Git的优势：</h3><blockquote><p>大部分操作在本地完成，不需要联网</p><p>完整性保证</p><p>尽可能添加数据而不是删除或修改数据</p><p>分支操作非常快捷流畅</p><p>与Linux命令全面兼容</p></blockquote><h3 id="代码托管中心"><a href="#代码托管中心" class="headerlink" title="代码托管中心"></a>代码托管中心</h3><blockquote><p>代码托管中心的任务：维护远程库</p><p>局域网环境：GitLab服务器</p><p>外网环境：GitHub，码云</p></blockquote><h3 id="Git命令行操作"><a href="#Git命令行操作" class="headerlink" title="Git命令行操作"></a>Git命令行操作</h3><ol><li><p>本库初始化（选择文件夹进行初始化）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br></pre></td></tr></table></figure></li><li><p>设置签名</p><ul><li><p>作用：区分不同开发者的身份</p></li><li><p>辨析：这里设置的签名和登录远程库(代码托管中心)的账号、密码没有任何关<br>系。</p></li><li><p>命令：</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">项目级别&#x2F;仓库级别：仅在当前本地库范围内有效</span><br><span class="line">git config user.name kiedeng</span><br><span class="line">git config user.email kiedeng@qq.com</span><br><span class="line">信息保存位置：.&#x2F;.git&#x2F;config 文件</span><br><span class="line"></span><br><span class="line">系统用户级别：登录当前操作系统的用户范围</span><br><span class="line">git config --global user.name tom_glb</span><br><span class="line">git config --global goodMorning_pro@atguigu.com</span><br><span class="line">信息保存位置：~&#x2F;.gitconfig 文件</span><br></pre></td></tr></table></figure></li><li><p>基本操作</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 状态查看</span><br><span class="line">git status</span><br><span class="line"># 添加 (将工作区的文件或目录提交到暂存区)</span><br><span class="line">git [filename]</span><br><span class="line"># 提交 (将暂存区的文件提交的本地库)</span><br><span class="line">git commit -m &quot;commit message&quot; [filename]</span><br><span class="line"># 查看历史版本</span><br><span class="line">git log</span><br><span class="line">git reflog</span><br><span class="line"># 版本的前进与后退（基于索引值操作）</span><br><span class="line">git reset --hard [局部索引值]</span><br><span class="line">git reset --hard a6ace91</span><br></pre></td></tr></table></figure></li><li><p>分支管理</p><ul><li>分支：在版本控制过程中，使用多条线同时推进多个任务。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 创建分支</span><br><span class="line">git branch [分支名]</span><br><span class="line"># 查看分支</span><br><span class="line">git branch -v</span><br><span class="line"># 切换分支</span><br><span class="line">git checkout [分支名]</span><br><span class="line"># 合并</span><br><span class="line">git merge [被和并的分支名]</span><br></pre></td></tr></table></figure></li></ol><h3 id="GitHub"><a href="#GitHub" class="headerlink" title="GitHub"></a>GitHub</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 查看所有远程地址别名</span><br><span class="line">git remote -v</span><br><span class="line"># 创建远程库地址别名</span><br><span class="line">git remote add [别名] [远程地址]</span><br><span class="line"># 推送 (将本地库上传到github仓库)</span><br><span class="line">git push [别名] [分支名]</span><br><span class="line"># 克隆(这样克隆：把远程库下载到本地，初始化本地库，创建别名)</span><br><span class="line">git origin [远程地址]</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri Feb 14 2020 01:46:27 GMT+0800 (GMT+08:00) --&gt;&lt;h3 id=&quot;Git的优势：&quot;&gt;&lt;a href=&quot;#Git的优势：&quot; class=&quot;headerlink&quot; title=&quot;Git的优势：&quot;&gt;&lt;/a&gt;
      
    
    </summary>
    
    
      <category term="使用手册" scheme="http://kiedeng.github.io/categories/%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/"/>
    
    
      <category term="Git" scheme="http://kiedeng.github.io/tags/Git/"/>
    
      <category term="GitHub" scheme="http://kiedeng.github.io/tags/GitHub/"/>
    
  </entry>
  
  <entry>
    <title>hexo搭建</title>
    <link href="http://kiedeng.github.io/2020/02/10/hexo%E6%90%AD%E5%BB%BA/"/>
    <id>http://kiedeng.github.io/2020/02/10/hexo%E6%90%AD%E5%BB%BA/</id>
    <published>2020-02-10T01:10:23.000Z</published>
    <updated>2020-02-10T03:42:36.626Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Feb 14 2020 01:46:27 GMT+0800 (GMT+08:00) --><p><strong>官方文档</strong>： <a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener" title="文档">链接</a></p><h3 id="一，使用Windows完成本地部署"><a href="#一，使用Windows完成本地部署" class="headerlink" title="一，使用Windows完成本地部署"></a>一，使用Windows完成本地部署</h3><ol><li><p>安装node.js和git,默认安装方式即可</p></li><li><p>安装hexo，打开cmd执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-cli -g</span><br></pre></td></tr></table></figure></li><li><p>cmd移动到选择的一个文件夹，比如：d:\blog(下面全部假设初始化在本路径),进行hexo的初始化</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo init blog</span><br></pre></td></tr></table></figure></li><li><p>在此目录安装</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install</span><br></pre></td></tr></table></figure></li><li><p>启动服务器，访问的默认地址：<a href="http://localhost:4000/" target="_blank" rel="noopener">http://localhost:4000/</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></table></figure></li></ol><h3 id="二，使用GitHub完成远程部署"><a href="#二，使用GitHub完成远程部署" class="headerlink" title="二，使用GitHub完成远程部署"></a>二，使用GitHub完成远程部署</h3><ol><li><p>注册，登录github</p></li><li><p>新建仓库步骤如下：</p><blockquote><p>点击右上角+号，new repository，在Repository name处填 （你的gitusername）.github.io（比如：kiedeng.github.io），然后直接点Create repository</p></blockquote></li><li><p>在你初始化的路径（比如的d:\blog）下有一个_config.xml,用记事本打开此文件，最后几行添加github信息</p><blockquote><p>#(对于repo，比如：<a href="https://github.com/kiedeng/kiedeng.github.io.git" target="_blank" rel="noopener">https://github.com/kiedeng/kiedeng.github.io.git</a>)</p><p>deploy:<br>type: git<br>repo: <a href="https://github.com/" target="_blank" rel="noopener">https://github.com/</a>( yours username)/（your username）.github.io.git<br>branck: master</p></blockquote></li><li><p>将cmd移动到d:\blog下，安装</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure></li><li><p>执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 清理</span><br><span class="line">hexo clean</span><br><span class="line"># 生成静态文件</span><br><span class="line">hexo generate</span><br><span class="line"># 上传</span><br><span class="line">hexo deploy</span><br></pre></td></tr></table></figure></li><li><p>在弹出的git窗口中输入你的GitHub邮箱和密码</p></li><li><p>部署完成，等待一会，使用比如：<a href="http://kiedeng.github.io/访问">http://kiedeng.github.io/访问</a></p></li></ol><h3 id="三，更换hexo主题"><a href="#三，更换hexo主题" class="headerlink" title="三，更换hexo主题"></a>三，更换hexo主题</h3><ol><li><p>找到hexo的主题</p><p>推荐主题：<a href="https://blog.csdn.net/zgd826237710/article/details/99671027" target="_blank" rel="noopener">链接</a></p></li><li><p>选择一款，到达它们的github仓库（如果该主题作者的有文档，按文档即可完成更换）</p></li><li><p>将该主题下载下来（克隆也行），解压到d:\blog\themes,将该文件目录更名，比如：kiedeng</p></li><li><p>打开d:\blog_config.xml,将theme: 后面的参数改为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theme: kiedeng</span><br></pre></td></tr></table></figure></li><li><p>然后就可以部署和上传了</p></li></ol><h3 id="四，绑定域名"><a href="#四，绑定域名" class="headerlink" title="四，绑定域名"></a>四，绑定域名</h3><ol><li>选择一个合适的域名，买下域名</li><li>在域名的详细界面，打开解析</li><li>设置dns解析</li></ol><img src="https://kiedeng.github.io/picture/hexo_picture/hexo_搭建/dns_config.png" width="" height="" align="right"><ol start="4"><li>在d:\blog\source目录下，新建一个叫CNAME的文件（<strong>强调：</strong>不能有后缀），里面的内容为你的域名，比如:<a href="http://www.kangdong.store" target="_blank" rel="noopener">www.kangdong.store</a></li><li>等待一小会即可进行访问</li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Fri Feb 14 2020 01:46:27 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;strong&gt;官方文档&lt;/strong&gt;： &lt;a href=&quot;https://hexo.io/zh-cn/docs/&quot; target=&quot;_bl
      
    
    </summary>
    
    
      <category term="部署" scheme="http://kiedeng.github.io/categories/%E9%83%A8%E7%BD%B2/"/>
    
    
      <category term="教程" scheme="http://kiedeng.github.io/tags/%E6%95%99%E7%A8%8B/"/>
    
      <category term="hexo" scheme="http://kiedeng.github.io/tags/hexo/"/>
    
  </entry>
  
</feed>
