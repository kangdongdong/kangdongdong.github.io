{"meta":{"title":"学习印记","subtitle":"","description":"","author":"康栋","url":"https://blog.kiedeng.site","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2020-02-09T14:37:29.946Z","updated":"2020-02-09T14:37:29.946Z","comments":false,"path":"/404.html","permalink":"https://blog.kiedeng.site/404.html","excerpt":"","text":""},{"title":"简介","date":"2021-03-05T13:56:45.863Z","updated":"2021-03-05T13:56:45.863Z","comments":false,"path":"about/index.html","permalink":"https://blog.kiedeng.site/about/index.html","excerpt":"","text":"教育2017.09.09 - 2021.06.06郑州轻工业大学本科 计算机科学与技术ACM集训队成员工作数据的离线数据处理和实时处理hadoop, spark,linux, scala附录"},{"title":"友情链接","date":"2021-03-05T12:44:49.227Z","updated":"2020-02-09T03:33:37.348Z","comments":true,"path":"links/index.html","permalink":"https://blog.kiedeng.site/links/index.html","excerpt":"","text":""},{"title":"书单","date":"2020-02-09T06:13:49.098Z","updated":"2020-02-09T03:33:37.347Z","comments":false,"path":"books/index.html","permalink":"https://blog.kiedeng.site/books/index.html","excerpt":"","text":""},{"title":"分类","date":"2020-02-09T15:25:56.144Z","updated":"2020-02-09T03:33:37.347Z","comments":false,"path":"categories/index.html","permalink":"https://blog.kiedeng.site/categories/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2020-02-09T15:26:19.807Z","updated":"2020-02-09T15:26:19.807Z","comments":false,"path":"repository/index.html","permalink":"https://blog.kiedeng.site/repository/index.html","excerpt":"","text":""},{"title":"tags","date":"2021-03-05T08:28:42.000Z","updated":"2021-03-05T08:28:42.193Z","comments":true,"path":"tags/index-1.html","permalink":"https://blog.kiedeng.site/tags/index-1.html","excerpt":"","text":""},{"title":"标签","date":"2021-03-05T12:46:33.475Z","updated":"2021-03-05T12:46:33.475Z","comments":false,"path":"tags/index.html","permalink":"https://blog.kiedeng.site/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"数据采集模块","slug":"数据采集模块","date":"2020-05-17T07:01:50.000Z","updated":"2020-05-17T09:14:14.771Z","comments":true,"path":"2020/05/17/数据采集模块/","link":"","permalink":"https://blog.kiedeng.site/2020/05/17/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E6%A8%A1%E5%9D%97/","excerpt":"","text":"一，hadoop安装1 HDFS存储多目录在hdfs-site.xml中操作1234&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;&#x2F;name&gt;&lt;value&gt;file:&#x2F;&#x2F;&#x2F;$&#123;hadoop.tmp.dir&#125;&#x2F;dfs&#x2F;data1,file:&#x2F;&#x2F;&#x2F;hd2&#x2F;dfs&#x2F;data2,file:&#x2F;&#x2F;&#x2F;hd3&#x2F;dfs&#x2F;data3,file:&#x2F;&#x2F;&#x2F;hd4&#x2F;dfs&#x2F;data4&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;数据平衡指令123bin&#x2F;start-balancer.sh –threshold 10## 对于参数10，代表的是集群中各个节点的磁盘空间利用率相差不超过10%bin&#x2F;stop-balancer.sh2 支持LZO压缩配置将编译好的hadoop-lzo-0.4.20.jar 放入hadoop-2.7.2/share/hadoop/common/，并且分发给其他hadoop机器core-site.xml增加配置支持LZO压缩，并分发给其他机器123456789101112131415161718192021&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt;&lt;property&gt;&lt;name&gt;io.compression.codecs&lt;/name&gt;&lt;value&gt;org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt; &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;重启集群3 LZO创建索引LZO的可切片特性依赖于索引，所以需要为LZO压缩文件创建索引123hadoop jar &#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;share&#x2F;hadoop&#x2F;common&#x2F;hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer &#x2F;input&#x2F;bigtable.lzo# hadoop jar (lzo的jar文件) com.hadoop.compression.lzo.DistributedLzoIndexer (输出文件)4 基准测试1）测试写性能1hadoop jar &#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-client-jobclient-2.7.2-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 128MB2）测试读性能1hadoop jar &#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-client-jobclient-2.7.2-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 128MB3）测试删除生成数据1hadoop jar &#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-client-jobclient-2.7.2-tests.jar TestDFSIO -clean5 Hadoop参数调优1 hdfs-site.xml12345dfs.namenode.handler.count&#x3D;20 * log2(Cluster Size)，比如集群规模为8台时，此参数设置为60NameNode有一个工作线程池，用来处理不同DataNode的并发心跳以及客户端并发的元数据操作。对于大集群或者有大量客户端的集群来说，通常需要增大参数dfs.namenode.handler.count的默认值10。设置该值的一般原则是将其设置为集群大小的自然对数乘以20，即20logN，N为集群大小。2 参数调优yarn-site.xml1234567内存利用率不够。这个一般是Yarn的2个配置造成的，单个任务可以申请的最大内存大小，和Hadoop单个节点可用内存大小。调节这两个参数能提高系统内存的利用率。（a）yarn.nodemanager.resource.memory-mb表示该节点上YARN可使用的物理内存总量，默认是8192（MB），注意，如果你的节点内存资源不够8GB，则需要调减小这个值，而YARN不会智能的探测节点的物理内存总量。（b）yarn.scheduler.maximum-allocation-mb单个任务可申请的最多物理内存量，默认是8192（MB）。3 Hadoop宕机如果MR造成系统宕机。此时要控制Yarn同时运行的任务数，和每个任务申请的最大内存。如果写入文件过量造成NameNode宕机。那么调高Kafka的存储大小，控制从Kafka到HDFS的写入速度。二，Zookeeper安装三，日志生成四，采集日志Flume五，Kafka安装六","categories":[{"name":"hadoop","slug":"hadoop","permalink":"https://blog.kiedeng.site/categories/hadoop/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"安装superset","slug":"安装superset","date":"2020-05-17T02:31:34.000Z","updated":"2020-05-17T02:47:16.263Z","comments":true,"path":"2020/05/17/安装superset/","link":"","permalink":"https://blog.kiedeng.site/2020/05/17/%E5%AE%89%E8%A3%85superset/","excerpt":"","text":"概述：Apache Superset是一个开源的、现代的、轻量级BI分析工具，能够对接多种数据源、拥有丰富的图标展示形式、支持自定义仪表盘，且拥有友好的用户界面，十分易用。1 安装python环境安装Miniconda1）下载Miniconda（Python3版本）下载地址：https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh2）安装Miniconda(1) 执行以下命令进行安装，并按照提示操作，直到安装完成。1bash Miniconda3-latest-Linux-x86_64.sh(2) 安装过程中，可以指定安装路径(3) 配置Miniconda的环境变量（可以不配，在安装路径的路径）(4)取消激活base环境1conda config --set auto_activate_base false创建Python3.6环境1 配置conda国内镜像123conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;freeconda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;mainconda config --set show_channel_urls yes2 创建Python3.6环境12345conda create --name superset python&#x3D;3.6说明：conda环境管理常用命令创建环境：conda create -n env_name查看所有环境：conda info --envs删除一个环境：conda remove -n env_name --all3 激活superset环境12conda activate supersetconda deactivate(退出激活)2 Superset部署1 安装依赖12sudo yum install -y python-setuptoolssudo yum install -y gcc gcc-c++ libffi-devel python-devel python-pip python-wheel openssl-devel cyrus-sasl-devel openldap-devel2 安装superset123456789101112131415## 安装（更新）setuptools和pippip install --upgrade setuptools pip -i https:&#x2F;&#x2F;pypi.douban.com&#x2F;simple&#x2F;## 安装Supetsetpip install apache-superset -i https:&#x2F;&#x2F;pypi.douban.com&#x2F;simple&#x2F;## 初始化Supetset数据库superset db upgrade## 创建管理员用户export FLASK_APP&#x3D;supersetflask fab create-admin## Superset初始化superset init3 启动superset1 安装gunicorn1pip install gunicorn -i https:&#x2F;&#x2F;pypi.douban.com&#x2F;simple&#x2F;2 启动superset（确保环境为superset）1234567gunicorn --workers 5 --timeout 120 --bind hadoop102:8787 superset:app --daemon 说明：--workers：指定进程个数--timeout：worker进程超时时间，超时会自动重启--bind：绑定本机地址，即为Superset访问地址--daemon：后台运行3 停止superseet1234ps -ef | awk &#39;&#x2F;gunicorn&#x2F; &amp;&amp; !&#x2F;awk&#x2F;&#123;print $2&#125;&#39; | xargs kill -9退出环境conda deactivate4 登录supserset1默认的端口为87873 Superset的使用","categories":[{"name":"安装","slug":"安装","permalink":"https://blog.kiedeng.site/categories/%E5%AE%89%E8%A3%85/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"azkaban安装部署","slug":"azkaban安装部署","date":"2020-05-15T09:12:43.000Z","updated":"2020-05-16T10:01:11.193Z","comments":true,"path":"2020/05/15/azkaban安装部署/","link":"","permalink":"https://blog.kiedeng.site/2020/05/15/azkaban%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/","excerpt":"","text":"生成密钥库所出现的问题补充：keytool -genkey -alias tomcat1 -keyalg RSA -keystore second.keystore参考链接：链接配置邮箱1234567# mail settingsmail.sender&#x3D;572245955@qq.commail.host&#x3D;smtp.qq.commail.user&#x3D;572245955@qq.commail.password&#x3D;(填写qq的授权码)job.failure.email&#x3D;job.success.email&#x3D;","categories":[{"name":"Azkaba","slug":"Azkaba","permalink":"https://blog.kiedeng.site/categories/Azkaba/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"常见命令行操作","slug":"常见命令行操作","date":"2020-05-13T08:25:07.000Z","updated":"2020-05-13T09:40:20.395Z","comments":true,"path":"2020/05/13/常见命令行操作/","link":"","permalink":"https://blog.kiedeng.site/2020/05/13/%E5%B8%B8%E8%A7%81%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/","excerpt":"","text":"Kafka常见命令1234567启动生产者bin&#x2F;kafka-console-producer.sh \\--broker-list hadoop102:9092 --topic first启动消费者（zookeeper存储offset）bin&#x2F;kafka-console-consumer.sh --topic atguigu --zookeeper hadoop102:2181启动消费者（本地存储offset）bin&#x2F;kafka-console-consumer.sh --topic shangguigu --bootstrap-server hadoop102:9092","categories":[{"name":"大数据","slug":"大数据","permalink":"https://blog.kiedeng.site/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"主题的美化","slug":"主题的美化","date":"2020-05-05T11:28:19.000Z","updated":"2020-05-06T13:22:42.344Z","comments":true,"path":"2020/05/05/主题的美化/","link":"","permalink":"https://blog.kiedeng.site/2020/05/05/%E4%B8%BB%E9%A2%98%E7%9A%84%E7%BE%8E%E5%8C%96/","excerpt":"","text":"一，增加天气效果打开心知天气的官网，注册账号，并开通免费版服务。之后在/handsome/component/headnav.php中搜索，在搜索到的地方的前一行添加如下代码，并把其中的公钥和私钥修改为你自己的即可。12345678910111213141516171819&lt;!-- 知心天气--&gt; &lt;div id&#x3D;&quot;tp-weather-widget&quot; class&#x3D;&quot;navbar-form navbar-form-sm navbar-left shift&quot;&gt;&lt;&#x2F;div&gt;&lt;script&gt;(function(T,h,i,n,k,P,a,g,e)&#123;g&#x3D;function()&#123;P&#x3D;h.createElement(i);a&#x3D;h.getElementsByTagName(i)[0];P.src&#x3D;k;P.charset&#x3D;&quot;utf-8&quot;;P.async&#x3D;1;a.parentNode.insertBefore(P,a)&#125;;T[&quot;ThinkPageWeatherWidgetObject&quot;]&#x3D;n;T[n]||(T[n]&#x3D;function()&#123;(T[n].q&#x3D;T[n].q||[]).push(arguments)&#125;);T[n].l&#x3D;+new Date();if(T.attachEvent)&#123;T.attachEvent(&quot;onload&quot;,g)&#125;else&#123;T.addEventListener(&quot;load&quot;,g,false)&#125;&#125;(window,document,&quot;script&quot;,&quot;tpwidget&quot;,&quot;&#x2F;&#x2F;widget.seniverse.com&#x2F;widget&#x2F;chameleon.js&quot;))&lt;&#x2F;script&gt;&lt;script&gt;tpwidget(&quot;init&quot;, &#123; &quot;flavor&quot;: &quot;slim&quot;, &quot;location&quot;: &quot;WX4FBXXFKE4F&quot;, &quot;geolocation&quot;: &quot;enabled&quot;, &quot;language&quot;: &quot;auto&quot;, &quot;unit&quot;: &quot;c&quot;, &quot;theme&quot;: &quot;chameleon&quot;, &quot;container&quot;: &quot;tp-weather-widget&quot;, &quot;bubble&quot;: &quot;enabled&quot;, &quot;alarmType&quot;: &quot;badge&quot;, &quot;color&quot;: &quot;#C6C6C6&quot;, &quot;uid&quot;: &quot;你的公钥&quot;, &quot;hash&quot;: &quot;你的私钥&quot;&#125;);tpwidget(&quot;show&quot;);&lt;&#x2F;script&gt;&lt;!-- 心知结束--&gt;二，美化123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118&#x2F;* * 文章一二三四级标题美化 *&#x2F;#post-content h1 &#123; font-size: 30px&#125;#post-content h2 &#123; position: relative; margin: 20px 0 32px!important; font-size: 1.55em;&#125;#post-content h3 &#123; font-size: 20px&#125;#post-content h4 &#123; font-size: 15px&#125;#post-content h2::after &#123; transition: all .35s; content: &quot;&quot;; position: absolute; background: linear-gradient(#3c67bd8c 30%,#3c67bd 70%); width: 1em; left: 0; box-shadow: 0 3px 3px rgba(32,160,255,.4); height: 3px; bottom: -8px;&#125;#post-content h2::before &#123; content: &quot;&quot;; width: 100%; border-bottom: 1px solid #eee; bottom: -7px; position: absolute&#125;#post-content h2:hover::after &#123; width: 2.5em;&#125;#post-content h1,#post-content h2,#post-content h3,#post-content h4,#post-content h5,#post-content h6 &#123; color: #666; line-height: 1.4; font-weight: 700; margin: 30px 0 10px 0&#125; &#x2F;* * 首页文章列表悬停上浮 *&#x2F;.blog-post .panel:not(article) &#123; transition: all 0.3s;&#125;.blog-post .panel:not(article):hover &#123; transform: translateY(-10px); box-shadow: 0 8px 10px rgba(73, 90, 47, 0.47);&#125;&#x2F;* * 文章内头图和文章图片悬停放大并将超出范围隐藏 *&#x2F;.entry-thumbnail &#123; overflow: hidden;&#125;#post-content img &#123; border-radius: 10px; transition: 0.5s;&#125;#post-content img:hover &#123; transform: scale(1.05);&#125;&#x2F;* *首页文章图片获取焦点放大 *&#x2F;.item-thumb &#123; cursor: pointer; transition: all 0.6s;&#125;.item-thumb:hover &#123; transform: scale(1.05);&#125;.item-thumb-small &#123; cursor: pointer; transition: all 0.6s;&#125;.item-thumb-small:hover &#123; transform: scale(1.05);&#125;&#x2F;*文章内打赏图标跳动*&#x2F;.btn - pay &#123; animation: star 0.5s ease - in-out infinite alternate;&#125;@keyframes star &#123; from &#123; transform: scale(1); &#125; to &#123; transform: scale(1.1); &#125;&#125;&#x2F;* *修改字体 *&#x2F;*&#123;font-family: &#39;Noto Serif SC&#39;, serif;font-family: &#39;Fira Code&#39;, monospace;&#125;三，头像呼吸光环和鼠标悬停旋转放大1234567891011121314151617181920212223242526272829303132.img-full &#123; width: 100px; border-radius: 50%; animation: light 4s ease-in-out infinite; transition: 0.5s;&#125;.img-full:hover &#123; transform: scale(1.15) rotate(720deg);&#125;@keyframes light &#123; 0% &#123; box-shadow: 0 0 4px #f00; &#125; 25% &#123; box-shadow: 0 0 16px #0f0; &#125; 50% &#123; box-shadow: 0 0 4px #00f; &#125; 75% &#123; box-shadow: 0 0 16px #0f0; &#125; 100% &#123; box-shadow: 0 0 4px #f00; &#125;&#125;四，qq头像的图片链接1https:&#x2F;&#x2F;q1.qlogo.cn&#x2F;g?b&#x3D;qq&amp;nk&#x3D;572245955&amp;s&#x3D;640五，自定义js美化1234567&#x2F;*彩色标签云*&#x2F;let tags &#x3D; document.querySelectorAll(&quot;#tag_cloud-2 a&quot;);let colorArr &#x3D; [&quot;#428BCA&quot;, &quot;#AEDCAE&quot;, &quot;#ECA9A7&quot;, &quot;#DA99FF&quot;, &quot;#FFB380&quot;, &quot;#D9B999&quot;];tags.forEach(tag &#x3D;&gt; &#123; tagsColor &#x3D; colorArr[Math.floor(Math.random() * colorArr.length)]; tag.style.backgroundColor &#x3D; tagsColor;&#125;);六，博主介绍特效12&lt;!--博主介绍的闪字特效--&gt;&lt;span class&#x3D;&quot;text-muted text-xs block&quot;&gt;&lt;div id&#x3D;&quot;chakhsu&quot;&gt;&lt;&#x2F;div&gt; &lt;script&gt; var chakhsu &#x3D; function (r) &#123;function t() &#123;return b[Math.floor(Math.random() * b.length)]&#125; function e() &#123;return String.fromCharCode(94 * Math.random() + 33)&#125; function n(r) &#123;for (var n &#x3D; document.createDocumentFragment(), i &#x3D; 0; r &gt; i; i++) &#123; var l &#x3D; document.createElement(&quot;span&quot;); l.textContent &#x3D; e(), l.style.color &#x3D; t(), n.appendChild(l) &#125; return n&#125;function i() &#123;var t &#x3D; o[c.skillI]; c.step ? c.step-- : (c.step &#x3D; g, c.prefixP &lt; l.length ? (c.prefixP &gt;&#x3D; 0 &amp;&amp; (c.text +&#x3D; l[c.prefixP]), c.prefixP++) : &quot;forward&quot; &#x3D;&#x3D;&#x3D; c.direction ? c.skillP &lt; t.length ? (c.text +&#x3D; t[c.skillP], c.skillP++) : c.delay ? c.delay-- : (c.direction &#x3D; &quot;backward&quot;, c.delay &#x3D; a) : c.skillP &gt; 0 ? (c.text &#x3D; c.text.slice(0, -1), c.skillP--) : (c.skillI &#x3D; (c.skillI + 1) % o.length, c.direction &#x3D; &quot;forward&quot;)), r.textContent &#x3D; c.text, r.appendChild(n(c.prefixP &lt; l.length ? Math.min(s, s + c.prefixP) : Math.min(s, t.length - c.skillP))), setTimeout(i, d) &#125; &#x2F;*以下内容自定义修改*&#x2F; var l &#x3D; &quot;&quot;, o &#x3D; [&quot;Keep Fighting&quot; ].map(function (r) &#123;return r + &quot;&quot;&#125;), a &#x3D; 2, g &#x3D; 1, s &#x3D; 5, d &#x3D; 75, b &#x3D; [&quot;rgb(110,64,170)&quot;, &quot;rgb(150,61,179)&quot;, &quot;rgb(191,60,175)&quot;, &quot;rgb(228,65,157)&quot;, &quot;rgb(254,75,131)&quot;, &quot;rgb(255,94,99)&quot;, &quot;rgb(255,120,71)&quot;, &quot;rgb(251,150,51)&quot;, &quot;rgb(226,183,47)&quot;, &quot;rgb(198,214,60)&quot;, &quot;rgb(175,240,91)&quot;, &quot;rgb(127,246,88)&quot;, &quot;rgb(82,246,103)&quot;, &quot;rgb(48,239,130)&quot;, &quot;rgb(29,223,163)&quot;, &quot;rgb(26,199,194)&quot;, &quot;rgb(35,171,216)&quot;, &quot;rgb(54,140,225)&quot;, &quot;rgb(76,110,219)&quot;, &quot;rgb(96,84,200)&quot;], c &#x3D; &#123;text: &quot;&quot;, prefixP: -s, skillI: 0, skillP: 0, direction: &quot;forward&quot;, delay: a, step: g&#125;; i() &#125;; chakhsu(document.getElementById(&#39;chakhsu&#39;)); &lt;&#x2F;script&gt; &lt;&#x2F;span&gt; &lt;&#x2F;span&gt;七，倒计时1&lt;style&gt; .gn_box&#123; border: none; border-radius: 15px; &#125; .gn_box &#123; padding: 10px 14px; margin: 10px; margin-bottom: 20px; text-align: center; background-color: #fff; &#125; #t_d&#123; color: #982585; font-size: 18px; &#125; #t_h&#123; color: #8f79c1; font-size: 18px; &#125; #t_m&#123; color: #65b4b5; font-size: 18px; &#125; #t_s&#123; color: #83caa3; font-size: 18px; &#125; &lt;&#x2F;style&gt; &lt;div class&#x3D;&quot;gn_box&quot;&gt; &lt;h1 style&#x3D;&quot;font-size:1em;&quot;&gt;&lt;font color&#x3D;&quot;#E80017&quot;&gt;2&lt;&#x2F;font&gt; &lt;font color&#x3D;&quot;#D1002E&quot;&gt;0&lt;&#x2F;font&gt; &lt;font color&#x3D;&quot;#BA0045&quot;&gt;2&lt;&#x2F;font&gt; &lt;font color&#x3D;&quot;#A3005C&quot;&gt;0&lt;&#x2F;font&gt; &lt;font color&#x3D;&quot;#8C0073&quot;&gt;年&lt;&#x2F;font&gt; &lt;font color&#x3D;&quot;#75008A&quot;&gt;-&lt;&#x2F;font&gt; &lt;font color&#x3D;&quot;#5E00A1&quot;&gt;秋&lt;&#x2F;font&gt; &lt;font color&#x3D;&quot;#4700B8&quot;&gt;招&lt;&#x2F;font&gt; &lt;font color&#x3D;&quot;#3000CF&quot;&gt;倒&lt;&#x2F;font&gt; &lt;font color&#x3D;&quot;#1900E6&quot;&gt;计&lt;&#x2F;font&gt; &lt;font color&#x3D;&quot;#0200FD&quot;&gt;时&lt;&#x2F;font&gt;&lt;&#x2F;h1&gt; &lt;center&gt; &lt;div id&#x3D;&quot;CountMsg&quot; class&#x3D;&quot;HotDate&quot;&gt; &lt;span id&#x3D;&quot;t_d&quot;&gt; 天&lt;&#x2F;span&gt; &lt;span id&#x3D;&quot;t_h&quot;&gt; 时&lt;&#x2F;span&gt;&lt;span id&#x3D;&quot;t_m&quot;&gt; 分&lt;&#x2F;span&gt; &lt;span id&#x3D;&quot;t_s&quot;&gt; 秒&lt;&#x2F;span&gt; &lt;&#x2F;div&gt; &lt;&#x2F;center&gt; &lt;script type&#x3D;&quot;text&#x2F;javascript&quot;&gt; function getRTime() &#123; var EndTime &#x3D; new Date(&#39;2020&#x2F;09&#x2F;1 00:00:00&#39;); var NowTime &#x3D; new Date(); var t &#x3D; EndTime.getTime() - NowTime.getTime(); var d &#x3D; Math.floor(t &#x2F; 1000 &#x2F; 60 &#x2F; 60 &#x2F; 24); var h &#x3D; Math.floor(t &#x2F; 1000 &#x2F; 60 &#x2F; 60 % 24); var m &#x3D; Math.floor(t &#x2F; 1000 &#x2F; 60 % 60); var s &#x3D; Math.floor(t &#x2F; 1000 % 60); var day &#x3D; document.getElementById(&quot;t_d&quot;); if (day !&#x3D; null) &#123; day.innerHTML &#x3D; d + &quot; 天&quot;; &#125; var hour &#x3D; document.getElementById(&quot;t_h&quot;); if (hour !&#x3D; null) &#123; hour.innerHTML &#x3D; h + &quot; 时&quot;; &#125; var min &#x3D; document.getElementById(&quot;t_m&quot;); if (min !&#x3D; null) &#123; min.innerHTML &#x3D; m + &quot; 分&quot;; &#125; var sec &#x3D; document.getElementById(&quot;t_s&quot;); if (sec !&#x3D; null) &#123; sec.innerHTML &#x3D; s + &quot; 秒&quot;; &#125; &#125; setInterval(getRTime, 1000); &lt;&#x2F;script&gt; &lt;&#x2F;div&gt; &lt;!--首页输出文章--&gt;八，疫情图1&lt;iframe src&#x3D;&quot;https:&#x2F;&#x2F;www.lovestu.com&#x2F;api&#x2F;project&#x2F;cnmapyinqing&#x2F;obj.php&quot; height&#x3D;&quot;500&quot; frameborder&#x3D;&quot;no&quot; border&#x3D;&quot;0&quot; width&#x3D;&quot;100%&quot;&gt; &lt;&#x2F;iframe&gt;参考链接：https://wanghongfeng.cn/handsome-diy.htmlhttps://rehtt.com/index.php/archives/193","categories":[{"name":"typecho","slug":"typecho","permalink":"https://blog.kiedeng.site/categories/typecho/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"ArrayList源码分析","slug":"ArrayList源码分析","date":"2020-04-20T04:22:21.000Z","updated":"2020-04-20T04:22:21.542Z","comments":true,"path":"2020/04/20/ArrayList源码分析/","link":"","permalink":"https://blog.kiedeng.site/2020/04/20/ArrayList%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"","categories":[{"name":"未分类","slug":"未分类","permalink":"https://blog.kiedeng.site/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"hql常见函数","slug":"hql常见函数","date":"2020-03-19T07:27:35.000Z","updated":"2020-03-19T07:28:42.883Z","comments":true,"path":"2020/03/19/hql常见函数/","link":"","permalink":"https://blog.kiedeng.site/2020/03/19/hql%E5%B8%B8%E8%A7%81%E5%87%BD%E6%95%B0/","excerpt":"","text":"常用日期函数unix_timestamp:返回当前或指定时间的时间戳from_unixtime：将时间戳转为日期格式current_date：当前日期current_timestamp：当前的日期加时间to_date：抽取日期部分year：获取年month：获取月day：获取日hour：获取时minute：获取分second：获取秒weekofyear：当前时间是一年中的第几周dayofmonth：当前时间是一个月中的第几天months_between： 两个日期间的月份add_months：日期加减月datediff：两个日期相差的天数date_add：日期加天数date_sub：日期减天数last_day：日期的当月的最后一天常用取整函数round： 四舍五入ceil： 向上取整floor： 向下取整常用字符串操作函数upper： 转大写lower： 转小写length： 长度trim： 前后去空格lpad： 向左补齐，到指定长度rpad： 向右补齐，到指定长度regexp_replace： SELECT regexp_replace(‘100-200’, ‘(\\d+)’, ‘num’) ；​ 使用正则表达式匹配目标字符串，匹配成功后替换！集合操作size： 集合中元素的个数map_keys： 返回map中的keymap_values: 返回map中的valuearray_contains: 判断array中是否包含某个元素sort_array： 将array中的元素排序","categories":[{"name":"未分类","slug":"未分类","permalink":"https://blog.kiedeng.site/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"HQL常见错误","slug":"HQL常见错误","date":"2020-03-18T14:03:47.000Z","updated":"2020-03-18T15:13:59.298Z","comments":true,"path":"2020/03/18/HQL常见错误/","link":"","permalink":"https://blog.kiedeng.site/2020/03/18/HQL%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/","excerpt":"","text":"行转列过程中分组的时候只写了group，忘记了by子查询不能写分号列名错误：1Error: Error while compiling statement: FAILED: SemanticException [Error 10004]: Line 3:6 Invalid table alias or column reference &#39;orderdata&#39;: (possible column names are: name, orderdate, cost) (state&#x3D;42000,code&#x3D;10004)","categories":[{"name":"hive","slug":"hive","permalink":"https://blog.kiedeng.site/categories/hive/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"简介","slug":"简介","date":"2020-03-18T14:03:31.000Z","updated":"2021-03-05T14:09:02.462Z","comments":true,"path":"2020/03/18/简介/","link":"","permalink":"https://blog.kiedeng.site/2020/03/18/%E7%AE%80%E4%BB%8B/","excerpt":"","text":"教育2017.09.09 - 2021.06.06郑州轻工业大学本科 计算机科学与技术ACM集训队成员工作数据的离线数据处理和实时处理hadoop, spark,linux, scala附录github：githubQQ:572245955","categories":[{"name":"成长","slug":"成长","permalink":"https://blog.kiedeng.site/categories/%E6%88%90%E9%95%BF/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"mongdb导入json文件","slug":"mongdb导入json文件","date":"2020-03-18T04:22:00.000Z","updated":"2020-03-18T04:29:16.461Z","comments":true,"path":"2020/03/18/mongdb导入json文件/","link":"","permalink":"https://blog.kiedeng.site/2020/03/18/mongdb%E5%AF%BC%E5%85%A5json%E6%96%87%E4%BB%B6/","excerpt":"","text":"1 安装mongdb使用宝塔安装mongdb2 配置mongdb注：开放27017端口3 导入数据mongoimport –db mall –collection userData –file /Users/yeo/Desktop/userdata.json注：执行命令在/www/server/mongodb/bin","categories":[{"name":"mongdb","slug":"mongdb","permalink":"https://blog.kiedeng.site/categories/mongdb/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"hive使用mysql编码问题","slug":"hive使用mysql编码问题","date":"2020-03-17T10:58:22.000Z","updated":"2020-03-17T11:07:52.618Z","comments":true,"path":"2020/03/17/hive使用mysql编码问题/","link":"","permalink":"https://blog.kiedeng.site/2020/03/17/hive%E4%BD%BF%E7%94%A8mysql%E7%BC%96%E7%A0%81%E9%97%AE%E9%A2%98/","excerpt":"","text":"修改my.cnf文件123456# 添加init_connect='SET collation_connection = utf8_unicode_ci'init_connect='SET NAMES utf8'character-set-server=utf8collation-server=utf8_unicode_ciskip-character-set-client-handshake重启mysql启动使用bin/hive脚本修改metastore数据库12show variables like &#39;char%&#39;;show variables like &quot;colla%&quot;;使用上面两个命令，查看是否都为utf8的编码，（除character_set _filesystem为binary）如果没有：则12345678修改表字段注解和表注解alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8;alter table TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;修改分区字段注解：alter table PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;alter table PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set utf8;修改索引注解：alter table INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;这样就可以显示中文字符的，不过在hive中插入中文字符还有些问题，只能显示上传文档为中文字符和描述为中文字符的情况","categories":[{"name":"hive","slug":"hive","permalink":"https://blog.kiedeng.site/categories/hive/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"mysql安装","slug":"mysql安装","date":"2020-03-16T10:35:24.000Z","updated":"2020-03-16T11:32:24.560Z","comments":true,"path":"2020/03/16/mysql安装/","link":"","permalink":"https://blog.kiedeng.site/2020/03/16/mysql%E5%AE%89%E8%A3%85/","excerpt":"","text":"检查是否安装Mysql12rpm -qa|grep mysql; &#x2F;&#x2F; 查询是否存在mysqlrpm -e --nodeps mysql-libs; &#x2F;&#x2F; 卸载mysqlrpm 是一个包管理工具-e 卸载程序-qa 查询安装的软件–nodeps 不验证软件包的依赖-i ,–install 安装软件包-v， –verbose 提供更多的详细信息输出-h ，–hash 软件包安装的时候列出哈希标记安装Mysql123456tar -xf mysql-5.7.28-1.el6.x86_64.rpm-bundle.tar &#x2F;&#x2F; 解压&#x2F;&#x2F; 安装[root@hadoop102 software]$ rpm -ivh mysql-community-common-5.7.28-1.el6.x86_64.rpm[root@hadoop102 software]$ rpm -ivh mysql-community-libs-5.7.28-1.el6.x86_64.rpm[root@hadoop102 software]$ rpm -ivh mysql-community-client-5.7.28-1.el6.x86_64.rpm[root@hadoop102 software]$ rpm -ivh mysql-community-server-5.7.28-1.el6.x86_64.rpm修改/etc/my.cnf12[mysqld]explicit_defaults_for_timestamp&#x3D;true &#x2F;&#x2F;显示指定默认值为timestamp类型的字段删除/etc/my.cnf文件中datadir指向的目录下的所有内容:启动Mysql1234567891011121314&#x2F;&#x2F; 初始化mysqld --initialize --user&#x3D;mysql&#x2F;&#x2F; 查看临时密码cat &#x2F;var&#x2F;log&#x2F;mysqld.log &#x2F;&#x2F; 启动mysql服务service mysqld start&#x2F;&#x2F; 登陆mysql mysql -uroot -pEnter password:&#x2F;&#x2F; 修改密码set password &#x3D; password(&quot;新密码&quot;)&#x2F;&#x2F; 修改root用户支持任意IP连接（在user表中）update user set host&#x3D; ‘%’ where user &#x3D; ‘root’;flush privileges ; &#x2F;&#x2F; 刷新配置","categories":[{"name":"Linux","slug":"Linux","permalink":"https://blog.kiedeng.site/categories/Linux/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"大数据相关单词","slug":"大数据相关单词","date":"2020-03-16T10:21:50.000Z","updated":"2020-03-16T16:37:30.491Z","comments":true,"path":"2020/03/16/大数据相关单词/","link":"","permalink":"https://blog.kiedeng.site/2020/03/16/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3%E5%8D%95%E8%AF%8D/","excerpt":"","text":"单词翻译row format delimited fields terminated by “\\t”行格式分隔的字段，以“ \\ t”结尾external外部comment评论partitioned by分区（分目录）sorted by排序clustered by分桶（分文件）serialize序列化deserialize反序列化create table test(​)","categories":[{"name":"hive","slug":"hive","permalink":"https://blog.kiedeng.site/categories/hive/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"Hadoop_HA高可用","slug":"Hadoop-HA高可用","date":"2020-03-08T05:52:37.000Z","updated":"2020-03-08T09:19:37.481Z","comments":true,"path":"2020/03/08/Hadoop-HA高可用/","link":"","permalink":"https://blog.kiedeng.site/2020/03/08/Hadoop-HA%E9%AB%98%E5%8F%AF%E7%94%A8/","excerpt":"","text":"包括HDFS的HA和YARN的HAHDFS-HA的工作机制​ 通过双NameNode消除单节点故障手动故障转移配置：在opt目录下创建ha文件夹，将hadoop复制到ha文件夹下配置hadoop-env.sh1export JAVA_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;jdk1.8.0_144配置core-site.xml1234567891011121314151617181920&lt;configuration&gt;&lt;!-- 把两个NameNode）的地址组装成一个集群mycluster --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;&#x2F;name&gt; &lt;value&gt;hdfs:&#x2F;&#x2F;mycluster&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- 声明journalnode服务器存储目录--&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;&#x2F;name&gt; &lt;value&gt;&#x2F;opt&#x2F;ha&#x2F;hadoop-2.7.2&#x2F;data&#x2F;jn&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;&#x2F;name&gt; &lt;value&gt;&#x2F;opt&#x2F;ha&#x2F;hadoop-2.7.2&#x2F;data&#x2F;tmp&lt;&#x2F;value&gt; &lt;&#x2F;property&gt;&lt;&#x2F;configuration&gt;配置hdfs-site.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&lt;configuration&gt; &lt;!-- 完全分布式集群名称 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;&#x2F;name&gt; &lt;value&gt;mycluster&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- 集群中NameNode节点都有哪些 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.mycluster&lt;&#x2F;name&gt; &lt;value&gt;nn1,nn2&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- nn1的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;&#x2F;name&gt; &lt;value&gt;hadoop102:9000&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- nn2的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;&#x2F;name&gt; &lt;value&gt;hadoop103:9000&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- nn1的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;&#x2F;name&gt; &lt;value&gt;hadoop102:50070&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- nn2的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;&#x2F;name&gt; &lt;value&gt;hadoop103:50070&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- 指定NameNode元数据在JournalNode上的存放位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;&#x2F;name&gt; &lt;value&gt;qjournal:&#x2F;&#x2F;hadoop102:8485;hadoop103:8485;hadoop104:8485&#x2F;mycluster&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;&#x2F;name&gt; &lt;value&gt;sshfence&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- 使用隔离机制时需要ssh无秘钥登录--&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;&#x2F;name&gt; &lt;value&gt;&#x2F;home&#x2F;atguigu&#x2F;.ssh&#x2F;id_rsa&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- 关闭权限检查--&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.enable&lt;&#x2F;name&gt; &lt;value&gt;false&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- 访问代理类：client，mycluster，active配置失败自动切换实现方式--&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;&#x2F;name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;&#x2F;value&gt; &lt;&#x2F;property&gt;&lt;&#x2F;configuration&gt;拷贝到其他节点启动集群各个集群启动JournalNode节点1sbin&#x2F;hadoop-daemon.sh start journalnode在[nn1]上，对其格式化，并启动12bin&#x2F;hdfs namenode -formatsbin&#x2F;hadoop-daemon.sh start namenode在[nn2]上，同步nn1的元数据信息1bin&#x2F;hdfs namenode -bootstrapStandby启动[nn2]1sbin&#x2F;hadoop-daemon.sh start namenode将[nn1]切换为Active1bin&#x2F;hdfs haadmin -transitionToActive nn1启动datanode1sbin&#x2F;hadoop-daemons.sh start datanode查看是否Active1bin&#x2F;hdfs haadmin -getServiceState nn1自动故障转移具体配置（1）在hdfs-site.xml中增加1234&lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;&#x2F;name&gt; &lt;value&gt;true&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;（2）在core-site.xml文件中增加1234&lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;&#x2F;name&gt; &lt;value&gt;hadoop102:2181,hadoop103:2181,hadoop104:2181&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;启动（1）关闭所有HDFS服务：sbin/stop-dfs.sh（2）启动Zookeeper集群：bin/zkServer.sh start（3）初始化HA在Zookeeper中状态：bin/hdfs zkfc -formatZK（4）启动HDFS服务：sbin/start-dfs.sh集群规划：hadoop102hadoop103hadoop104NameNodeNameNodeZKFCZKFCJournalNodeJournalNodeJournalNodeDataNodeDataNodeDataNodeZKZKZKResourceManagerNodeManagerNodeManagerNodeManagerYARN-HA配置yarn-site.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!--启用resourcemanager ha--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--声明两台resourcemanager的地址--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;cluster-yarn1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;hadoop102&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;hadoop103&lt;/value&gt; &lt;/property&gt; &lt;!--指定zookeeper集群的地址--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;hadoop102:2181,hadoop103:2181,hadoop104:2181&lt;/value&gt; &lt;/property&gt; &lt;!--启用自动恢复--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--指定resourcemanager的状态信息存储在zookeeper集群--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;启动YARN（1）在hadoop102中执行：sbin/start-yarn.sh（2）在hadoop103中执行：sbin/yarn-daemon.sh start resourcemanager","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://blog.kiedeng.site/categories/Hadoop/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"shell脚本文件","slug":"shell脚本文件","date":"2020-03-07T16:39:20.000Z","updated":"2020-03-07T16:41:48.903Z","comments":true,"path":"2020/03/08/shell脚本文件/","link":"","permalink":"https://blog.kiedeng.site/2020/03/08/shell%E8%84%9A%E6%9C%AC%E6%96%87%E4%BB%B6/","excerpt":"","text":"myjps1234567#!/bin/bashfor i in hadoop102 hadoop103 hadoop104do echo \"************* $i Jps ***********\" ssh $i /opt/module/jdk1.8.0_144/bin/jpsdonezookeeper集群管理脚本1234567891011121314151617181920212223242526#!/bin/bashif [ $# -eq 0 ]then echo \"No args Input....\"fifor i in hadoop102 hadoop103 hadoop104do case $1 in \"start\") echo \"*****************Start $i Zookeeper *************\" ssh $i /opt/module/zookeeper-3.4.10/bin/zkServer.sh start ;; \"stop\") echo \"*****************Stop $i Zookeeper *************\" ssh $i /opt/module/zookeeper-3.4.10/bin/zkServer.sh stop ;; \"status\") echo \"*****************Status $i Zookeeper *************\" ssh $i /opt/module/zookeeper-3.4.10/bin/zkServer.sh status ;; *) echo \"Input Args Error......\" esacdone文件同步脚本12345678910111213141516171819202122232425#!/bin/bash#1 获取输入参数个数，如果没有参数，直接退出pcount=$#if((pcount==0)); thenecho no args;exit;fi#2 获取文件名称p1=$1fname=`basename $p1`echo fname=$fname#3 获取上级目录到绝对路径pdir=`cd -P $(dirname $p1); pwd`echo pdir=$pdir#4 获取当前用户名称user=`whoami`#5 循环for((host=103; host&lt;105; host++)); do echo ------------------- hadoop$host -------------- rsync -rvl $pdir/$fname $user@hadoop$host:$pdirdone","categories":[{"name":"Linux","slug":"Linux","permalink":"https://blog.kiedeng.site/categories/Linux/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"shell操作","slug":"shell操作","date":"2020-03-07T09:28:34.000Z","updated":"2020-03-08T05:39:36.259Z","comments":true,"path":"2020/03/07/shell操作/","link":"","permalink":"https://blog.kiedeng.site/2020/03/07/shell%E6%93%8D%E4%BD%9C/","excerpt":"","text":"更改权限：chmod 777 [sh文件]案例案例：创建文件，写入数据1234#!&#x2F;bin&#x2F;bashcd &#x2F;home&#x2F;atguigu&#x2F;zzulitouch chuang.txtecho &quot;kangdong&quot; &gt;&gt; chuang.txtShell中的变量系统变量1$HOME,$PWD,$SHELL,$USER等全局变量：export A自定义变量撤销变量：unset 变量声明静态变量：readonly变量，注意：不能unset特殊变量:$ n$n （功能描述：n为数字，$0代表该脚本名称，$1-$9代表第一到第九个参数，十以上的参数，十以上的参数需要用大括号包含，如${10}）特殊变量：$获取所有输入参数个数，常用于循环特殊变量：$ *、$ @​ $ * （功能描述：这个变量代表命令行中所有的参数，$*把所有的参数看成一个整体）​ $ @ （功能描述：这个变量也代表命令行中所有的参数，不过$@把每个参数区分对待）特殊变量：$ ?判断最后一次执行的命令的返回状态，0位成功，非0为失败运算符“$ ((运算式))”或“$[运算式]”expr + , - , *, /, % 加，减，乘，除，取余注意：expr运算符间要有空格条件判断格式：[condition]判断条件：两个整数比较-lt 小于（less than） -le 小于或等于（less equal）-eq等于（equal） -gt大于（greater than）-ge大于等于（greater equal） -ne不等于（Not equal）按照文件权限进行判断-r 读 -w写 -x 执行（execute）按照文件类型进行判断-f 文件存在并且是一个常规文件-e 文件存在 -d为目录流程控制1 if判断12345678#!&#x2F;bin&#x2F;bashif [ $1 -eq &quot;2&quot; ]then echo &quot;2&quot;elif [ $1 -eq &quot;3&quot; ]then echo &quot;3&quot;fi2 case语句注意事项：1) case行尾必须为单词“in”，每一个模式匹配必须以右括号“）”结束。2) 双分号“;;”表示命令序列结束，相当于java中的break。3) 最后的“*）”表示默认模式，相当于java中的default。案例：123456789101112131415#!/bin/bashcase $1 in\"1\") echo \"1 kangdong\";;\"2\") echo \"2 kangdong\";;\"3\") echo \"3 kangdong\";;*) echo \"* kangdong\";;esac3 for循环1234567#&#x2F;bin&#x2F;bashs&#x3D;0for((i&#x3D;0;i&lt;&#x3D;100;i++))do s&#x3D;$[$s+$i]doneecho $s4 比较$ *与$ @未被双引号包含时，都分开输出，当被双引号包含时，“$*”会将所有参数作为一个整体输出123456789101112#!/bin/bashfor i in \"$*\"do echo \"I am $i\"donefor i in \"$@\"do echo \"My name $i\"done5 while循环12345678910#!/bin/bashs=0i=0while [ $i -le 100 ]do s=$[$i+$s] i=$[$i+1]doneecho $sread读取控制台输入read(选项)(参数)​ 选项：-p：指定读取值时的提示符；-t：指定读取值时等待的时间（秒）。参数​ 变量：指定读取值的变量名12read -t 9 -p \"输入值:\" nameecho $name函数1 系统函数basename [string / pathname] [suffix] 求文件名dirname ：求文件的目录2 自定义函数12345678910111213#!/bin/bashfunction sum()&#123; s=0 s=$[$1+$2] echo $s&#125;read -p \"输入s1：\" n1;read -p \"输入s2: \" n2;sum $n1 $n2Shell工具cutcut [选项参数] filename-f 列号 -d 分隔符sedawk一个强大的文本分析工具，把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分再进行分析处理。sort企业真实面试题（重点）《未完，，待续》","categories":[{"name":"Linux","slug":"Linux","permalink":"https://blog.kiedeng.site/categories/Linux/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"Zookeeper","slug":"Zookeeper","date":"2020-03-04T05:09:50.000Z","updated":"2020-03-08T05:30:50.554Z","comments":true,"path":"2020/03/04/Zookeeper/","link":"","permalink":"https://blog.kiedeng.site/2020/03/04/Zookeeper/","excerpt":"","text":"​ Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目。多作为集群提供服务的中间件。​ Zookeeper从设计模式角度来理解，是一个基于观察者模式设计的分布式服务管理框架。应用场景​ 提供的服务包括：统一命名服务，统一配置管理，统一集群管理，服务器节点动态上下线，软负载均衡等。1 统一命名服务​ 在分布式环境下，经常需要对应用/服务进行统一命名，便于识别。2 统一配置管理​ 1）分布式环境下，配置文件同步非常常见​ 2）配置管理可交由ZooKeeper实现3 统一集群管理​ 1）分布式环境下，实时掌握节点的状态是必要的​ 2）ZooKeeper可以实现实时监控节点状态变化4 服务器动态上下线​ 客户端能实时洞察服务器上下线的变化5 软负载均衡​ 在Zookeeper中记录每台服务器的访问数，让访问数最少的服务器去处理最新的客户端请求Zookeeper安装本地模式安装部署1 安装jdk，拷贝Zookeeper，解压到指定目录1[atguigu@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/2 配置修改将conf路径下的zoo_sample.cfg修改为zoo.cfg修改dataDir路径，改为：dataDir=/opt/module/zookeeper-3.4.10/zkData创建zkDataa文件夹3 操作Zookeeper启动：bin/zkServer.sh start查看进程是否启动：jps查看状态：bin/zkServer.sh status启动客户端：bin/zkCli.sh退出：quit停止：bin/zkServer.sh stop四字命令ruok测试服务是否处于正确状态，如果确实如此，那么服务返回 imok ,否则不做任何响应。conf3.3.0版本引入的，打印出服务相关配置的详细信息cons列出所有连接到这台服务器的客户端全部会话详细信息。包括 接收/发送的包数量，会话id，操作延迟、最后的操作执行等等信息crst重置所有连接的连接和会话统计信息dump列出那些比较重要的会话和临时节点。这个命令只能在leader节点上有用envi打印出服务环境的详细信息命令格式：nc localhost 2181注: 使用之前，需要先安装nc，可以使用yum方式进行安装.配置参数解读zoo.cfg参数含义：tickTime：通信心跳数，Zookeeper服务器与客户端心跳时间，单位initLimit=10:LF初始通信时限（第一次连接的超时时间）syncLimit=5：LF同步通信时限（最大响应时间单位）clientPort=2181：客户端连接端口Zookeeper内容原理选举机制半数机制：集群中半数以上机器存活，集群可用。Zookeeper虽然在配置中并没有指定Master和Slave。但是，Zookeeper工作时，是有一个节点为Leader，其他则为Follower，Leader是通过内部的选举机制临时产生的。节点类型持久：客户端和服务器断开连接后，创建的节点不删除短暂：客户端和服务器断开连接后，创建的节点自己删除监听器原理监听器原理详解：首先要有一个main（）线程创建Zookeeper客户端，两个线程，一个负责网络连接通信（connect），一个负责监听（listener）通过connect线程将注册的监听事件发送给Zookeeper。在Zookeeper的注册监听器列表将注册的监听事件添加到列表中监听数据或路径变化，就会将这个消息发送给listener线程listener线程内部调用了process（）方法写数据流程Client发送一个写请求如果Server不是Leader，在转给Leader，请求广播给各个Server当Leader收到大多数Server数据写成功了，就说明数据写成功了server通知Client数据写成功了Zookeeper实战1 集群规划在hadoop102,103,104三个节点上部署Zookeeper2 解压安装（1）解压到/opt/module目录下（2）同步到hadoop103,1043 配置服务器编号（1）在zookeeper目录下创建zkData（2）在zkData目录下创建一个myid的文件（3）编写myid文件，添加编号（4）拷贝配置好的zookeeper到其他机器上4 配置zoo.cfg文件（1）重命名zoo_sample.cfg为zoo.cfg（2）打开zoo.cfg文件修改数据存储路径配置，添加配置1234#######################cluster##########################server.2&#x3D;hadoop102:2888:3888server.3&#x3D;hadoop103:2888:3888server.4&#x3D;hadoop104:2888:3888（3）同步zoo.cfg配置文件（4）配置参数解读： server.A=B:C:DA是一个数字，代表这个是几号服务器B是这个服务器的ip地址C是这个服务器与集群中的Leader服务器交换信息的端口D是集群服务器挂了，此端口执行服务器相互通信端口5 集群操作（1）分别启动Zookeeper：bin/zkServer.sh start（2）查看状态：bin/zkServer.sh status客户端命令行操作命令基本语法功能描述help显示所有操作命令ls path [watch]使用 ls 命令来查看当前znode中所包含的内容ls2 path [watch]查看当前节点数据并能看到更新次数等数据create普通创建 -s 含有序列 -e 临时（重启或者超时消失）get path [watch]获得节点的值set设置节点的具体值stat查看节点状态delete删除节点rmr递归删除节点启动客户端：bin/zkCli.sh创短暂节点：create -e /sanguo “zhouyu”创建带序号的节点：create -s /sanguo “kang”监听节点数据：get /sang watch监听节点数目：ls /sanguo watch删除节点：deletermr /san查看节点状态：stat /sanAPI应用pom文件123456789101112131415161718&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.zookeeper/zookeeper --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.10&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;log4j.properties文件需要在项目的src/main/resources目录下，新建一个文件，命名为“log4j.properties”，在文件中填入12345678log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n log4j.appender.logfile=org.apache.log4j.FileAppender log4j.appender.logfile.File=target/spring.log log4j.appender.logfile.layout=org.apache.log4j.PatternLayout log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n创建Zookeeper客户端12345678910111213141516171819202122232425private static String connectString = \"hadoop102:2181,hadoop103:2181,hadoop104:2181\"; private static int sessionTimeout = 2000; private ZooKeeper zkClient = null; @Before public void init() throws Exception &#123; zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; // 收到事件通知后的回调函数（用户的业务逻辑） System.out.println(event.getType() + \"--\" + event.getPath()); // 再次启动监听 try &#123; zkClient.getChildren(\"/\", true); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125;创建子节点1234567// 创建子节点@Testpublic void create() throws Exception &#123; // 参数1：要创建的节点的路径； 参数2：节点数据 ； 参数3：节点权限 ；参数4：节点的类型 String nodeCreated = zkClient.create(\"/atguigu\", \"jinlian\".getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);&#125;获取子节点并监听节点变化12345678910111213// 获取子节点@Testpublic void getChildren() throws Exception &#123; List&lt;String&gt; children = zkClient.getChildren(\"/\", true); for (String child : children) &#123; System.out.println(child); &#125; // 延时阻塞 Thread.sleep(Long.MAX_VALUE);&#125;判断Znode是否存在12345678// 判断znode是否存在@Testpublic void exist() throws Exception &#123; Stat stat = zkClient.exists(\"/eclipse\", false); System.out.println(stat == null ? \"not exist\" : \"exist\");&#125;监听服务器节点动态上下线案例需求：某分布式系统中，主节点可以有多台，可以动态上下线，任意一台客户端都能实时感知到主节点服务器的上下线。先在集群上创建/servers节点12[zk: localhost:2181(CONNECTED) 10] create /servers \"servers\"Created /servers服务器端注册代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package com.atguigu.zookeeper1;import java.io.IOException;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooDefs.Ids;import org.apache.zookeeper.ZooKeeper;public class DistributeServer &#123; public static void main(String[] args) throws Exception &#123; DistributeServer server = new DistributeServer(); // 1 连接zookeepeer集群 server.getConnect(); // 2 注册节点 server.regist(args[0]); // 3 业务逻辑处理 server.business(); &#125; private void business() throws InterruptedException &#123; Thread.sleep(Long.MAX_VALUE); &#125; private void regist(String hostname) throws KeeperException, InterruptedException &#123; String path = zkClient.create(\"/servers/server\", hostname.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); System.out.println(hostname +\"is online!!\"); &#125; private String connectString=\"hadoop102:2181,hadoop103:2181,hadoop104:2181\"; private int sessionTimeout=2000; private ZooKeeper zkClient; private void getConnect() throws IOException &#123; zkClient=new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; public void process(WatchedEvent arg0) &#123; &#125; &#125;); &#125;&#125;客户端代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273package com.atguigu.zookeeper1;import java.io.IOException;import java.util.ArrayList;import java.util.List;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;public class DistributeClient &#123; public static void main(String[] args) throws Exception &#123; // 1 获取zookeeper集群连接 DistributeClient client = new DistributeClient(); client.getConnect(); // 2 注册监听 client.getChlidren(); // 3 业务逻辑处理 client.business(); &#125; private void business() throws InterruptedException &#123; Thread.sleep(Long.MAX_VALUE); &#125; private void getChlidren() throws KeeperException, InterruptedException &#123; List&lt;String&gt; children = zkClient.getChildren(\"/servers\", true); ArrayList&lt;String&gt; hosts = new ArrayList&lt;String&gt;(); for (String child : children) &#123; byte[] data = zkClient.getData(\"/servers/\"+child, false, null); hosts.add(new String(data)); &#125; System.out.println(hosts); &#125; private String connectString=\"hadoop102:2181,hadoop103:2181,hadoop104:2181\"; private int sessionTimeout=2000; private ZooKeeper zkClient; private void getConnect() &#123; try &#123; zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; public void process(WatchedEvent event) &#123; try &#123; getChlidren(); &#125; catch (KeeperException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125;); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;&#125;","categories":[{"name":"Zookeeper","slug":"Zookeeper","permalink":"https://blog.kiedeng.site/categories/Zookeeper/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"数据压缩","slug":"数据压缩","date":"2020-03-01T04:39:39.000Z","updated":"2020-03-01T09:04:22.605Z","comments":true,"path":"2020/03/01/数据压缩/","link":"","permalink":"https://blog.kiedeng.site/2020/03/01/%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9/","excerpt":"","text":"1 概念：​ 压缩技术能够有效减少底层存储系统读写字节数。在运行MR程序是，I/O操作，网络数据传输，Shuffle和Merge要花大量的时间​ 压缩是提高Hadoop运行效率的一种优化策略​ 注：运行密集型的job，少用压缩；IO密集型的job，多用压缩2 MR支持的压缩编码压缩格式hadoop自带？算法文件扩展名是否可切分换成压缩格式后，原来的程序是否需要修改DEFLATE是，直接使用DEFLATE.deflate否和文本处理一样，不需要修改Gzip是，直接使用DEFLATE.gz否和文本处理一样，不需要修改bzip2是，直接使用bzip2.bz2是和文本处理一样，不需要修改LZO否，需要安装LZO.lzo是需要建索引，还需要指定输入格式Snappy否，需要安装Snappy.snappy否和文本处理一样，不需要修改Hadoop引入了编码/解码器，如：压缩格式对应的编码/解码器DEFLATEorg.apache.hadoop.io.compress.DefaultCodecgziporg.apache.hadoop.io.compress.GzipCodecbzip2org.apache.hadoop.io.compress.BZip2CodecLZOcom.hadoop.compression.lzo.LzopCodecSnappyorg.apache.hadoop.io.compress.SnappyCodec3 压缩方式选择Gzip压缩优点：压缩率比较高，压缩速度比较快，本身支持，在应用中处理Gzip格式的文件和直接处理文本一样，大部分Linux系统自带；缺点：不支持Split应用场景：一个块大小内的数据，比如一天或者一个小时的日志信息Bzip2压缩优点：支持Split，具有很高的压缩率，比Gzip压缩率要高，自带，使用方便缺点：压缩/解压速度比较慢应用场景：适合对速度要求不高，但需要很高压缩率的时候；Lzo压缩优点：压缩/解压速度比较快，合理的压缩率；支持Split,是Hadoop中最流行的压缩格式；缺点：压缩率比Gzip低一些；需要安装，为了支持Split需要建索引，还需要指令InputFormat为Lzo格式应用场景：一个很大的文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，Lzo优点越明显Snappy压缩优点：高速压缩速度和合理的压缩率缺点：不支持Split应用场景：Map输出数据比较大的时候，作为Map到Reduce的中间数据的压缩格式；或者MapReduce的输出和另外一个MapReduce的输入4 压缩位置选择1 输入端采用压缩2 Mapper输出采用压缩3 Reduce输出采用压缩5 压缩参数配置参数默认值阶段建议io.compression.codecs （在core-site.xml中配置）org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec输入压缩Hadoop使用文件扩展名判断是否支持某种编解码器mapreduce.map.output.compress（在mapred-site.xml中配置）falsemapper输出这个参数设为true启用压缩mapreduce.map.output.compress.codec（在mapred-site.xml中配置）org.apache.hadoop.io.compress.DefaultCodecmapper输出企业多使用LZO或Snappy编解码器在此阶段压缩数据mapreduce.output.fileoutputformat.compress（在mapred-site.xml中配置）falsereducer输出这个参数设为true启用压缩mapreduce.output.fileoutputformat.compress.codec（在mapred-site.xml中配置）org.apache.hadoop.io.compress. DefaultCodecreducer输出使用标准工具或者编解码器，如gzip和bzip2mapreduce.output.fileoutputformat.compress.type（在mapred-site.xml中配置）RECORDreducer输出SequenceFile输出使用的压缩类型：NONE和BLOCK6 压缩案例压缩解压案例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package com.atguigu.mapreduce.compress;import java.io.File;import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.FileOutputStream;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.io.compress.CompressionCodec;import org.apache.hadoop.io.compress.CompressionCodecFactory;import org.apache.hadoop.io.compress.CompressionInputStream;import org.apache.hadoop.io.compress.CompressionOutputStream;import org.apache.hadoop.util.ReflectionUtils;public class TestCompress &#123; public static void main(String[] args) throws Exception &#123; compress(\"e:/hello.txt\",\"org.apache.hadoop.io.compress.BZip2Codec\");// decompress(\"e:/hello.txt.bz2\"); &#125; // 1、压缩 private static void compress(String filename, String method) throws Exception &#123; // （1）获取输入流 FileInputStream fis = new FileInputStream(new File(filename)); Class codecClass = Class.forName(method); CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration()); // （2）获取输出流 FileOutputStream fos = new FileOutputStream(new File(filename + codec.getDefaultExtension())); CompressionOutputStream cos = codec.createOutputStream(fos); // （3）流的对拷 IOUtils.copyBytes(fis, cos, 1024*1024*5, false); // （4）关闭资源 cos.close(); fos.close();fis.close(); &#125; // 2、解压缩 private static void decompress(String filename) throws FileNotFoundException, IOException &#123; // （0）校验是否能解压缩 CompressionCodecFactory factory = new CompressionCodecFactory(new Configuration()); CompressionCodec codec = factory.getCodec(new Path(filename)); if (codec == null) &#123; System.out.println(\"cannot find codec for file \" + filename); return; &#125; // （1）获取输入流 CompressionInputStream cis = codec.createInputStream(new FileInputStream(new File(filename))); // （2）获取输出流 FileOutputStream fos = new FileOutputStream(new File(filename + \".decoded\")); // （3）流的对拷 IOUtils.copyBytes(cis, fos, 1024*1024*5, false); // （4）关闭资源 cis.close(); fos.close(); &#125;&#125;Map输出端采用压缩12345// 在Driver添加// 开启map端输出压缩configuration.setBoolean(\"mapreduce.map.output.compress\", true);// 设置map端输出压缩方式configuration.setClass(\"mapreduce.map.output.compress.codec\", BZip2Codec.class, CompressionCodec.class);Reduce输出采用压缩1234// 设置reduce端输出压缩开启FileOutputFormat.setCompressOutput(job, true);// 设置压缩的方式FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class);","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://blog.kiedeng.site/categories/Hadoop/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"MapReduce框架原理","slug":"MapReduce框架原理","date":"2020-02-29T09:08:56.000Z","updated":"2020-02-29T13:57:55.988Z","comments":true,"path":"2020/02/29/MapReduce框架原理/","link":"","permalink":"https://blog.kiedeng.site/2020/02/29/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86/","excerpt":"","text":"总结MapReduce框架原理1 InputFotmat数据输入待写2 MapReduce工作流程待写3 Shuffle机制3.1 Shuffle机制介绍Map方法之后，Reduce方法之前的数据称之为Shuffle3.2 主要功能Partition分区，WritableComparable排序，Combiner合并，GroupingComparator分组（辅助排序）4 MapTask工作机制​ （1）Read阶段：MapTask通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value。​ （2）Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value。​ （3）Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner），并写入一个环形内存缓冲区中。​ （4）Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。​ 溢写阶段详情：​ 步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号Partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。​ 步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。​ 步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中。​ （5）Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。​ 当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。​ 在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor（默认10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。​ 让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。5 RedeceTask工作机制​ （1）Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。​ （2）Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。​ （3）Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。​ （4）Reduce阶段：reduce()函数将计算结果写到HDFS上。6 OutputFormat数据输出​ OutputFormat是MapReduce输出的基类，所有实现MapReduce输出都实现了OutputFormat接口。下面我们介绍几种常见的OutputFormat实现类。​ 1.文本输出TextOutputFormat默认的输出格式是TextOutputFormat，它把每条记录写为文本行。它的键和值可以是任意类型，因为TextOutputFormat调用toString0方法把它们转换为字符串。​ 2.SequenceFileOutputFormat将SequenceFileOutputFormat输出作为后续MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑，很容易被压缩。​ 3.自定义OutputFormat根据用户需求，自定义实现输出。6.1 自定义OutputFomat步骤​ （1）自定义一个类继承FileOutputFormat。1234567891011121314151617package com.atguigu.mapreduce.outputformat;import java.io.IOException;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.RecordWriter;import org.apache.hadoop.mapreduce.TaskAttemptContext;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class FilterOutputFormat extends FileOutputFormat&lt;Text, NullWritable&gt;&#123; @Override public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter(TaskAttemptContext job) throws IOException, InterruptedException &#123; &#x2F;&#x2F; 创建一个RecordWriter return new FilterRecordWriter(job); &#125;&#125;​ （2）改写RecordWriter，具体改写输出数据的方法write。​ RecordWriter格式：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package com.atguigu.mapreduce.outputformat;import java.io.IOException;import org.apache.hadoop.fs.FSDataOutputStream;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.RecordWriter;import org.apache.hadoop.mapreduce.TaskAttemptContext;public class FilterRecordWriter extends RecordWriter&lt;Text, NullWritable&gt; &#123; FSDataOutputStream atguiguOut &#x3D; null; FSDataOutputStream otherOut &#x3D; null; public FilterRecordWriter(TaskAttemptContext job) &#123; &#x2F;&#x2F; 1 获取文件系统 FileSystem fs; try &#123; fs &#x3D; FileSystem.get(job.getConfiguration()); &#x2F;&#x2F; 2 创建输出文件路径 Path atguiguPath &#x3D; new Path(&quot;e:&#x2F;atguigu.log&quot;); Path otherPath &#x3D; new Path(&quot;e:&#x2F;other.log&quot;); &#x2F;&#x2F; 3 创建输出流 atguiguOut &#x3D; fs.create(atguiguPath); otherOut &#x3D; fs.create(otherPath); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void write(Text key, NullWritable value) throws IOException, InterruptedException &#123; &#x2F;&#x2F; 判断是否包含“atguigu”输出到不同文件 if (key.toString().contains(&quot;atguigu&quot;)) &#123; atguiguOut.write(key.toString().getBytes()); &#125; else &#123; otherOut.write(key.toString().getBytes()); &#125; &#125; @Override public void close(TaskAttemptContext context) throws IOException, InterruptedException &#123; &#x2F;&#x2F; 关闭资源IOUtils.closeStream(atguiguOut); IOUtils.closeStream(otherOut); &#125;&#125;注意：记得将自定义输出格式设置到job中12// 要将自定义的输出格式组件设置到job中job.setOutputFormatClass(FilterOutputFormat.class);7 Join多种应用7.1 工作原理​ Map端的主要工作：为来自不同表或文件的key/value对，打标签以区别不同来源的记录。然后用连接字段作为key，其余部分和新加的标志作为value，最后进行输出。​ Reduce端的主要工作：在Reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录（在Map阶段已经打标志分开，最后进行合并就ok了。7.2 Reduce join​ 在Mapper阶段使得连接属性为key，其余属性为value（自定义bean对象，记得序列化），再用一个标记属性标记来自于哪个文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package com.atguigu.mapreduce.table;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.FileSplit;public class TableMapper extends Mapper&lt;LongWritable, Text, Text, TableBean&gt;&#123;String name; TableBean bean = new TableBean(); Text k = new Text(); @Override protected void setup(Context context) throws IOException, InterruptedException &#123; // 1 获取输入文件切片 FileSplit split = (FileSplit) context.getInputSplit(); // 2 获取输入文件名称 name = split.getPath().getName(); &#125; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取输入数据 String line = value.toString(); // 2 不同文件分别处理 if (name.startsWith(\"order\")) &#123;// 订单表处理 // 2.1 切割 String[] fields = line.split(\"\\t\"); // 2.2 封装bean对象 bean.setOrder_id(fields[0]); bean.setP_id(fields[1]); bean.setAmount(Integer.parseInt(fields[2])); bean.setPname(\"\"); bean.setFlag(\"order\"); k.set(fields[1]); &#125;else &#123;// 产品表处理 // 2.3 切割 String[] fields = line.split(\"\\t\"); // 2.4 封装bean对象 bean.setP_id(fields[0]); bean.setPname(fields[1]); bean.setFlag(\"pd\"); bean.setAmount(0); bean.setOrder_id(\"\"); k.set(fields[0]); &#125; // 3 写出 context.write(k, bean); &#125;&#125;​ 在Reduce阶段，输出连接成功的bean对象123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.atguigu.mapreduce.table;import java.io.IOException;import java.util.ArrayList;import org.apache.commons.beanutils.BeanUtils;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class TableReducer extends Reducer&lt;Text, TableBean, TableBean, NullWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;TableBean&gt; values, Context context) throws IOException, InterruptedException &#123; // 1准备存储订单的集合 ArrayList&lt;TableBean&gt; orderBeans = new ArrayList&lt;&gt;(); // 2 准备bean对象 TableBean pdBean = new TableBean(); for (TableBean bean : values) &#123; if (\"order\".equals(bean.getFlag())) &#123;// 订单表 // 拷贝传递过来的每条订单数据到集合中 TableBean orderBean = new TableBean(); try &#123; BeanUtils.copyProperties(orderBean, bean); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; orderBeans.add(orderBean); &#125; else &#123;// 产品表 try &#123; // 拷贝传递过来的产品表到内存中 BeanUtils.copyProperties(pdBean, bean); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; // 3 表的拼接 for(TableBean bean:orderBeans)&#123; bean.setPname (pdBean.getPname()); // 4 数据写出去 context.write(bean, NullWritable.get()); &#125; &#125;&#125;缺点：这种方式中，合并的操作是在Reduce阶段完成，Reduce端的处理压力太大，Map节点的运算负载则很低，资源利用率不高，且在Reduce阶段极易产生数据倾斜。map join使用场景：Map Join适用于一张表十分小、一张表很大的场景。具体办法：采用DistributedCache​ （1）在Mapper的setup阶段，将文件读取到缓存集合中。​ （2）在驱动函数中加载缓存。​ // 缓存普通文件到Task运行节点。​ job.addCacheFile(new URI(“file://e:/cache/pd.txt”));注：简单说就是把一个小表用map表示，用大表的连接属性直接映射出小表的属性对于驱动模块Driver来说1234&#x2F;&#x2F; 6 加载缓存数据job.addCacheFile(new URI(&quot;file:&#x2F;&#x2F;&#x2F;e:&#x2F;input&#x2F;inputcache&#x2F;pd.txt&quot;)); &#x2F;&#x2F; 7 Map端Join的逻辑不需要Reduce阶段，设置reduceTask数量为0job.setNumReduceTasks(0);对于mapper来说，先在map前读取缓存数据12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package test;import java.io.BufferedReader;import java.io.FileInputStream;import java.io.IOException;import java.io.InputStreamReader;import java.util.HashMap;import java.util.Map;import org.apache.commons.lang.StringUtils;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class DistributedCacheMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123; Map&lt;String, String&gt; pdMap = new HashMap&lt;&gt;(); @Override protected void setup(Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; // 1 获取缓存的文件 URI[] cacheFiles = context.getCacheFiles(); String path = cacheFiles[0].getPath().toString(); BufferedReader reader = new BufferedReader(new InputStreamReader(new FileInputStream(path), \"UTF-8\")); String line; while(StringUtils.isNotEmpty(line = reader.readLine()))&#123; // 2 切割 String[] fields = line.split(\"\\t\"); // 3 缓存数据到集合 pdMap.put(fields[0], fields[1]); &#125; // 4 关流 reader.close(); &#125; Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 截取 String[] fields = line.split(\"\\t\"); // 3 获取产品id String pId = fields[1]; // 4 获取商品名称 String pdName = pdMap.get(pId); // 5 拼接 k.set(line + \"\\t\"+ pdName); // 6 写出 context.write(k, NullWritable.get()); &#125;&#125;8 计数器应用与数据清洗（ETL）​ Hadoop为每个作业维护若干内置计数器，以描述多项指标。例如，某些计数器记录已处理的字节数和记录数，使用户可监控已处理的输入数据量和已产生的输出数据量。使用方法：1context.getCounter(\"map\",\"失败\").increment(1);9 MapRedece开发总结1. 输入数据接口：InputFormat（1）默认使用的实现类是：TextInputFormat（2）TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回。（3）KeyValueTextlnputFomat每一行均为一条记录，被分隔符分割为key，value。默认分隔符是tab（\\t）。（4）NlinelnputFormat按照指定的行数N来划分切片。（5）CombineTextlnputFormat可以把多个小文件合并成一个切片处理，提高处理效率。（6）用户还可以自定义ImputFormat。2.逻辑处理接口：Mapper用户根据业务需求实现其中三个方法：map）setup）cleanup）3.Partitioner分区（1）有默认实现HashPartitioner，逻辑是根据key的哈希值和numReduces来返回一个分区号；key hashCode（）&amp;Integer.MAXVALUE%numReduces（2）如果业务上有特别的需求，可以自定义分区。4.Comparable排序（1）当我们用自定义的对象作为key来输出时，就必须要实现WritableComparable接口，重写其中的compareTo0方法。（2）部分排序：对最终输出的每一个文件进行内部排序。（3）全排序：对所有数据进行排序，通常只有一个Reduce。（4）二次排序：排序的条件有两个。5.Combiner合并Combiner合并可以提高程序执行效率，减少I0传输。但是使用时必须不能影响原有的业务处理结果。6.Reduce端分组：GroupingComparator在Reduce端对key进行分组。应用于：在接收的key为bean对象时，想让一个或几个字段相同（全部字段比较不相同）的key进入到同一个reduce方法时，可以采用分组排序。7.逻辑处理接口：Reducer用户根据业务需求实现其中三个方法：reduce();setup();cleanup()8.输出数据接口：OutputFormat（1）默认实现类是TextOutputFormat，功能逻辑是：将每一个KV对，向目标文本文件输出一行。（2）将SequenceFileOutputFormat输出作为后续MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑，很容易被压缩。（3）用户还可以自定义OutputFormat。","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://blog.kiedeng.site/categories/Hadoop/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"bean对象没有序列化","slug":"bean对象没有序列化","date":"2020-02-29T08:39:08.000Z","updated":"2020-02-29T08:54:00.899Z","comments":true,"path":"2020/02/29/bean对象没有序列化/","link":"","permalink":"https://blog.kiedeng.site/2020/02/29/bean%E5%AF%B9%E8%B1%A1%E6%B2%A1%E6%9C%89%E5%BA%8F%E5%88%97%E5%8C%96/","excerpt":"","text":"在使用bean对象进行数据传输的过程中，一定要注意序列化，不然将出现map输出收集器的错误12345678910111213142020-02-29 16:36:31,063 WARN [org.apache.hadoop.mapred.MapTask] - Unable to initialize MapOutputCollector org.apache.hadoop.mapred.MapTask$MapOutputBufferjava.lang.NullPointerException at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.init(MapTask.java:1011) at org.apache.hadoop.mapred.MapTask.createSortingCollector(MapTask.java:402) at org.apache.hadoop.mapred.MapTask.access$100(MapTask.java:81) at org.apache.hadoop.mapred.MapTask$NewOutputCollector.&lt;init&gt;(MapTask.java:698) at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:770) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341) at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:748)出错位置：Reduce Join案例","categories":[{"name":"常见错误","slug":"常见错误","permalink":"https://blog.kiedeng.site/categories/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"MapReduce流程图","slug":"MapReduce流程图","date":"2020-02-28T14:22:07.000Z","updated":"2020-02-28T16:11:22.122Z","comments":true,"path":"2020/02/28/MapReduce流程图/","link":"","permalink":"https://blog.kiedeng.site/2020/02/28/MapReduce%E6%B5%81%E7%A8%8B%E5%9B%BE/","excerpt":"","text":"","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://blog.kiedeng.site/categories/Hadoop/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"文档资料","slug":"文档资料","date":"2020-02-28T12:58:31.000Z","updated":"2020-02-28T14:57:22.875Z","comments":true,"path":"2020/02/28/文档资料/","link":"","permalink":"https://blog.kiedeng.site/2020/02/28/%E6%96%87%E6%A1%A3%E8%B5%84%E6%96%99/","excerpt":"","text":"HDFS:https://books.kiedeng.site/hadoop/hdfs.pdfMapReduce:https://books.kiedeng.site/hadoop/mapreduce.pdf","categories":[{"name":"工具","slug":"工具","permalink":"https://blog.kiedeng.site/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"在线预览文件pdfJS","slug":"在线预览文件pdfJS","date":"2020-02-28T12:31:38.000Z","updated":"2020-02-28T12:53:52.987Z","comments":true,"path":"2020/02/28/在线预览文件pdfJS/","link":"","permalink":"https://blog.kiedeng.site/2020/02/28/%E5%9C%A8%E7%BA%BF%E9%A2%84%E8%A7%88%E6%96%87%E4%BB%B6pdfJS/","excerpt":"","text":"1 下载pdf.js插件地址: http://mozilla.github.io/pdf.js/下载的时候选择Stable（稳定版），下载完成以后进行解压，2 部署在tomcat的webapps文件夹下新建一个目录比如books，进解压的文件传入。运行tomcat即可以进行访问http://localhost:8080/books/web/viewer.html3 添加文档访问自己需要访问的pdf文档，就可以先在books下新建一个目录，比如hadoop，将自己的1.pdf文档放入hadoop目录下，就可以通过http://localhost:8080/books/hadoop/1.pdf访问。4 云服务器将localhost换成服务器的ip或者域名即可","categories":[{"name":"Tomcat","slug":"Tomcat","permalink":"https://blog.kiedeng.site/categories/Tomcat/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"GroupingComparator错误","slug":"GroupingComparator错误","date":"2020-02-28T10:43:08.000Z","updated":"2020-02-28T11:00:25.025Z","comments":true,"path":"2020/02/28/GroupingComparator错误/","link":"","permalink":"https://blog.kiedeng.site/2020/02/28/GroupingComparator%E9%94%99%E8%AF%AF/","excerpt":"","text":"在进行重写的时候千万要小心，一不小心就会导致出错，注意参数！错误处：12345@Overridepublic int compare(Object a, Object b) &#123; &#x2F;&#x2F; TODO Auto-generated method stub return super.compare(a, b);&#125;改正：123456789101112131415@Overridepublic int compare(WritableComparable a, WritableComparable b) &#123; &#x2F;&#x2F; 要求只要id相同，就认为是相同的key OrderBean aBean &#x3D; (OrderBean) a; OrderBean bBean &#x3D; (OrderBean) b; int result; if (aBean.getOrder_id() &gt; bBean.getOrder_id()) &#123; result &#x3D; 1; &#125;else if(aBean.getOrder_id() &lt; bBean.getOrder_id())&#123; result &#x3D; -1; &#125;else &#123; result &#x3D; 0; &#125; return result;&#125;出错案例：统计order的top1","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://blog.kiedeng.site/categories/Hadoop/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"typora与typecho的问题","slug":"typora与typecho的问题","date":"2020-02-27T15:59:39.000Z","updated":"2020-02-27T16:38:41.704Z","comments":true,"path":"2020/02/27/typora与typecho的问题/","link":"","permalink":"https://blog.kiedeng.site/2020/02/27/typora%E4%B8%8Etypecho%E7%9A%84%E9%97%AE%E9%A2%98/","excerpt":"","text":"标题问题​ 为了能使typecho的“文章目录”能够正确的显示，在typora设置的标题必须是连续的；比如：#","categories":[{"name":"未分类","slug":"未分类","permalink":"https://blog.kiedeng.site/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"Combiner合并","slug":"Combiner合并","date":"2020-02-27T09:47:40.000Z","updated":"2020-02-27T09:47:40.129Z","comments":true,"path":"2020/02/27/Combiner合并/","link":"","permalink":"https://blog.kiedeng.site/2020/02/27/Combiner%E5%90%88%E5%B9%B6/","excerpt":"","text":"","categories":[{"name":"未分类","slug":"未分类","permalink":"https://blog.kiedeng.site/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"map值类型不匹配","slug":"map值类型不匹配","date":"2020-02-27T07:48:50.000Z","updated":"2020-02-27T08:47:02.567Z","comments":true,"path":"2020/02/27/map值类型不匹配/","link":"","permalink":"https://blog.kiedeng.site/2020/02/27/map%E5%80%BC%E7%B1%BB%E5%9E%8B%E4%B8%8D%E5%8C%B9%E9%85%8D/","excerpt":"","text":"map值类型不匹配123java.lang.Exception: java.io.IOException: Type mismatch in value from map: expected com.atguigu.mr.sort.FlowBean, received org.apache.hadoop.io.Text at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462) at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)错误位置：进行排序案例的时候发生的错误12job.setMapOutputKeyClass(FlowBean.class);job.setMapOutputValueClass(FlowBean.class);//应该为Text.class注：写Driver驱动的时候，要特别注意类型错误问题因为不太懂这方面的错，所以","categories":[{"name":"常见错误","slug":"常见错误","permalink":"https://blog.kiedeng.site/categories/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"WritableComparable排序","slug":"WritableComparable排序","date":"2020-02-27T04:37:15.000Z","updated":"2020-02-28T07:01:59.398Z","comments":true,"path":"2020/02/27/WritableComparable排序/","link":"","permalink":"https://blog.kiedeng.site/2020/02/27/WritableComparable%E6%8E%92%E5%BA%8F/","excerpt":"","text":"排序的分类部分排序MapReduce根据输入记录的键对数据集排序，保证输出的每个文件内部有序全排序最终输出结果为一个文件，且文件内部有序辅助排序：（GroupingComparator分组)在Reduece端对key进行分组。应用于key为bean对象时，想让一个或几个字段相同的key 进入到同一个reduce方法时，可以采用分组排序二次排序在自定义排序过程中，如果compareTo中的判断添加为两个即为二次排序自定义排序WritableComparable1.原理分析​ bean对象作为key传输，需要实现WritableComparable接口重写compareTo方法，就可以实现排序12345678910111213@Overridepublic int compareTo(FlowBean o) &#123; int result; &#x2F;&#x2F; 按照总流量大小，倒序排列 if (sumFlow &gt; bean.getSumFlow()) &#123; result &#x3D; -1; &#125;else if (sumFlow &lt; bean.getSumFlow()) &#123; result &#x3D; 1; &#125;else &#123; result &#x3D; 0; &#125; return result;&#125;WritableComparable排序案例实操（全排序）需求：对总流量进行排序代码实现使用以前的Flowcount就可以实现，代码就可以实现，略排序案例（区内排序）需求：要求每个省份手机号输出的文件中按照总流量内部排序注意点：区间排序中，需要添加的是自定义的Patitioner分区类与在驱动类中添加分区类，设置Reducetask个数12345678910111213141516171819202122232425262728package com.atguigu.mapreduce.sort;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Partitioner;public class ProvincePartitioner extends Partitioner&lt;FlowBean, Text&gt; &#123; @Override public int getPartition(FlowBean key, Text value, int numPartitions) &#123; // 1 获取手机号码前三位 String preNum = value.toString().substring(0, 3); int partition = 4; // 2 根据手机号归属地设置分区 if (\"136\".equals(preNum)) &#123; partition = 0; &#125;else if (\"137\".equals(preNum)) &#123; partition = 1; &#125;else if (\"138\".equals(preNum)) &#123; partition = 2; &#125;else if (\"139\".equals(preNum)) &#123; partition = 3; &#125; return partition; &#125;&#125;1234// 加载自定义分区类job.setPartitionerClass(ProvincePartitioner.class);// 设置Reducetask个数job.setNumReduceTasks(5);GroupingComparator分组（辅助排序）对Reduce阶段的数据根据某一个或几个字段进行分组自定义继承WritableComparator重写compare()方法12345@Overridepublic int compare(WritableComparable a, WritableComparable b) &#123; // 比较的业务逻辑 return result;&#125;创建一个构造将比较对象的类传给父类123protected OrderGroupingComparator() &#123; super(OrderBean.class, true);&#125;GroupingComparator分组案例实操需求：有如下订单数据；表4-2 订单数据订单id商品id成交金额0000001Pdt_01222.8Pdt_0233.80000002Pdt_03522.8Pdt_04122.4Pdt_05722.40000003Pdt_06232.8Pdt_0233.8现在需要求出每一个订单中最贵的商品。期望输出数据*1 222.82 722.43 232.8需求分析利用“订单id和成交金额”作为key，可以将Map阶段读取到的所有订单数据按照id升序排序，如果id相同按照金额降序排序，发送到Reduce在Reduce端利用groupComparator将订单id相同的kv合成为组，然后取第一个即是该订单中最贵商品代码实现定义订单信息OrderBean类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package com.atguigu.mapreduce.order;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.WritableComparable;public class OrderBean implements WritableComparable&lt;OrderBean&gt; &#123; private int order_id; // 订单id号 private double price; // 价格 public OrderBean() &#123; super(); &#125; public OrderBean(int order_id, double price) &#123; super(); this.order_id = order_id; this.price = price; &#125; @Override public void write(DataOutput out) throws IOException &#123; out.writeInt(order_id); out.writeDouble(price); &#125; @Override public void readFields(DataInput in) throws IOException &#123; order_id = in.readInt(); price = in.readDouble(); &#125; @Override public String toString() &#123; return order_id + \"\\t\" + price; &#125; public int getOrder_id() &#123; return order_id; &#125; public void setOrder_id(int order_id) &#123; this.order_id = order_id; &#125; public double getPrice() &#123; return price; &#125; public void setPrice(double price) &#123; this.price = price; &#125; // 二次排序 @Override public int compareTo(OrderBean o) &#123; int result; if (order_id &gt; o.getOrder_id()) &#123; result = 1; &#125; else if (order_id &lt; o.getOrder_id()) &#123; result = -1; &#125; else &#123; // 价格倒序排序 result = price &gt; o.getPrice() ? -1 : 1; &#125; return result; &#125;&#125;编写OrderSortMapper类12345678910111213141516171819202122232425262728package com.atguigu.mapreduce.order;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class OrderMapper extends Mapper&lt;LongWritable, Text, OrderBean, NullWritable&gt; &#123; OrderBean k = new OrderBean(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 截取 String[] fields = line.split(\"\\t\"); // 3 封装对象 k.setOrder_id(Integer.parseInt(fields[0])); k.setPrice(Double.parseDouble(fields[2])); // 4 写出 context.write(k, NullWritable.get()); &#125;&#125;编写OrderSortGroupingComparator类12345678910111213141516171819202122232425262728package com.atguigu.mapreduce.order;import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;public class OrderGroupingComparator extends WritableComparator &#123; protected OrderGroupingComparator() &#123; super(OrderBean.class, true); &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; OrderBean aBean = (OrderBean) a; OrderBean bBean = (OrderBean) b; int result; if (aBean.getOrder_id() &gt; bBean.getOrder_id()) &#123; result = 1; &#125; else if (aBean.getOrder_id() &lt; bBean.getOrder_id()) &#123; result = -1; &#125; else &#123; result = 0; &#125; return result; &#125;&#125;编写OrderSortReducer类12345678910111213package com.atguigu.mapreduce.order;import java.io.IOException;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.Reducer;public class OrderReducer extends Reducer&lt;OrderBean, NullWritable, OrderBean, NullWritable&gt; &#123; @Override protected void reduce(OrderBean key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; context.write(key, NullWritable.get()); &#125;&#125;编写OrderSortDriver类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.atguigu.mapreduce.order;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class OrderDriver &#123; public static void main(String[] args) throws Exception, IOException &#123;// 输入输出路径需要根据自己电脑上实际的输入输出路径设置 args = new String[]&#123;\"e:/input/inputorder\" , \"e:/output1\"&#125;; // 1 获取配置信息 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2 设置jar包加载路径 job.setJarByClass(OrderDriver.class); // 3 加载map/reduce类 job.setMapperClass(OrderMapper.class); job.setReducerClass(OrderReducer.class); // 4 设置map输出数据key和value类型 job.setMapOutputKeyClass(OrderBean.class); job.setMapOutputValueClass(NullWritable.class); // 5 设置最终输出数据的key和value类型 job.setOutputKeyClass(OrderBean.class); job.setOutputValueClass(NullWritable.class); // 6 设置输入数据和输出数据路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 8 设置reduce端的分组 job.setGroupingComparatorClass(OrderGroupingComparator.class); // 7 提交 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125;","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://blog.kiedeng.site/categories/Hadoop/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"Partition分区","slug":"Partition分区","date":"2020-02-26T15:51:54.000Z","updated":"2020-02-26T16:46:55.670Z","comments":true,"path":"2020/02/26/Partition分区/","link":"","permalink":"https://blog.kiedeng.site/2020/02/26/Partition%E5%88%86%E5%8C%BA/","excerpt":"","text":"问题引出：将结果按照条件输出到不同文件（分区）中默认的分区是根据key的hashCode对ReduceTasks个数取模得到的。自定义Partitioner步骤自定义继承Partitioner,重写getPartition()方法在job驱动中，设置自定义Partitioner1job.setPatitionerClass(CustonPartitioner.class)自定义Partition后，要根据自定义Partition的逻辑设置相对应的ReduceTask1job.setNumReduceTasd();分区总结（1）如果Reduce Task的数量&gt;getPartition的结果数，则会多产生几个空的输出文件part-r-000xx；（2）如果1&lt;ReduceTask的数量&lt;getPartition的结果数，则有一部分分区数据无处安放，会Exception；（3）如果ReduceTask的数量=1，则不管MapTask端输出多少个分区文件，最终结果都交给这一个Reduce Task，最终也就只会产生一个结果文件part-r-00000；（4）分区号必须从零开始，逐一累加。Partition案例实操题目：将统计结果按照手机归属地不同省份输出到不同文件中（分区）添加分区类12345678910111213141516171819202122232425262728package com.atguigu.mapreduce.flowsum;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Partitioner;public class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; &#123; @Override public int getPartition(Text key, FlowBean value, int numPartitions) &#123; // 1 获取电话号码的前三位 String preNum = key.toString().substring(0, 3); int partition = 4; // 2 判断是哪个省 if (\"136\".equals(preNum)) &#123; partition = 0; &#125;else if (\"137\".equals(preNum)) &#123; partition = 1; &#125;else if (\"138\".equals(preNum)) &#123; partition = 2; &#125;else if (\"139\".equals(preNum)) &#123; partition = 3; &#125; return partition; &#125;&#125;在驱动函数中增加自定义数据分区设置和ReduceTask设置1234// 8 指定自定义数据分区job.setPartitionerClass(ProvincePartitioner.class);// 9 同时指定相应数量的reduce taskjob.setNumReduceTasks(5);","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://blog.kiedeng.site/categories/Hadoop/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"Text导包错误","slug":"Text导包错误","date":"2020-02-26T05:52:34.000Z","updated":"2020-02-26T05:58:21.071Z","comments":true,"path":"2020/02/26/Text导包错误/","link":"","permalink":"https://blog.kiedeng.site/2020/02/26/Text%E5%AF%BC%E5%8C%85%E9%94%99%E8%AF%AF/","excerpt":"","text":"123456789101112131415java.lang.ClassCastException: class com.sun.jersey.core.impl.provider.entity.XMLJAXBElementProvider$Text at java.lang.Class.asSubclass(Class.java:3404) at org.apache.hadoop.mapred.JobConf.getOutputKeyComparator(JobConf.java:887) at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.init(MapTask.java:1004) at org.apache.hadoop.mapred.MapTask.createSortingCollector(MapTask.java:402) at org.apache.hadoop.mapred.MapTask.access$100(MapTask.java:81) at org.apache.hadoop.mapred.MapTask$NewOutputCollector.&lt;init&gt;(MapTask.java:698) at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:770) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341) at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:748)错误的导入了Text包，1234错误： import com.sun.jersey.core.impl.provider.entity.XMLJAXBElementProvider.Text;正确的包： import org.apache.hadoop.io.Text;","categories":[{"name":"常见错误","slug":"常见错误","permalink":"https://blog.kiedeng.site/categories/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"LeetCode计划表","slug":"LeetCode计划表","date":"2020-02-21T05:26:42.000Z","updated":"2020-02-21T05:33:01.684Z","comments":true,"path":"2020/02/21/LeetCode计划表/","link":"","permalink":"https://blog.kiedeng.site/2020/02/21/LeetCode%E8%AE%A1%E5%88%92%E8%A1%A8/","excerpt":"","text":"题目序号题目名称总结1Two Sum4Median of Two Sorted Arrays11Container With Most Water17Letter Combinations of a Phone Number21Merge Two Sorted Lists23Merge k Sorted Lists37Sudoku Solver39Combination Sum40Combination Sum II51N-Queens53Maximum Subarray56Merge Intervals57Insert Interval62Unique Paths63Unique Paths II64Minimum Path Sum69sqrt(x)70Climbing Stairs72Edit Distance78Subsets79Word Search91Decode Ways98Validate Binary Search Tree100Same Tree102Binary Tree Level Order Traversal110Balanced Binary Tree112Path Sum113Path Sum II115Distinct Subsequences120Triangle121Best Time to Buy and Sell Stock124Binary Tree Maximum Path Sum126Word Ladder II127Word Ladder128Longest Consecutive Sequence139Word Break (revisit)140Word Break II141Linked List Cycle145Binary Tree Postorder Traversal146LRU Cache148Sort List149Max Points on a Line153Find Minimum in Rotated Sorted Array154Find Minimum in Rotated Sorted Array II169Majority Element174Dungeon Game198House Robber200Number of Islands204Count Primes207Course Schedule208Implement Trie (Prefix Tree)210Course Schedule II216Combination Sum III218The Skyline Problem221Maximal Square239Sliding Window Maximum241Different Ways to Add Parentheses263Ugly Number264Ugly Number II268Missing Number282Expression Add Operators289Game of Life295Find Median from Data Stream297Serialize and Deserialize Binary Tree300Longest Increasing Subsequence301Remove Invalid Parentheses303Range Sum Query - Immutable304Range Sum Query 2D - Immutable309Best Time to Buy and Sell Stock with Cooldown312Burst Balloons315Count of Smaller Numbers After Self321Create Maximum Number322Coin Change329Longest Increasing Path in a Matrix332Reconstruct Itinerary347Top K Frequent Elements377Combination Sum IV380Insert Delete GetRandom O(1)381Insert Delete GetRandom O(1) - Duplicates allowed391Perfect Rectangle399Evaluate Division404Sum of Left Leaves409Longest Palindrome410Split Array Largest Sum416Partition Equal Subset Sum417Pacific Atlantic Water Flow432All O`one Data Structure438Find All Anagrams in a String449Serialize and Deserialize BST450Delete Node in a BST451Sort Characters By Frequency452Minimum Number of Arrows to Burst Balloons455Assign Cookies460LFU Cache461Hamming Distance463Island Perimeter464Can I Win470Implement Rand10() Using Rand7()476Number Complement477Total Hamming Distance480Sliding Window Median486Predict the Winner488Zuma Game494Target Sum2494Target Sum504Base 7516Longest Palindromic Subsequence525Contiguous Array530Minimum Absolute Difference in BST540Single Element in a Sorted Array543Diameter of Binary Tree546Remove Boxes547Friend Circles551Student Attendance Record I560Subarray Sum Equals K561Array Partition I566Reshape the Matrix567Permutation in String576Out of Boundary Paths606Construct String from Binary Tree611Valid Triangle Number617Merge Two Binary Trees621Task Scheduler628Maximum Product of Three Numbers633Sum of Square Numbers636Exclusive Time of Functions637Average of Levels in Binary Tree639Decode Ways II652Find Duplicate Subtrees654Maximum Binary Tree655Print Binary Tree657Judge Route Circle664Strange Printer668Kth Smallest Number in Multiplication Table669Trim a Binary Search Tree671Second Minimum Node In a Binary Tree673Number of Longest Increasing Subsequence674Longest Continuous Increasing Subsequence675Cut Off Trees for Golf Event676Implement Magic Dictionary677Map Sum Pairs678Valid Parenthesis String680Valid Palindrome II681Next Closest Time682Baseball Game683K Empty Slots684Redundant Connection685Redundant Connection II687Longest Univalue Path688Knight Probability in Chessboard690Employee Importance692Top K Frequent Words699Falling Squares707Design Linked List712Minimum ASCII Delete Sum for Two Strings715Range Module719Find K-th Smallest Pair Distance720Longest Word in Dictionary724Find Pivot Index725Split Linked List in Parts726Number of Atoms728Self Dividing Numbers729My Calendar I730Count Different Palindromic Subsequences731My Calendar II732My Calendar III733Flood Fill734Sentence Similarity735Asteroid Collision736Parse Lisp Expression737Sentence Similarity II740Delete and Earn741Cherry Pickup742Closest Leaf in a Binary Tree743Network Delay Time744Find Smallest Letter Greater Than Target745Prefix and Suffix Search746Min Cost Climbing Stairs748Shortest Completing Word749Contain Virus752Open the Lock753Cracking the Safe754Reach a Number755Pour Water758Bold Words in String759Employee Free Time762Prime Number of Set Bits in Binary Representation763Partition Labels769Max Chunks To Make Sorted773Sliding Puzzle775Global and Local Inversions778Swim in Rising Water784Letter Case Permutation786K-th Smallest Prime Fraction787Cheapest Flights Within K Stops790Domino and Tromino Tiling792Number of Matching Subsequences799Champagne Tower801Minimum Swaps To Make Sequences Increasing802Find Eventual Safe States803Bricks Falling When Hit813Largest Sum of Averages815Bus Routes817Linked List Components818Race Car2818Race Car823Binary Trees With Factors826Most Profit Assigning Work827Making A Large Island841Keys and Rooms847Shortest Path Visiting All Nodes848Shifting Letters856Score of Parentheses863All Nodes Distance K in Binary Tree864Shortest Path to Get All Keys865Smallest Subtree with all the Deepest Nodes871Minimum Number of Refueling Stops873Length of Longest Fibonacci Subsequence877Stone Game879Profitable Schemes882Reachable Nodes In Subdivided Graph886Possible Bipartition889Construct Binary Tree from Preorder and Postorder Traversal891Sum of Subsequence Widths894All Possible Full Binary Trees895Maximum Frequency Stack898Bitwise ORs of Subarrays901Online Stock Span902Numbers At Most N Given Digit Set9233Sum With Multiplicity926Flip String to Monotone Increasing934Shortest Bridge935Knight Dialer936Stamping The Sequence943Find the Shortest Superstring952Largest Component Size by Common Factor956Tallest Billboard959Regions Cut By Slashes964Least Operators to Express Number967Numbers With Same Consecutive Differences972Equal Rational Numbers973K Closest Points to Origin975Odd Even Jump979Distribute Coins in Binary Tree980Unique Paths III1000Minimum Cost to Merge Stones1017Convert to Base -21019Next Greater Node In Linked List1024Video Stitching1043Partition Array for Maximum Sum1092Shortest Common Supersequenc1105Filling Bookcase Shelves1106Parsing A Boolean Expression1124Longest Well-Performing Interval1125Smallest Sufficient Team1129Shortest Path with Alternating Colors的","categories":[{"name":"算法","slug":"算法","permalink":"https://blog.kiedeng.site/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://blog.kiedeng.site/tags/LeetCode/"}]},{"title":"MapReduce工作流程","slug":"MapReduce工作流程","date":"2020-02-20T13:39:47.000Z","updated":"2020-02-20T13:39:47.432Z","comments":true,"path":"2020/02/20/MapReduce工作流程/","link":"","permalink":"https://blog.kiedeng.site/2020/02/20/MapReduce%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B/","excerpt":"","text":"","categories":[{"name":"未分类","slug":"未分类","permalink":"https://blog.kiedeng.site/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"Shuffle机制","slug":"Shuffle机制","date":"2020-02-20T13:38:49.000Z","updated":"2020-02-20T13:38:49.220Z","comments":true,"path":"2020/02/20/Shuffle机制/","link":"","permalink":"https://blog.kiedeng.site/2020/02/20/Shuffle%E6%9C%BA%E5%88%B6/","excerpt":"","text":"","categories":[{"name":"未分类","slug":"未分类","permalink":"https://blog.kiedeng.site/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"InputFormat数据输入","slug":"InputFormat数据输入","date":"2020-02-20T13:38:16.000Z","updated":"2020-02-28T02:10:46.494Z","comments":true,"path":"2020/02/20/InputFormat数据输入/","link":"","permalink":"https://blog.kiedeng.site/2020/02/20/InputFormat%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5/","excerpt":"","text":"切片与MapTask并行度决定机制数据块：Block是HDFS物理上把数据分成一块一块。数据切片：数据切片只是逻辑上对输入进行分片，并不会再磁盘上将其切分成片进行存储。Job提交流程源码和切片源码详解Job提交流程源码详解12345678910111213141516171819202122232425262728293031323334waitForCompletion()submit();// 1建立连接 connect(); // 1）创建提交Job的代理 new Cluster(getConfiguration()); // （1）判断是本地yarn还是远程 initialize(jobTrackAddr, conf); // 2 提交jobsubmitter.submitJobInternal(Job.this, cluster) // 1）创建给集群提交数据的Stag路径 Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf); // 2）获取jobid ，并创建Job路径 JobID jobId = submitClient.getNewJobID(); // 3）拷贝jar包到集群copyAndConfigureFiles(job, submitJobDir); rUploader.uploadFiles(job, jobSubmitDir);// 4）计算切片，生成切片规划文件writeSplits(job, submitJobDir); maps = writeNewSplits(job, jobSubmitDir); input.getSplits(job);// 5）向Stag路径写XML配置文件writeConf(conf, submitJobFile); conf.writeXml(out);// 6）提交Job,返回提交状态status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());FileInputFormat切片机制1 切片机制简单的按照文件的内容长度进行切片切片大小，默认等于Block大小切片时不考虑数据集整体，而是逐个针对每一个文件单独切片注：每次切片，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片2 源码中计算机切片大小的公式Math. max(minSize, Math.min(maxSize, blockSize));maprecduce.input.fileinputformat.split.minsize=1默认值为1mapreduce.input.fileinputformat.split maxsize=LongMAXValue 默认值Long.MAXValue,因此，默认情况下，切片大小=blocksize3 获取切片信息API1234//获取切片的文件名称String name=inputSplit.getPath（）.getName（）：//根据文件类型获取切片信息FileSplit inputSplit =（FileSplit）context.get InputSplit（）；CombineTextInputFormat切片机制​ 默认的TextInputFormat切片机制是对任务按文件规划切片，会产生大量小文件，产生大量的MapTask，处理效率极其低下，故CombineTextInputFormat来处理大量小文件的情况。1 虚拟存储切片最大值设置1CombineTextInputFormat. setMaxInputSplitSize(job,4194304)://4m-2 切片机制生成切片过程包括：虚拟存储过程和切片过程二部分虚拟存储过程将目录下的所有文件与setMaxInputSize比较，小于则切分为一块，大于且小于两倍，则平分这一块，大于两倍，则切出一块setMaxInputSize的块切片过程判断虚拟存储文件大小是否大于setMaxInputSplitSize值，大于或等于则单独形成一个切片，否则和下一个切片进行合并，形成一个切片CombineTextInputFormat案例实操需求：将输入的大量小文件合并成一个切片统一处理（以WordCount为基础），准备四个文件。只需要在Driver中添加输入格式即可：12345// 如果不设置InputFormat，它默认用的是TextInputFormat.classjob.setInputFormatClass(CombineTextInputFormat.class);//虚拟存储切片最大值设置4mCombineTextInputFormat.setMaxInputSplitSize(job, 4194304);FileInputFormat实现类​ FileInputFormat 常见的接口实现类包括：TextinputFormat、KeyValue TextInputFormat、NLinelnputFormat、CombineTextinputFormat，自定义InputFormat等。1 TextInputFormatFileInputFile默认的实现类，按行读，键为字节偏移量，LongWritable类型2 KeyValueTextInputFormat通过分隔符，分为key，value，课通过设置1234// 设置切割符conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, \" \");// 设置输入格式job.setInputFormatClass(KeyValueTextInputFormat.class);3 NlineInputFormat按照行数N来划分，即切片数=输入文件总行数/N1234&#x2F;&#x2F; 7设置每个切片InputSplit中划分三条记录NLineInputFormat.setNumLinesPerSplit(job, 3); &#x2F;&#x2F; 8使用NLineInputFormat处理记录数 job.setInputFormatClass(NLineInputFormat.class);4 自定义InputFormat需要自定义一个类继承FileInputFormat改写RecordReader，实现封装为KV输出是使用SequenceFileOutFormat输出合并文件","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://blog.kiedeng.site/categories/Hadoop/"}],"tags":[{"name":"MapReduce","slug":"MapReduce","permalink":"https://blog.kiedeng.site/tags/MapReduce/"}]},{"title":"序列化案例","slug":"序列化案例","date":"2020-02-16T07:08:00.000Z","updated":"2020-02-20T11:27:43.180Z","comments":true,"path":"2020/02/16/序列化案例/","link":"","permalink":"https://blog.kiedeng.site/2020/02/16/%E5%BA%8F%E5%88%97%E5%8C%96%E6%A1%88%E4%BE%8B/","excerpt":"","text":"需求：统计每一个手机号耗费的总上行流量，下行流量，总流量编写MapReduce程序：编写流量统计的Bean对象123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package com.atguigu.mapreduce.flowsum;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.Writable;// 1 实现writable接口public class FlowBean implements Writable&#123; private long upFlow; private long downFlow; private long sumFlow; //2 反序列化时，需要反射调用空参构造函数，所以必须有 public FlowBean() &#123; super(); &#125; public FlowBean(long upFlow, long downFlow) &#123; super(); this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = upFlow + downFlow; &#125; //3 写序列化方法 @Override public void write(DataOutput out) throws IOException &#123; out.writeLong(upFlow); out.writeLong(downFlow); out.writeLong(sumFlow); &#125; //4 反序列化方法 //5 反序列化方法读顺序必须和写序列化方法的写顺序必须一致 @Override public void readFields(DataInput in) throws IOException &#123; this.upFlow = in.readLong(); this.downFlow = in.readLong(); this.sumFlow = in.readLong(); &#125; // 6 编写toString方法，方便后续打印到文本 @Override public String toString() &#123; return upFlow + \"\\t\" + downFlow + \"\\t\" + sumFlow; &#125; public long getUpFlow() &#123; return upFlow; &#125; public void setUpFlow(long upFlow) &#123; this.upFlow = upFlow; &#125; public long getDownFlow() &#123; return downFlow; &#125; public void setDownFlow(long downFlow) &#123; this.downFlow = downFlow; &#125; public long getSumFlow() &#123; return sumFlow; &#125; public void setSumFlow(long sumFlow) &#123; this.sumFlow = sumFlow; &#125;&#125;编写Mapper类1234567891011121314151617181920212223242526272829303132333435package com.atguigu.mapreduce.flowsum;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class FlowCountMapper extends Mapper&lt;LongWritable, Text, Text, FlowBean&gt;&#123; FlowBean v = new FlowBean(); Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 切割字段 String[] fields = line.split(\"\\t\"); // 3 封装对象 // 取出手机号码 String phoneNum = fields[1]; // 取出上行流量和下行流量 long upFlow = Long.parseLong(fields[fields.length - 3]); long downFlow = Long.parseLong(fields[fields.length - 2]); k.set(phoneNum); v.set(downFlow, upFlow); // 4 写出 context.write(k, v); &#125;&#125;编写Reducer类1234567891011121314151617181920212223242526package com.atguigu.mapreduce.flowsum;import java.io.IOException;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class FlowCountReducer extends Reducer&lt;Text, FlowBean, Text, FlowBean&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;FlowBean&gt; values, Context context)throws IOException, InterruptedException &#123; long sum_upFlow = 0; long sum_downFlow = 0; // 1 遍历所用bean，将其中的上行流量，下行流量分别累加 for (FlowBean flowBean : values) &#123; sum_upFlow += flowBean.getUpFlow(); sum_downFlow += flowBean.getDownFlow(); &#125; // 2 封装对象 FlowBean resultBean = new FlowBean(sum_upFlow, sum_downFlow); // 3 写出 context.write(key, resultBean); &#125;&#125;编写Driver驱动类1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.atguigu.mapreduce.flowsum;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class FlowsumDriver &#123; public static void main(String[] args) throws IllegalArgumentException, IOException, ClassNotFoundException, InterruptedException &#123; // 输入输出路径需要根据自己电脑上实际的输入输出路径设置args = new String[] &#123; \"e:/input/inputflow\", \"e:/output1\" &#125;; // 1 获取配置信息，或者job对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 6 指定本程序的jar包所在的本地路径 job.setJarByClass(FlowsumDriver.class); // 2 指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(FlowCountMapper.class); job.setReducerClass(FlowCountReducer.class); // 3 指定mapper输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(FlowBean.class); // 4 指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); // 5 指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125;","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://blog.kiedeng.site/categories/Hadoop/"}],"tags":[{"name":"MapReduce","slug":"MapReduce","permalink":"https://blog.kiedeng.site/tags/MapReduce/"}]},{"title":"序列化概述","slug":"序列化概述","date":"2020-02-16T07:06:59.000Z","updated":"2020-02-20T10:58:32.980Z","comments":true,"path":"2020/02/16/序列化概述/","link":"","permalink":"https://blog.kiedeng.site/2020/02/16/%E5%BA%8F%E5%88%97%E5%8C%96%E6%A6%82%E8%BF%B0/","excerpt":"","text":"序列化​ 序列化就是把内存中的对象，转换为字节序列（或者其他数据传输协议）以便于存储到磁盘（持久化）和网络传输。​ 反序列化就是将收到字节序列或者是磁盘的持久化数据，转换为内存中的对象原因：一般来说，“活的对象”只生产在内存里，关机断电就没有了，并且不能发送，然而序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机。由于java序列化是一个重量级序列化框架（Serializable），序列化的时候，会附带很多额外的信息（各种校验信息，Header，继承体系等），不便于在网络中高效传输。故Hadoop开发了一套序列化机制(Writable)。Hadoop序列化特点：紧凑：高效实用存储空间快速：读写数据的额外开销小可扩展：随着通信协议的升级而升级互操作：支持多语言的交互实现序列化接口（Writable）必须实现Writable接口反序列化时，需要反射调用空参构造函数，所以必须要有空参构造123public FlowBean()&#123; super();&#125;重写序列化方法123456@Overridepublic void write(DataOutput out) throws IOException &#123; out.writeLong(upFlow); out.writeLong(downFlow); out.writeLong(sumFlow);&#125;重写反序列化方法123456@Overridepublic void readFields(DataInput in) throws IOException &#123; upFlow = in.readLong(); downFlow = in.readLong(); sumFlow = in.readLong();&#125;注意反序列化的顺序和序列化的顺序完全一致重写toString（），可用“\\t”分开，方便后续用如果需要将自定义的bean放在key中传输，则还需要实现Comparable接口，因为MapReduce框中的Shuffle过程要求对key必须能排序。12345@Overridepublic int compareTo(FlowBean o) &#123; // 倒序排列，从大到小 return this.sumFlow &gt; o.getSumFlow() ? -1 : 1;&#125;","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://blog.kiedeng.site/categories/Hadoop/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"centos安装MySQL","slug":"centos安装MySQL","date":"2020-02-16T02:14:41.000Z","updated":"2020-02-16T02:15:28.523Z","comments":true,"path":"2020/02/16/centos安装MySQL/","link":"","permalink":"https://blog.kiedeng.site/2020/02/16/centos%E5%AE%89%E8%A3%85MySQL/","excerpt":"","text":"","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://blog.kiedeng.site/categories/MySQL/"}],"tags":[{"name":"部署","slug":"部署","permalink":"https://blog.kiedeng.site/tags/%E9%83%A8%E7%BD%B2/"}]},{"title":"WordCount案例","slug":"WordCount案例","date":"2020-02-15T07:23:09.000Z","updated":"2020-02-15T07:24:46.955Z","comments":true,"path":"2020/02/15/WordCount案例/","link":"","permalink":"https://blog.kiedeng.site/2020/02/15/WordCount%E6%A1%88%E4%BE%8B/","excerpt":"","text":"需求：在给定的文本文件中输出每个单词出现的总次数创建maven工程pom.xml文件中添加一下依赖123456789101112131415161718192021222324252627&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;在项目的资源目录下/src/main/resources目录下，新建一个文件，命名为“log4j.properties”12345678log4j.rootLogger=INFO, stdoutlog4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%nlog4j.appender.logfile=org.apache.log4j.FileAppenderlog4j.appender.logfile.File=target/spring.loglog4j.appender.logfile.layout=org.apache.log4j.PatternLayoutlog4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n编写程序编写mapper1234567891011121314151617181920212223242526272829package com.atguigu.mapreduce;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class WordcountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123; Text k = new Text(); IntWritable v = new IntWritable(1); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 切割 String[] words = line.split(\" \"); // 3 输出 for (String word : words) &#123; k.set(word); context.write(k, v); &#125; &#125;&#125;编写Reduce类12345678910111213141516171819202122232425package com.atguigu.mapreduce.wordcount;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class WordcountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123;int sum;IntWritable v = new IntWritable(); @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException &#123; // 1 累加求和 sum = 0; for (IntWritable count : values) &#123; sum += count.get(); &#125; // 2 输出 v.set(sum); context.write(key,v); &#125;&#125;编写Driver驱动类12345678910111213141516171819202122232425262728293031323334353637383940414243package com.atguigu.mapreduce.wordcount;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class WordcountDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1 获取配置信息以及封装任务 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2 设置jar加载路径 job.setJarByClass(WordcountDriver.class); // 3 设置map和reduce类 job.setMapperClass(WordcountMapper.class); job.setReducerClass(WordcountReducer.class); // 4 设置map输出 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 5 设置最终输出kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // 6 设置输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 提交 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125;测试直接在eclipse/Idea上测试集群上测试maven打jar包，需要添加打包插件依赖12345678910111213141516171819202122232425262728293031323334&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.3.2&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin &lt;/artifactId&gt; &lt;configuration&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;com.atguigu.mr.WordcountDriver&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;如果工程显示红叉，项目右键-&gt;maven-&gt;update project即可maven install，将jar包放到Hadoop集群执行WordCount12hadoop jar wc.jar com.atguigu.wordcount.WordcountDriver /user/atguigu/input /user/atguigu/output","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://blog.kiedeng.site/categories/Hadoop/"},{"name":"Mapreduce","slug":"Hadoop/Mapreduce","permalink":"https://blog.kiedeng.site/categories/Hadoop/Mapreduce/"}],"tags":[{"name":"案例","slug":"案例","permalink":"https://blog.kiedeng.site/tags/%E6%A1%88%E4%BE%8B/"}]},{"title":"HDFS_2.X新特性","slug":"HDFS-2-X新特性","date":"2020-02-13T17:46:00.000Z","updated":"2020-02-14T17:34:24.151Z","comments":true,"path":"2020/02/14/HDFS-2-X新特性/","link":"","permalink":"https://blog.kiedeng.site/2020/02/14/HDFS-2-X%E6%96%B0%E7%89%B9%E6%80%A7/","excerpt":"","text":"集群间的数据拷贝scp实现两个远程主机之间的文件复制​ scp -r hello.txt root@hadoop103:/user/atguigu/hello.txt // 推 push​ scp -r [root@hadoop103:/user/atguigu/hello.txt hello.txt](mailto:root@hadoop103:/user/atguigu/hello.txt hello.txt) // 拉 pull​ scp -r root@hadoop103:/user/atguigu/hello.txt root@hadoop104:/user/atguigu //是通过本地主机中转实现两个远程主机的文件复制；如果在两个远程主机之间ssh没有配置的情况下可以使用该方式。采用distcp命令实现两个Hadoop集群之间的递归数据复制12 bin/hadoop distcphdfs://haoop102:9000/user/atguigu/hello.txt hdfs://hadoop103:9000/user/atguigu/hello.txt小文件存档弊端​ 每个文件按块存储，每个块的数据在NameNode的内存中，因此HDFS存储小文件会非常低效。因为大量小文件会用尽NameNode的大部分内存。解决小文件的办法之一​ HDFS存档文件或Har文件，是一个更高效的文件存档工具。HDFS存档文件对内还是一个一个独立文件，对NameNode而言是一个整体，减少NameNode内存实例需要启动YARN进程归档文件​ 把/user/atguigu/input目录里面的所有文件归档成一个叫input.har的归档文件，并把归档后文件存储到/user/atguigu/output路径下。1bin/hadoop archive -archiveName input.har –p /user/atguigu/input /user/atguigu/output查看归档12hadoop fs -lsr /user/atguigu/output/input.harhadoop fs -lsr har:///user/atguigu/output/input.har解归档文件1hadoop fs -cp har:/// user/atguigu/output/input.har/* /user/atguigu回收站开启回收站功能参数说明默认值fs.trash.interval=0,0表示禁用回收站默认值fs.trash.checkpoint.interval=0,检查回收站的间隔时间。如果为0，则该值和fs.tarsh.interval的参数相等要求fs.trash.checkpoint.interval&lt;=fs.trash.interval启动回收站修改core-site.xml,配置垃圾回收时间为1分钟1234&lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt;查看回收站回收站在集群中的路径：/user/atguigu/.Trash/….修改访问垃圾回收站用户名称进入垃圾回收站用户名称，默认是dr.who，修改为atguigu用户1234&lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;atguigu&lt;/value&gt;&lt;/property&gt;调用moveToTrash（）才进入回收站12Trash trash = New Trash(conf);trash.moveToTrash(path);恢复回收站数据12hadoop fs -mv/user/atguigu/.Trash/Current/user/atguigu/input /user/atguigu/input清空回收站1hadoop fs -expunge快照管理12345678hdfs dfsadmin -allowSnapshot 路径（功能描述：开启指定目录的快照功能）hdfs dfsadmin -disallowSnapshot 路径（功能描述：禁用指定目录的快照功能，默认是禁用）hdfs dfs -createSnapshot 路径（功能描述：对目录创建快照）hdfs dfs -createSnapshot 路径名称（功能描述：指定名称创建快照）hdfs dfs -renameSnapshot 略径旧名称新名称（功能描述：重命名快照）hdfs lsSnapshottableDir（功能描述：列出当前用户所有可快照目录）hdfs snapshotDiff 路径1 路径2（功能描述：比较两个快照目录的不同之处）hdfs dfs -deleteSnapshot&lt;path&gt;&lt;snapshotName&gt;（功能描述：删除快照）开启/禁用指定目录的快照功能12hdfs dfsadmin -allowSnapshot /user/atguigu/inputhdfs dfsadmin -disallowSnapshot /user/atguigu/input对目录创建快照12hdfs dfs -createSnapshot /user/atguigu/inputhdfs dfs -lsr /user/atguigu/input/.snapshot/指定名称创建快照1hdfs dfs -createSnapshot /user/atguigu/input miao170508快照重命名1hdfs dfs -renameSnapshot /user/atguigu/input/ miao170508 atguigu170508列出当前用户所有可快照目录1hdfs lsSnapshottableDir比较两个快照目录的不同之处1hdfs snapshotDiff /user/atguigu/input/ . .snapshot/atguigu170508恢复快照1hdfs dfs -cp /user/atguigu/input/.snapshot/s20170708-134303.027 /user","categories":[{"name":"HDFS","slug":"HDFS","permalink":"https://blog.kiedeng.site/categories/HDFS/"},{"name":"Hadoop","slug":"HDFS/Hadoop","permalink":"https://blog.kiedeng.site/categories/HDFS/Hadoop/"}],"tags":[{"name":"原理","slug":"原理","permalink":"https://blog.kiedeng.site/tags/%E5%8E%9F%E7%90%86/"}]},{"title":"NN与DataNode","slug":"NN与DataNode","date":"2020-02-13T17:21:19.000Z","updated":"2020-02-20T06:52:17.345Z","comments":true,"path":"2020/02/14/NN与DataNode/","link":"","permalink":"https://blog.kiedeng.site/2020/02/14/NN%E4%B8%8EDataNode/","excerpt":"","text":"NN和2NN工作机制FsImage:磁盘中备份元数据的文件Edits：每当元数据有更新或者添加元数据时，修改内存中元数据并追加到Edits，为防止该文件数据过大影响效率，因此需要定期进行FsImage和Edits合并，引入一个节点SecondaryNamenode,专门用于FsImage和Edits的合并第一阶段：NameNode启动第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。客户端对元数据进行增删改的请求。NameNode记录操作日志，更新滚动日志。NameNode在内存中对数据进行增删改。第二阶段：Secondary NameNode工作Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。Secondary NameNode请求执行CheckPoint。NameNode滚动正在写的Edits日志。将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。Secondary NameNode加载编辑日志和镜像文件到内存，并合并。生成新的镜像文件fsimage.chkpoint。拷贝fsimage.chkpoint到NameNode。NameNode将fsimage.chkpoint重新命名成fsimage。* opt/module/hadoop-2.7.2/data/tmp/dfs/name/current *oiv查看Fsimage语法：hdfs oiv -p 文件类型 -i镜像文件 -o 转换后文件输出路径1hdfs oiv -p XML -i fsimage_0000000000000000025 -o /opt/module/hadoop-2.7.2/fsimage.xmloev查看Edits文件语法：hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径1hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-2.7.2/edits.xmlCheckPoint时间设置通常情况下，SecondaryNameNade每隔一个小时执行一次，在【hdfs-default.xml】中设置1234&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;3600&lt;/value&gt;&lt;/property&gt;一分钟检查一次操作次数，3当操作次数达到1百万时，SecondaryNameNode执行一次。1234567891011&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.txns&lt;/name&gt; &lt;value&gt;1000000&lt;/value&gt;&lt;description&gt;操作动作次数&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.check.period&lt;/name&gt; &lt;value&gt;60&lt;/value&gt;&lt;description&gt; 1分钟检查一次操作次数&lt;/description&gt;&lt;/property &gt;NameNode故障处理方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录kill -9 NameNode过程删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name）拷贝SecondaryNameNode中数据到原NameNode存储数据目录1scp -r atguigu@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* ./name/重新启动NameNode方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中。修改hdfs-site.xml中的123456789&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;120&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp/dfs/name&lt;/value&gt;&lt;/property&gt;kill -9 NameNode进程删除NameNode存储的数据将2NN存储数据的目录拷贝到NN存储数据的评级目录，并删除in_use.lock文件导入检查点数据sbin/hadoop-daemon.sh start namenode启动NameNodesbin/hadoop-daemon.sh start namenode安全模式（1）bin/hdfs dfsadmin -safemode get （功能描述：查看安全模式状态）（2）bin/hdfs dfsadmin -safemode enter （功能描述：进入安全模式状态）（3）bin/hdfs dfsadmin -safemode leave （功能描述：离开安全模式状态）（4）bin/hdfs dfsadmin -safemode wait （功能描述：等待安全模式状态）DataNodeDataNode工作机制一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。集群运行中可以安全加入和退出一些机器。保证DataNode的数据完整性1）当DataNode读取Block的时候，它会计算CheckSum。2）如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。3）Client读取其他DataNode上的Block。4）DataNode在其文件创建后周期验证CheckSum掉线时限参数设置如果定义超时时间为TimeOut，则超时时长的计算公式为：TimeOut =2dfs.namenode.heartbeat.recheck-interval+10dfs.heartbeat interval。而默认的dfs.namenode.heartbeat.recheck-interval 大小为5分钟，dfs.heartbeatinterval默认为3秒。配置位置：hdfs-site.xml，配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。12345678&lt;property&gt; &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;/name&gt; &lt;value&gt;300000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.heartbeat.interval&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt;服役新节点环境准备​ （1）在hadoop104主机上再克隆一台hadoop105主机​ （2）修改IP地址和主机名称​ （3）删除原来HDFS文件系统留存的文件（/opt/module/hadoop-2.7.2/data和log）​ （4）source一下配置文件12sbin&#x2F;hadoop-daemon.sh start datanodesbin&#x2F;yarn-daemon.sh start nodemanager添加白名单在NameNode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts文件，将允许的集群IP写入在NameNode的hdfs-site.xml配置文件中增加dfs.hosts属性1234&lt;property&gt; &lt;name&gt;dfs.hosts&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts&lt;/value&gt;&lt;/property&gt;配置文件分发刷新namenode：hdfs dfsadmin -refreshNodes更新ResourceManager节点：yarn rmadmin -refreshNodes数据如果不平衡，则：start-balancer.sh添加黑名单（不能同时有节点出现在两个中）在NameNode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts.exclude文件在NameNode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性1234&lt;property&gt;&lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude&lt;/value&gt;&lt;/property&gt;刷新namenode和ResourcemanagerDatanode多目录配置DataNode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本hdfs-site.xml1234&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;&lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2&lt;/value&gt;&lt;/property&gt;","categories":[{"name":"HDFS","slug":"HDFS","permalink":"https://blog.kiedeng.site/categories/HDFS/"},{"name":"Hadoop","slug":"HDFS/Hadoop","permalink":"https://blog.kiedeng.site/categories/HDFS/Hadoop/"}],"tags":[{"name":"原理","slug":"原理","permalink":"https://blog.kiedeng.site/tags/%E5%8E%9F%E7%90%86/"}]},{"title":"HDFS的数据流","slug":"HDFS的数据流","date":"2020-02-13T10:03:05.000Z","updated":"2020-02-13T17:20:15.945Z","comments":true,"path":"2020/02/13/HDFS的数据流/","link":"","permalink":"https://blog.kiedeng.site/2020/02/13/HDFS%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B5%81/","excerpt":"","text":"HDFS写数据流程客户端通过Distributed FileSysetm模块向NameNode请求上传文件，NameNode检查文件是否已存在，父目录是否存在NameNode返回是否可以上传客户端请求第一个Block上传哪几个DataNode服务器上NameNode返回三个DataNode节点，分别为dn1，dn2，dn3客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成dn1，dn2，dn3逐级应答客户端客户端开始往dn1上传第一个Block（先从磁盘读取数据收到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传个dn3，dn1没传一个packet会放入一个应答队列等待应答当一个Bloc传输完成之后，客户端再次请求NameNode上传第二个Block服务器节点距离：两个节点到达最近的共同祖先的距离总和。机架感知对于常见情况，当复制因子为3时，HDFS的放置策略是将一个副本放在本地机架中的一个节点上，另一个放在本地机架中的另一个节点上，最后一个放在不同机架中的另一个节点上HDFS读数据流程客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。","categories":[{"name":"HDFS","slug":"HDFS","permalink":"https://blog.kiedeng.site/categories/HDFS/"},{"name":"Hadoop","slug":"HDFS/Hadoop","permalink":"https://blog.kiedeng.site/categories/HDFS/Hadoop/"}],"tags":[{"name":"原理","slug":"原理","permalink":"https://blog.kiedeng.site/tags/%E5%8E%9F%E7%90%86/"}]},{"title":"HDFS客户端操作","slug":"HDFS客户端操作","date":"2020-02-13T03:16:18.000Z","updated":"2020-02-28T02:03:37.072Z","comments":true,"path":"2020/02/13/HDFS客户端操作/","link":"","permalink":"https://blog.kiedeng.site/2020/02/13/HDFS%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%93%8D%E4%BD%9C/","excerpt":"","text":"客户端环境准备1 将编译hadoop jar包放在路径下（比如：D:\\environment\\hadoop-2.7.2）2 配置HADOOP_HOME环境变量HADOOP_HOME : D:\\environment\\hadoop-2.7.23 配置Path环境变量%HADOOP_HOME%\\bin4 创建一个Maven工程5 导入相应的依赖坐标+日志添加123456789101112131415161718192021222324252627282930313233343536373839404142&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.atguigu&lt;/groupId&gt; &lt;artifactId&gt;HDFS-0529&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;jdk.tools&lt;/groupId&gt; &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;JAVA_HOME&#125;/lib/tools.jar&lt;/systemPath&gt; &lt;/dependency&gt;&lt;/dependencies&gt; &lt;/project&gt;需要再项目的src/main/resources目录下，新建一个文件，命令为”log4j.properties”,在文件中填写log4j.rootLogger=INFO, stdoutlog4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%nlog4j.appender.logfile=org.apache.log4j.FileAppenderlog4j.appender.logfile.File=target/spring.loglog4j.appender.logfile.layout=org.apache.log4j.PatternLayoutlog4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n6 创建包名：com.atguigu.hdfs7 创建HdfsClient类12345678910111213141516171819public class HdfsClient&#123; @Testpublic void testMkdirs() throws IOException, InterruptedException, URISyntaxException&#123; // 1 获取文件系统 Configuration configuration = new Configuration(); // 配置在集群上运行 // configuration.set(\"fs.defaultFS\", \"hdfs://hadoop102:9000\"); // FileSystem fs = FileSystem.get(configuration); FileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:9000\"), configuration, \"atguigu\"); // 2 创建目录 fs.mkdirs(new Path(\"/1108/daxian/banzhang\")); // 3 关闭资源 fs.close(); &#125;&#125;8 执行程序客户端去操作HDFS时，是有一个用户身份的。默认情况下，HDFS客户端API会从JVM中获取一个参数来作为自己的用户身份：-DHADOOP_USER_NAME=atguigu，atguigu为用户名称。1234567891011public static void main(String[] args) throws Exception &#123; // 配置文件 Configuration conf = new Configuration(); conf.set(\"fs.defaultFS\", \"hdfs://hadoop102:9000\"); // 获取客户端对象 FileSystem fs = FileSystem.get(conf); fs.mkdirs(new Path(\"/kangdong\")); // 关闭客户端 fs.close(); System.out.println(\"Over!\"); &#125;HDFS的API操作HDFS文件上传（测试参数优先级）1 编写源代码1234567891011public static void main(String[] args) throws Exception &#123; // 配置文件 Configuration conf = new Configuration(); conf.set(\"fs.defaultFS\", \"hdfs://hadoop102:9000\"); // 获取客户端对象 FileSystem fs = FileSystem.get(conf); fs.mkdirs(new Path(\"/kangdong\")); // 关闭客户端 fs.close(); System.out.println(\"Over!\"); &#125;2 将hdfs-site.xml拷贝到项目的根目录（即资源目录下）123456789&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;3 参数优先级参数优先级排序：（1）客户端代码中设置的值 &gt;（2）ClassPath下的用户自定义配置文件 &gt;（3）然后是服务器的默认配置HDFS文件的下载12345// boolean delSrc 指是否将原文件删除// Path src 指要下载的文件路径// Path dst 指将文件下载到的路径// boolean useRawLocalFileSystem 是否开启文件校验fs.copyToLocalFile(false, new Path(\"/banzhang.txt\"), new Path(\"e:/banhua.txt\"), true);HDFS文件的删除1fs.delete(new Path(\"/0508/\"), true);//第一参数是路径，第二个参数是判断是否递归删除HDFS文件的改名1fs.rename(new Path(\"/banzhang.txt\"), new Path(\"/banhua.txt\"));HDFS文件详情的查看123456789101112131415161718192021222324252627282930313233343536373839404142@Testpublic void testListFiles() throws IOException, InterruptedException, URISyntaxException&#123; // 1获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:9000\"), configuration, \"atguigu\"); // 2 获取文件详情 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path(\"/\"), true); while(listFiles.hasNext())&#123; LocatedFileStatus status = listFiles.next(); // 输出详情 // 文件名称 System.out.println(status.getPath().getName()); // 长度 System.out.println(status.getLen()); // 权限 System.out.println(status.getPermission()); // 分组 System.out.println(status.getGroup()); // 获取存储的块信息 BlockLocation[] blockLocations = status.getBlockLocations(); for (BlockLocation blockLocation : blockLocations) &#123; // 获取块存储的主机节点 String[] hosts = blockLocation.getHosts(); for (String host : hosts) &#123; System.out.println(host); &#125; &#125; System.out.println(\"-----------班长的分割线----------\"); &#125;// 3 关闭资源fs.close();&#125;HDFS的I/O流操作HDFS文件上传12345678910111213@Test public void IOcopyFromLocal() throws Exception&#123; Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:9000\"), conf, \"atguigu\"); FileInputStream fis = new FileInputStream(new File(\"d:/kiedeng.txt\")); FSDataOutputStream fout = fs.create(new Path(\"/kg.txt\")); IOUtils.copyBytes(fis, fout, conf); IOUtils.closeStream(fis); IOUtils.closeStream(fout); fs.close(); &#125;HDFS文件下载12345678910111213141516171819202122// 文件下载@Testpublic void getFileFromHDFS() throws IOException, InterruptedException, URISyntaxException&#123; // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:9000\"), configuration, \"atguigu\"); // 2 获取输入流 FSDataInputStream fis = fs.open(new Path(\"/banhua.txt\")); // 3 获取输出流 FileOutputStream fos = new FileOutputStream(new File(\"e:/banhua.txt\")); // 4 流的对拷 IOUtils.copyBytes(fis, fos, configuration); // 5 关闭资源 IOUtils.closeStream(fos); IOUtils.closeStream(fis); fs.close();&#125;定位文件读取下载第一块1234567891011121314151617181920212223242526@Testpublic void readFileSeek1() throws IOException, InterruptedException, URISyntaxException&#123; // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:9000\"), configuration, \"atguigu\"); // 2 获取输入流 FSDataInputStream fis = fs.open(new Path(\"/hadoop-2.7.2.tar.gz\")); // 3 创建输出流 FileOutputStream fos = new FileOutputStream(new File(\"e:/hadoop-2.7.2.tar.gz.part1\")); // 4 流的拷贝 byte[] buf = new byte[1024]; for(int i =0 ; i &lt; 1024 * 128; i++)&#123; fis.read(buf); fos.write(buf); &#125; // 5关闭资源 IOUtils.closeStream(fis); IOUtils.closeStream(fos);fs.close();&#125;下载第二块1234567891011121314151617181920212223@Testpublic void readFileSeek2() throws IOException, InterruptedException, URISyntaxException&#123; // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:9000\"), configuration, \"atguigu\"); // 2 打开输入流 FSDataInputStream fis = fs.open(new Path(\"/hadoop-2.7.2.tar.gz\")); // 3 定位输入数据位置 fis.seek(1024*1024*128); // 4 创建输出流 FileOutputStream fos = new FileOutputStream(new File(\"e:/hadoop-2.7.2.tar.gz.part2\")); // 5 流的对拷 IOUtils.copyBytes(fis, fos, configuration); // 6 关闭资源 IOUtils.closeStream(fis); IOUtils.closeStream(fos);&#125;合并文件​ 在Window命令窗口中进入到目录E:\\，然后执行如下命令，对数据进行合并​ type hadoop-2.7.2.tar.gz.part2 &gt;&gt; hadoop-2.7.2.tar.gz.part1合并完成后，将hadoop-2.7.2.tar.gz.part1重新命名为hadoop-2.7.2.tar.gz。解压发现该tar包非常完整。","categories":[{"name":"HDFS","slug":"HDFS","permalink":"https://blog.kiedeng.site/categories/HDFS/"},{"name":"Hadoop","slug":"HDFS/Hadoop","permalink":"https://blog.kiedeng.site/categories/HDFS/Hadoop/"}],"tags":[{"name":"代码","slug":"代码","permalink":"https://blog.kiedeng.site/tags/%E4%BB%A3%E7%A0%81/"}]},{"title":"Hadoop编译源码","slug":"Hadoop编译源码","date":"2020-02-12T03:29:53.000Z","updated":"2020-02-28T00:55:28.921Z","comments":true,"path":"2020/02/12/Hadoop编译源码/","link":"","permalink":"https://blog.kiedeng.site/2020/02/12/Hadoop%E7%BC%96%E8%AF%91%E6%BA%90%E7%A0%81/","excerpt":"","text":"jar包准备hadoop-2.7.2-src.tar.gzjdk-8u144-linux-x64.tar.gzapache-ant-1.9.9-bin.tar.gz（build工具，打包用的）apache-maven-3.0.5-bin.tar.gzprotobuf-2.5.0.tar.gz（序列化的框架）jar包安装1 JDK解压，配置环境变量JAVA_HOME和PATH#JAVA_HOME：export JAVA_HOME=/opt/module/jdk1.8.0_144export PATH=$PATH:$JAVA_HOME/binsource /etc/profile进行生效2 Maven解压，配置MAVEN_HOME和PATH[root@hadoop101 software]# tar -zxvf apache-maven-3.0.5-bin.tar.gz -C /opt/module/[root@hadoop101 apache-maven-3.0.5]# vi conf/settings.xml​​nexus-aliyun​central​Nexus aliyun​http://maven.aliyun.com/nexus/content/groups/public​[root@hadoop101 apache-maven-3.0.5]# vi /etc/profile#MAVEN_HOMEexport MAVEN_HOME=/opt/module/apache-maven-3.0.5export PATH=$PATH:$MAVEN_HOME/bin[root@hadoop101 software]#source /etc/profile验证命令：mvn-version2.1 ant解压、配置 ANT _HOME和PATH[root@hadoop101 software]# tar -zxvf apache-ant-1.9.9-bin.tar.gz -C /opt/module/[root@hadoop101 apache-ant-1.9.9]# vi /etc/profile#ANT_HOMEexport ANT_HOME=/opt/module/apache-ant-1.9.9export PATH=$PATH:$ANT_HOME/bi[root@hadoop101 software]#source /etc/profile2.2 安装 glibc-headers 和 g++ 命令如下[root@hadoop101 apache-ant-1.9.9]# yum install glibc-headers[root@hadoop101 apache-ant-1.9.9]# yum install gcc-c++2.3 安装make和cmake[root@hadoop101 apache-ant-1.9.9]# yum install make[root@hadoop101 apache-ant-1.9.9]# yum install cmake2.4 解压protobuf ，进入到解压后protobuf主目录，/opt/module/protobuf-2.5.0，然后相继执行命令[root@hadoop101 software]# tar -zxvf protobuf-2.5.0.tar.gz -C /opt/module/[root@hadoop101 opt]# cd /opt/module/protobuf-2.5.0/[root@hadoop101 protobuf-2.5.0]#./configure[root@hadoop101 protobuf-2.5.0]# make[root@hadoop101 protobuf-2.5.0]# make check[root@hadoop101 protobuf-2.5.0]# make install[root@hadoop101 protobuf-2.5.0]# ldconfig[root@hadoop101 hadoop-dist]# vi /etc/profile#LD_LIBRARY_PATHexport LD_LIBRARY_PATH=/opt/module/protobuf-2.5.0export PATH=$PATH:$LD_LIBRARY_PATH[root@hadoop101 software]#source /etc/profile2.5 安装openssl库[root@hadoop101 software]#yum install openssl-devel2.6 安装 ncurses-devel库[root@hadoop101 software]#yum install ncurses-devel编译源码3.1 解压源码到/opt/目录tar -zxvf hadoop-2.7.2-src.tar.gz -C /opt/3.2 进入源码主目录3.3 通过maven执行编译命令mvn package -Pdist,native -DskipTests -Dtar3.4 成功的64位hadoop包在/opt/hadoop-2.7.2-src/hadoop-dist/target下","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://blog.kiedeng.site/categories/Hadoop/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"软件推荐","slug":"软件推荐","date":"2020-02-11T10:48:17.000Z","updated":"2020-02-11T11:25:52.268Z","comments":true,"path":"2020/02/11/软件推荐/","link":"","permalink":"https://blog.kiedeng.site/2020/02/11/%E8%BD%AF%E4%BB%B6%E6%8E%A8%E8%8D%90/","excerpt":"","text":"TinyTask自动化操作工具Ditto复制神器SecureCRT一般大数据使用的远程工具PanDownload网盘下载天若OCR文字识别用于文字识别Snipaste截图贴图工具Typoramarkdown编辑器Notepad++编辑器，可以进行文件夹文字查找Sublime编辑器，可以远程服务器Bandizip解压工具VMware虚拟机Xshell/Xftp远程连接工具Adobe Acrobat DCpdf阅读，编辑Anki记忆工具Everything查看文件PotPlayer播放器Chrome浏览器","categories":[{"name":"工具","slug":"工具","permalink":"https://blog.kiedeng.site/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"自动化","slug":"自动化","permalink":"https://blog.kiedeng.site/tags/%E8%87%AA%E5%8A%A8%E5%8C%96/"}]},{"title":"Linux使用手册","slug":"Linux使用手册","date":"2020-02-10T08:57:11.000Z","updated":"2020-02-11T04:56:13.605Z","comments":true,"path":"2020/02/10/Linux使用手册/","link":"","permalink":"https://blog.kiedeng.site/2020/02/10/Linux%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/","excerpt":"","text":"用户管理命令#添加useradd kiedeng 或者 useradd -g [组名] 用户名#设置密码useradd 用户名#查看用户是否存在id 用户名#查看创建的用户cat /etc/passwd#切换用户su 用户名（没有获得环境变量）su - 用户名 （获得环境变量）#删除用户userdel 用户名userdel -r 用户名 （用户和用户目录全部删除）#设置root权限vim /etc/sudoers在root下中添加一行用户名 ALL=(ALL) ALL用户名 ALL=(ALL) NOPASSWDALL （不需要密码）修改usermod 修改用户usemod -g 用户组 用户名用户组管理添加：groupadd 组名删除：groupdel 组名修改组：groupmod -n 新组名 老组名查看组：cat /etc/group文件权限类总共10位：0~90位：-表示文件，d代表文件，l代表链接文档1-3位确定属主的该文件权限4-6位确定属组的该文件的权限7-9位确定其他用户改文件的权限修改文件权限chmod 777 a.txtchmod -R 777 xiyou （递归删除）改变所有者chown [选项] [最终用户] [文件或者目录] 选项为-R （递归操作）改变所属组chgrp [最终用户组] [文件或者目录]搜索查找类find 查找文件或者目录基本语法：find [搜索范围] [选项]选项-name&lt;查询方式&gt; 按照指定文件名查找-user&lt;用户名&gt; 指定用户查找-size&lt;文件大小&gt; 按照文件大小查找 （+为大于，-为小于）比如：find /home -size +20458locate快速定位文件路径更新：updatedb基本语法：locate 搜索文件grep 过滤查找及“|”管道符基本语法：grep 选项 查找内容 源文件压缩和解压缩gzip/gunzip压缩只能压缩文件，不能压缩目录命令：gzip 文件；gunzip 文件. gzzip/unzip压缩基本语法：zip [选项] xxx.zip 将要压缩的内容 （目录或文件）unzip [选项] xxx.zip 解压文件选项：-d&lt;目录&gt; 指定压缩后文件存放的目录tar 打包tar [选项] xxx.tar.gz 将要打包进去的内容选项-c 产生tar打包文件-v 显示详细信息-f 指定压缩后的文件名-z 打包同时压缩-x 解包.tar文件压缩：tar -zcvf kie.tar.gz a.txt b.txt解压：tar -zxvf kie.tar.gz -C /opt磁盘分区类","categories":[{"name":"使用手册","slug":"使用手册","permalink":"https://blog.kiedeng.site/categories/%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://blog.kiedeng.site/tags/Linux/"}]},{"title":"hadoop集群搭建","slug":"hadoop集群搭建","date":"2020-02-10T06:11:15.000Z","updated":"2020-05-16T10:31:43.300Z","comments":true,"path":"2020/02/10/hadoop集群搭建/","link":"","permalink":"https://blog.kiedeng.site/2020/02/10/hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/","excerpt":"","text":"克隆虚拟机配置好的Linux虚拟机-&gt; 管理 -&gt; 克隆1 删除网卡，复制物理地址：vim /etc/udev/rules.d/70-persistent-net.rules1删除eht0的那一行，将下一行的eth0改为eth12 配置网络：vim /etc/sysconfig/network-scripts/ifcfg-eth0(删除UUID)123456IPADDR&#x3D;192.168.1.101 设置ipONBOOT&#x3D;yesNM_CONTROLLED&#x3D;yesB00TPROTO&#x3D;staticGATEWAY&#x3D;192.168.1.2DNS1&#x3D;192.168.1.2 和网关一致3 修改主机名称：vim /etc/sysconfig/network12NETWORKING&#x3D;yesHOSTNAME&#x3D;hadoopl 014 修改主机映射：vim /etc/hosts5 windows的映射：C:\\Windows\\System32\\drivers\\etc安装JDK1 通过sftp将jdk放在/opt/software下1tar -zxvf [gz文件名] -C [解压的路径]2 设置环境路径123#JAVA_HOMEexport JAVA_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;jdk1.8.0_144export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin3 让修改以后的文件生效1source &#x2F;etc&#x2F;profile安装Hadoop将hadoop-2.7.2.tar.gz传入，解压在/etc/profile添加环境变量1234##HADOOP_HOMEexport HADOOP_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;binexport PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;sbin生效文件：source /etc/profileHadoop的目录结构bin目录：存放对Hadoop相关服务（HDFS和YARN）进行操作的脚本etc目录：Hadoop的配置目录，存放Hadoop的配置文件lib目录：存放Hadoop的本地库（对数据进行压缩解压功能）sbin目录：存放启动或者停止Hadoop相关服务的脚本share目录：存放hadoop的jar包，官方文档和案例Hadoop的运行模式​ 本地模式，伪分布式模式，完成分布式模式一 本地模式官方Grep案例在hadoop-2.7.2文件下面创建一个input文件夹复制文件到input文件夹里面执行share文件夹下的MapReduce程序1bin&#x2F;hadoop jar share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-2.7.2.jar grep input output &#39;dfs[a-z.]+&#39;查看输出结果1cat output&#x2F;*官方WordCount案例（计算单词个数）创建文件夹（wcinput）在该文件夹创建并编辑文件内容执行1hadoop jar share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-2.7.2.jar wordcount wcinput wcoutput查看结果1hadoop-2.7.2]$ cat wcoutput&#x2F;part-r-00000二 伪分布式运行模式启动HDFS并运行MapReduce程序1 配置集群hadoop-env.sh(添加jdk路径)1export JAVA_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;jdk1.8.0_144core-site.xml123456789101112131415161718192021&lt;!-- 指定HDFS中NameNode的地址 --&gt;&lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt;&lt;value&gt;hdfs://hadoop101:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;&lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;&lt;/property&gt;hdfs-site.xml123456789&lt;!-- 指定HDFS副本的数量 --&gt;&lt;property&gt;&lt;name&gt;dfs.replication&lt;/name&gt;&lt;value&gt;1&lt;/value&gt;&lt;/property&gt;2 启动集群格式化NameNode（第一次启动时格式化，以后就不要格式化bin/hdfs namenode -format启动NameNodesbin/hadoop-daemon.sh start namenode启动DataNodesbin/hadoop-daemon.sh start datanode3 查看集群web访问默认端口：50070打开失败：https://www.cnblogs.com/zlslch/p/6604189.htmllog日志：/opt/module/hadoop-2.7.2/logs不能一直格式化NameNode原因会产生新的集群id，导致NameNode和DataNode的集群id不一致，集群找不到数据，所以格式化前先删除data和log日志4 操作集群（wordcount）执行操作和本地模式一样启动YARN并运行MapReduce程序1 配置集群yarn-env.sh(添加路径)1export JAVA_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;jdk1.8.0_144配置yarn-site.xml123456789101112131415161718192021&lt;!-- Reducer获取数据的方式 --&gt;&lt;property&gt;&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;&lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定YARN的ResourceManager的地址 --&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;&lt;value&gt;hadoop101&lt;/value&gt;&lt;/property&gt;mapred-env.sh(添加jdk路径)(对mapred-site.xml.template重新命名为) mapred-site.xml123456789&lt;!-- 指定MR运行在YARN上 --&gt;&lt;property&gt;&lt;name&gt;mapreduce.framework.name&lt;/name&gt;&lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;2 启动集群启动前必须先启动NameNode和DataNode启动ResourceManager1sbin&#x2F;yarn-daemon.sh start resourcemanager启动NodeManager1sbin&#x2F;yarn-daemon.sh start nodemanager3 集群操作yarn访问端口：8088删除文件系统的ouput文件bin/hdfs dfs -rm -R /user/kiedong/output执行MapReduce程序bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/atguigu/input /user/atguigu/output配置历史服务器配置mapred-site.xml,添加12345678910111213141516171819&lt;!-- 历史服务器端地址 --&gt;&lt;property&gt;&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;&lt;value&gt;hadoop101:10020&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史服务器web端地址 --&gt;&lt;property&gt;&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;&lt;value&gt;hadoop101:19888&lt;/value&gt;&lt;/property&gt;启动历史服务器sbin/mr-jobhistory-daemon.sh start historyserver端口：19888配置日志的聚集日志聚集的概念：应用程序运行完成以后，程序运行日志上传到HDFS系统上日志聚集的好处：方便开发调试注意：开启此功能，需要重新启动NodeManager，ResourceManager和HistoryManageryarn-site.xml123456789101112131415161718192021&lt;!-- 日志聚集功能使能 --&gt;&lt;property&gt;&lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 日志保留时间设置7天 --&gt;&lt;property&gt;&lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;&lt;value&gt;604800&lt;/value&gt;&lt;/property&gt;关闭,然后NodeManager 、ResourceManager和HistoryManager执行WordCount案例查看日志端口：19888三 完全分布式运行模式（重点）主要步骤：准备三台客户机（关闭防火墙，静态ip，主机名称）安装JDK配置环境变量安装Hadoop配置环境变量配置集群单点启动配置ssh群起并测试集群编写集群分发脚本xsyncscp（secure copy）安全拷贝scp可以实现服务器与服务器之间的数据拷贝语法:将hadoop101传给hadoop102scp -r /opt/module root@hadoop102:/opt/modulersync远程同步工具主要用于备份和镜像。与scp区别：rsync只对差异性文件做更新。scp是复制所有文件rsync -rvl /opt/software root@hadoop102:/opt/software-r 递归 -v显示复制过程 -l拷贝符号连接xsync集群分发脚本（本集群使用）需求：循环复制文件到所有节点的相同目录下在home/用户名/bin这个目录下存放脚本，这样次用户可以在系统任何地方直接执行编写代码：vim xsync12345678910111213141516171819202122232425#!/bin/bash#1 获取输入参数个数，如果没有参数，直接退出pcount=$#if((pcount==0)); thenecho no args;exit;fi#2 获取文件名称p1=$1fname=`basename $p1`echo fname=$fname#3 获取上级目录到绝对路径pdir=`cd -P $(dirname $p1); pwd`echo pdir=$pdir#4 获取当前用户名称user=`whoami`#5 循环for((host=103; host&lt;105; host++)); do echo ------------------- hadoop$host -------------- rsync -rvl $pdir/$fname $user@hadoop$host:$pdirdone修改脚本xsync具有执行权限chomod 777 xsync配置集群核心配置文件core-site.xml123456789101112131415161718192021&lt;!-- 指定HDFS中NameNode的地址 --&gt;&lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt;&lt;value&gt;hdfs://hadoop102:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;&lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;&lt;/property&gt;HDFS配置文件hadoop-en.sh：配置环境变量hdfs-site.xml1234567891011121314151617&lt;property&gt;&lt;name&gt;dfs.replication&lt;/name&gt;&lt;value&gt;3&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;&lt;value&gt;hadoop104:50090&lt;/value&gt;&lt;/property&gt;YARN配置文件yarn-env.sh：配置环境变量yarn-site.xml123456789101112131415161718192021&lt;!-- Reducer获取数据的方式 --&gt;&lt;property&gt;&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;&lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定YARN的ResourceManager的地址 --&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;&lt;value&gt;hadoop103&lt;/value&gt;&lt;/property&gt;MapReduce配置文件mapred-env.sh：配置环境变量配置mapred-env.sh1234567891011cp mapred-site.xml.template mapred-site.xml&lt;!-- 指定MR运行在Yarn上 --&gt;&lt;property&gt;&lt;name&gt;mapreduce.framework.name&lt;/name&gt;&lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;集群单点启动第一次启动，需要格式化NameNodehadoop namenode -format在hadoop102上启动NameNodehadoop-daemon.sh start namenode在hadoop102,103,104分别启动DataNodessh无密码登陆配置原理将A服务器生成的公钥拷贝给B服务器，当A将数据用私钥A加密，则B用A的公钥解密，并将数据用公钥加密给A生成公钥和私钥ssh-keygen -t rsa将公钥拷贝到免密的目标机器上ssh-copy-id hadoop102ssh-copy-id hadoop103ssh-copy-id hadoop104注意：root用户需要再进行一次拷贝，在hadoop103生成公私钥拷贝给其他机器.ssh文件夹下文件功能解释known_hosts:记录ssh访问过计算机的公钥id_rsa : 生成的私钥id_rsa.pub:生成的公钥authorized_keys ： 存放授权过得无密登录服务器公钥群起集群配置slaves，并同步slaveshadoop102hadoop103hadoop104启动集群第一次先格式化NameNode（先停止以前运行的namenode和datanode，然后删除data和log数据）启动sbin/start-dfs.sh (hadoop102上)sbin/start-yarn.sh（hadoop03上）","categories":[{"name":"部署","slug":"部署","permalink":"https://blog.kiedeng.site/categories/%E9%83%A8%E7%BD%B2/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://blog.kiedeng.site/tags/Hadoop/"},{"name":"大数据","slug":"大数据","permalink":"https://blog.kiedeng.site/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"Linux安装与配置","slug":"Linux安装与配置","date":"2020-02-10T05:35:12.000Z","updated":"2020-02-28T02:13:53.516Z","comments":true,"path":"2020/02/10/Linux安装与配置/","link":"","permalink":"https://blog.kiedeng.site/2020/02/10/Linux%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/","excerpt":"","text":"安装时注意事项(用VMware安装centos[大数据虚拟机])检查BIOS虚拟化支持内存默认设置为2048MB最大磁盘大小默认为20GB是否对CD媒体进行测试，直接跳过Skip创建自定义分区(都是标准分区)123boot 默认： 100MBwap 默认：2048MB&#x2F; 默认：15360自定义系统软件1234基本系统：兼容程序库；基本应用程序：互联网浏览器桌面：除了KDE,其余都选语言支持：中文支持Kdump去掉查看网络IP和网关编辑 -&gt; 虚拟网络编辑器 -&gt; NAT模式 即可看到子网IPNET设置可以看到网关设置IP自动获取登录后，通过界面来设置自动获取指定固定ip​ 直接修改配置文件来指定IP,并可以连接到外网(程序员推荐)，编辑 vi /etc/sysconfig/network-scripts/ifcfg-eth0​ 要求：将ip地址配置的静态的，ip地址为192.168.xxx.xxx1234567891011121314DEVICE&#x3D;eth0 #接口名（设备,网卡）HWADDR&#x3D;00:0C:2x:6x:0x:xx #MAC地址 TYPE&#x3D;Ethernet #网络类型（通常是Ethemet）UUID&#x3D;926a57ba-92c6-4231-bacb-f27e5e6a9f44 #随机id#系统启动的时候网络接口是否有效（yes&#x2F;no）ONBOOT&#x3D;yes # IP的配置方法[none|static|bootp|dhcp]（引导时不使用协议|静态分配IP|BOOTP协议|DHCP协议）BOOTPROTO&#x3D;static #IP地址IPADDR&#x3D;192.168.189.130 #网关 GATEWAY&#x3D;192.168.189.2 #域名解析器DNS1&#x3D;192.168.189.2重启网络服务或者重启系统生效：service network restart 、reboot修改主机名查看当前主机名：hostname修改主机名：/etc/hostname修改主机映射文件：vim /etc/sysconfig/network1234NETWORKING&#x3D;yesNETWORKING_IPV6&#x3D;noHOSTNAME&#x3D; hadoop &#x2F;&#x2F;写入新的主机名注意：主机名称不要有“_”下划线修改ip与主机的映射：/etc/hosts1192.168.102.130 hadoopWindows设置本地dns解析C:\\Windows\\System32\\drivers\\etc\\hosts添加内容：192.168.102.130 hadoop","categories":[{"name":"部署","slug":"部署","permalink":"https://blog.kiedeng.site/categories/%E9%83%A8%E7%BD%B2/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://blog.kiedeng.site/tags/Linux/"},{"name":"VMware","slug":"VMware","permalink":"https://blog.kiedeng.site/tags/VMware/"},{"name":"centos","slug":"centos","permalink":"https://blog.kiedeng.site/tags/centos/"}]},{"title":"git的使用","slug":"git的使用","date":"2020-02-10T03:44:17.000Z","updated":"2020-02-10T05:13:33.539Z","comments":true,"path":"2020/02/10/git的使用/","link":"","permalink":"https://blog.kiedeng.site/2020/02/10/git%E7%9A%84%E4%BD%BF%E7%94%A8/","excerpt":"","text":"Git的优势：大部分操作在本地完成，不需要联网完整性保证尽可能添加数据而不是删除或修改数据分支操作非常快捷流畅与Linux命令全面兼容代码托管中心代码托管中心的任务：维护远程库局域网环境：GitLab服务器外网环境：GitHub，码云Git命令行操作本库初始化（选择文件夹进行初始化）1git init设置签名作用：区分不同开发者的身份辨析：这里设置的签名和登录远程库(代码托管中心)的账号、密码没有任何关系。命令：123456789项目级别&#x2F;仓库级别：仅在当前本地库范围内有效git config user.name kiedenggit config user.email kiedeng@qq.com信息保存位置：.&#x2F;.git&#x2F;config 文件系统用户级别：登录当前操作系统的用户范围git config --global user.name tom_glbgit config --global goodMorning_pro@atguigu.com信息保存位置：~&#x2F;.gitconfig 文件基本操作123456789101112# 状态查看git status# 添加 (将工作区的文件或目录提交到暂存区)git [filename]# 提交 (将暂存区的文件提交的本地库)git commit -m &quot;commit message&quot; [filename]# 查看历史版本git loggit reflog# 版本的前进与后退（基于索引值操作）git reset --hard [局部索引值]git reset --hard a6ace91分支管理分支：在版本控制过程中，使用多条线同时推进多个任务。12345678# 创建分支git branch [分支名]# 查看分支git branch -v# 切换分支git checkout [分支名]# 合并git merge [被和并的分支名]GitHub12345678# 查看所有远程地址别名git remote -v# 创建远程库地址别名git remote add [别名] [远程地址]# 推送 (将本地库上传到github仓库)git push [别名] [分支名]# 克隆(这样克隆：把远程库下载到本地，初始化本地库，创建别名)git origin [远程地址]","categories":[{"name":"使用手册","slug":"使用手册","permalink":"https://blog.kiedeng.site/categories/%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/"}],"tags":[{"name":"Git","slug":"Git","permalink":"https://blog.kiedeng.site/tags/Git/"},{"name":"GitHub","slug":"GitHub","permalink":"https://blog.kiedeng.site/tags/GitHub/"}]},{"title":"hexo搭建","slug":"hexo搭建","date":"2020-02-10T01:10:23.000Z","updated":"2020-02-28T02:07:12.795Z","comments":true,"path":"2020/02/10/hexo搭建/","link":"","permalink":"https://blog.kiedeng.site/2020/02/10/hexo%E6%90%AD%E5%BB%BA/","excerpt":"","text":"官方文档： 链接一，使用Windows完成本地部署1 安装node.js和git,默认安装方式即可2 安装hexo，打开cmd执行1npm install hexo-cli -g3 cmd移动到选择的一个文件夹，比如：d:\\blog(下面全部假设初始化在本路径),进行hexo的初始化1hexo init blog4 在此目录安装1npm install5 启动服务器，访问的默认地址：http://localhost:4000/1hexo s二，使用GitHub完成远程部署1 注册，登录github2 新建仓库步骤如下：点击右上角+号，new repository，在Repository name处填 （你的gitusername）.github.io（比如：kiedeng.github.io），然后直接点Create repository3 在你初始化的路径（比如的d:\\blog）下有一个_config.xml,用记事本打开此文件，最后几行添加github信息12345(对于repo，比如：https:&#x2F;&#x2F;github.com&#x2F;kiedeng&#x2F;kiedeng.github.io.git)deploy:type: gitrepo: https:&#x2F;&#x2F;github.com&#x2F;( yours username)&#x2F;（your username）.github.io.gitbranck: master4 将cmd移动到d:\\blog下，安装1npm install hexo-deployer-git --save5 执行123456# 清理hexo clean# 生成静态文件hexo generate# 上传hexo deploy在弹出的git窗口中输入你的GitHub邮箱和密码部署完成，等待一会，使用比如：http://kiedeng.github.io/访问三，更换hexo主题找到hexo的主题推荐主题：链接选择一款，到达它们的github仓库（如果该主题作者的有文档，按文档即可完成更换）将该主题下载下来（克隆也行），解压到d:\\blog\\themes,将该文件目录更名，比如：kiedeng打开d:\\blog_config.xml,将theme: 后面的参数改为1theme: kiedeng然后就可以部署和上传了四，绑定域名选择一个合适的域名，买下域名在域名的详细界面，打开解析设置dns解析在d:\\blog\\source目录下，新建一个叫CNAME的文件（强调：不能有后缀），里面的内容为你的域名，比如:www.kangdong.store等待一小会即可进行访问","categories":[{"name":"部署","slug":"部署","permalink":"https://blog.kiedeng.site/categories/%E9%83%A8%E7%BD%B2/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://blog.kiedeng.site/tags/%E6%95%99%E7%A8%8B/"},{"name":"hexo","slug":"hexo","permalink":"https://blog.kiedeng.site/tags/hexo/"}]}],"categories":[{"name":"hadoop","slug":"hadoop","permalink":"https://blog.kiedeng.site/categories/hadoop/"},{"name":"安装","slug":"安装","permalink":"https://blog.kiedeng.site/categories/%E5%AE%89%E8%A3%85/"},{"name":"Azkaba","slug":"Azkaba","permalink":"https://blog.kiedeng.site/categories/Azkaba/"},{"name":"大数据","slug":"大数据","permalink":"https://blog.kiedeng.site/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"typecho","slug":"typecho","permalink":"https://blog.kiedeng.site/categories/typecho/"},{"name":"未分类","slug":"未分类","permalink":"https://blog.kiedeng.site/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/"},{"name":"hive","slug":"hive","permalink":"https://blog.kiedeng.site/categories/hive/"},{"name":"成长","slug":"成长","permalink":"https://blog.kiedeng.site/categories/%E6%88%90%E9%95%BF/"},{"name":"mongdb","slug":"mongdb","permalink":"https://blog.kiedeng.site/categories/mongdb/"},{"name":"Linux","slug":"Linux","permalink":"https://blog.kiedeng.site/categories/Linux/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://blog.kiedeng.site/categories/Hadoop/"},{"name":"Zookeeper","slug":"Zookeeper","permalink":"https://blog.kiedeng.site/categories/Zookeeper/"},{"name":"常见错误","slug":"常见错误","permalink":"https://blog.kiedeng.site/categories/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"name":"工具","slug":"工具","permalink":"https://blog.kiedeng.site/categories/%E5%B7%A5%E5%85%B7/"},{"name":"Tomcat","slug":"Tomcat","permalink":"https://blog.kiedeng.site/categories/Tomcat/"},{"name":"算法","slug":"算法","permalink":"https://blog.kiedeng.site/categories/%E7%AE%97%E6%B3%95/"},{"name":"MySQL","slug":"MySQL","permalink":"https://blog.kiedeng.site/categories/MySQL/"},{"name":"Mapreduce","slug":"Hadoop/Mapreduce","permalink":"https://blog.kiedeng.site/categories/Hadoop/Mapreduce/"},{"name":"HDFS","slug":"HDFS","permalink":"https://blog.kiedeng.site/categories/HDFS/"},{"name":"Hadoop","slug":"HDFS/Hadoop","permalink":"https://blog.kiedeng.site/categories/HDFS/Hadoop/"},{"name":"使用手册","slug":"使用手册","permalink":"https://blog.kiedeng.site/categories/%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/"},{"name":"部署","slug":"部署","permalink":"https://blog.kiedeng.site/categories/%E9%83%A8%E7%BD%B2/"}],"tags":[{"name":"默认","slug":"默认","permalink":"https://blog.kiedeng.site/tags/%E9%BB%98%E8%AE%A4/"},{"name":"LeetCode","slug":"LeetCode","permalink":"https://blog.kiedeng.site/tags/LeetCode/"},{"name":"MapReduce","slug":"MapReduce","permalink":"https://blog.kiedeng.site/tags/MapReduce/"},{"name":"部署","slug":"部署","permalink":"https://blog.kiedeng.site/tags/%E9%83%A8%E7%BD%B2/"},{"name":"案例","slug":"案例","permalink":"https://blog.kiedeng.site/tags/%E6%A1%88%E4%BE%8B/"},{"name":"原理","slug":"原理","permalink":"https://blog.kiedeng.site/tags/%E5%8E%9F%E7%90%86/"},{"name":"代码","slug":"代码","permalink":"https://blog.kiedeng.site/tags/%E4%BB%A3%E7%A0%81/"},{"name":"自动化","slug":"自动化","permalink":"https://blog.kiedeng.site/tags/%E8%87%AA%E5%8A%A8%E5%8C%96/"},{"name":"Linux","slug":"Linux","permalink":"https://blog.kiedeng.site/tags/Linux/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://blog.kiedeng.site/tags/Hadoop/"},{"name":"大数据","slug":"大数据","permalink":"https://blog.kiedeng.site/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"VMware","slug":"VMware","permalink":"https://blog.kiedeng.site/tags/VMware/"},{"name":"centos","slug":"centos","permalink":"https://blog.kiedeng.site/tags/centos/"},{"name":"Git","slug":"Git","permalink":"https://blog.kiedeng.site/tags/Git/"},{"name":"GitHub","slug":"GitHub","permalink":"https://blog.kiedeng.site/tags/GitHub/"},{"name":"教程","slug":"教程","permalink":"https://blog.kiedeng.site/tags/%E6%95%99%E7%A8%8B/"},{"name":"hexo","slug":"hexo","permalink":"https://blog.kiedeng.site/tags/hexo/"}]}