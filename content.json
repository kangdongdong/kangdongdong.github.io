{"meta":{"title":"成长印记","subtitle":"","description":"","author":"康栋","url":"http://kiedeng.github.io","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2020-02-09T14:37:29.946Z","updated":"2020-02-09T14:37:29.946Z","comments":false,"path":"/404.html","permalink":"http://kiedeng.github.io/404.html","excerpt":"","text":""},{"title":"书单","date":"2020-02-09T06:13:49.098Z","updated":"2020-02-09T03:33:37.347Z","comments":false,"path":"books/index.html","permalink":"http://kiedeng.github.io/books/index.html","excerpt":"","text":""},{"title":"简介","date":"2021-03-05T13:56:45.863Z","updated":"2021-03-05T13:56:45.863Z","comments":false,"path":"about/index.html","permalink":"http://kiedeng.github.io/about/index.html","excerpt":"","text":"教育2017.09.09 - 2021.06.06 郑州轻工业大学 本科 计算机科学与技术 ACM集训队成员 工作数据的离线数据处理和实时处理 hadoop, spark,linux, scala 附录"},{"title":"Repositories","date":"2020-02-09T15:26:19.807Z","updated":"2020-02-09T15:26:19.807Z","comments":false,"path":"repository/index.html","permalink":"http://kiedeng.github.io/repository/index.html","excerpt":"","text":""},{"title":"分类","date":"2020-02-09T15:25:56.144Z","updated":"2020-02-09T03:33:37.347Z","comments":false,"path":"categories/index.html","permalink":"http://kiedeng.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2021-03-05T08:28:42.000Z","updated":"2021-03-05T08:28:42.193Z","comments":true,"path":"tags/index-1.html","permalink":"http://kiedeng.github.io/tags/index-1.html","excerpt":"","text":""},{"title":"友情链接","date":"2021-03-05T12:44:49.227Z","updated":"2020-02-09T03:33:37.348Z","comments":true,"path":"links/index.html","permalink":"http://kiedeng.github.io/links/index.html","excerpt":"","text":""},{"title":"标签","date":"2021-03-05T12:46:33.475Z","updated":"2021-03-05T12:46:33.475Z","comments":false,"path":"tags/index.html","permalink":"http://kiedeng.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"","slug":"beautifulsoup 知识点总结","date":"2021-04-26T04:49:12.554Z","updated":"2021-04-27T08:27:42.231Z","comments":true,"path":"2021/04/26/beautifulsoup 知识点总结/","link":"","permalink":"http://kiedeng.github.io/2021/04/26/beautifulsoup%20%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/","excerpt":"","text":"beautifulsoup 知识点总结 一，导包，读取文档内容12from bs4 import BeautifulSoupsoup = BeautifulSoup(text,&quot;lxml&quot;) 二，基本方法使用123456789# 查找第一个属性为tbody的标签s = soup.find(&quot;tbody&quot;)# 返回为tag的列表，可以再在进行查找操作s.contents# 返回的类型基本为str，不能进行继续查找操作s.children","categories":[],"tags":[]},{"title":"小说阅读器","slug":"小说阅读器","date":"2021-03-09T06:13:28.000Z","updated":"2021-03-09T06:16:05.514Z","comments":true,"path":"2021/03/09/小说阅读器/","link":"","permalink":"http://kiedeng.github.io/2021/03/09/%E5%B0%8F%E8%AF%B4%E9%98%85%E8%AF%BB%E5%99%A8/","excerpt":"","text":"功能介绍一个自定义数据源的阅读小说工具 软件路径https://github.com/gedoor/legado 数据源https://yuedu.xiu2.xyz/shuyuan https://moonbegonia.github.io/Source/yuedu/audio.json https://gitee.com/namofree/yuedu3/raw/legado3booksource/legado3_booksource_by_Namo.json","categories":[{"name":"生活","slug":"生活","permalink":"http://kiedeng.github.io/categories/%E7%94%9F%E6%B4%BB/"}],"tags":[{"name":"小说 工具","slug":"小说-工具","permalink":"http://kiedeng.github.io/tags/%E5%B0%8F%E8%AF%B4-%E5%B7%A5%E5%85%B7/"}]},{"title":"数据采集模块","slug":"数据采集模块","date":"2020-05-17T07:01:50.000Z","updated":"2020-05-17T09:14:14.771Z","comments":true,"path":"2020/05/17/数据采集模块/","link":"","permalink":"http://kiedeng.github.io/2020/05/17/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E6%A8%A1%E5%9D%97/","excerpt":"","text":"一，hadoop安装1 HDFS存储多目录在hdfs-site.xml中操作 1234&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;&lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///hd2/dfs/data2,file:///hd3/dfs/data3,file:///hd4/dfs/data4&lt;/value&gt;&lt;/property&gt; 数据平衡指令 123bin/start-balancer.sh –threshold 10## 对于参数10，代表的是集群中各个节点的磁盘空间利用率相差不超过10%bin/stop-balancer.sh 2 支持LZO压缩配置 将编译好的hadoop-lzo-0.4.20.jar 放入hadoop-2.7.2/share/hadoop/common/，并且分发给其他hadoop机器 core-site.xml增加配置支持LZO压缩，并分发给其他机器 123456789101112131415161718192021&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt;&lt;property&gt;&lt;name&gt;io.compression.codecs&lt;/name&gt;&lt;value&gt;org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt; &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 重启集群 3 LZO创建索引LZO的可切片特性依赖于索引，所以需要为LZO压缩文件创建索引 123hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /input/bigtable.lzo# hadoop jar (lzo的jar文件) com.hadoop.compression.lzo.DistributedLzoIndexer (输出文件) 4 基准测试1）测试写性能 1hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.2-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 128MB 2）测试读性能 1hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.2-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 128MB 3）测试删除生成数据 1hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.2-tests.jar TestDFSIO -clean 5 Hadoop参数调优1 hdfs-site.xml 12345dfs.namenode.handler.count=20 * log2(Cluster Size)，比如集群规模为8台时，此参数设置为60NameNode有一个工作线程池，用来处理不同DataNode的并发心跳以及客户端并发的元数据操作。对于大集群或者有大量客户端的集群来说，通常需要增大参数dfs.namenode.handler.count的默认值10。设置该值的一般原则是将其设置为集群大小的自然对数乘以20，即20logN，N为集群大小。 2 参数调优yarn-site.xml 1234567内存利用率不够。这个一般是Yarn的2个配置造成的，单个任务可以申请的最大内存大小，和Hadoop单个节点可用内存大小。调节这两个参数能提高系统内存的利用率。（a）yarn.nodemanager.resource.memory-mb表示该节点上YARN可使用的物理内存总量，默认是8192（MB），注意，如果你的节点内存资源不够8GB，则需要调减小这个值，而YARN不会智能的探测节点的物理内存总量。（b）yarn.scheduler.maximum-allocation-mb单个任务可申请的最多物理内存量，默认是8192（MB）。 3 Hadoop宕机 如果MR造成系统宕机。此时要控制Yarn同时运行的任务数，和每个任务申请的最大内存。 如果写入文件过量造成NameNode宕机。那么调高Kafka的存储大小，控制从Kafka到HDFS的写入速度。 二，Zookeeper安装三，日志生成四，采集日志Flume五，Kafka安装六","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://kiedeng.github.io/categories/hadoop/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"安装superset","slug":"安装superset","date":"2020-05-17T02:31:34.000Z","updated":"2020-05-17T02:47:16.263Z","comments":true,"path":"2020/05/17/安装superset/","link":"","permalink":"http://kiedeng.github.io/2020/05/17/%E5%AE%89%E8%A3%85superset/","excerpt":"","text":"概述：Apache Superset是一个开源的、现代的、轻量级BI分析工具，能够对接多种数据源、拥有丰富的图标展示形式、支持自定义仪表盘，且拥有友好的用户界面，十分易用。 1 安装python环境安装Miniconda1）下载Miniconda（Python3版本） 下载地址：https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh 2）安装Miniconda (1) 执行以下命令进行安装，并按照提示操作，直到安装完成。 1bash Miniconda3-latest-Linux-x86_64.sh (2) 安装过程中，可以指定安装路径 (3) 配置Miniconda的环境变量（可以不配，在安装路径的路径） (4)取消激活base环境 1conda config --set auto_activate_base false 创建Python3.6环境1 配置conda国内镜像 123conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/freeconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/mainconda config --set show_channel_urls yes 2 创建Python3.6环境 12345conda create --name superset python=3.6说明：conda环境管理常用命令创建环境：conda create -n env_name查看所有环境：conda info --envs删除一个环境：conda remove -n env_name --all 3 激活superset环境 12conda activate supersetconda deactivate(退出激活) 2 Superset部署1 安装依赖 12sudo yum install -y python-setuptoolssudo yum install -y gcc gcc-c++ libffi-devel python-devel python-pip python-wheel openssl-devel cyrus-sasl-devel openldap-devel 2 安装superset 12345678910111213141516## 安装（更新）setuptools和pippip install --upgrade setuptools pip -i https://pypi.douban.com/simple/## 安装Supetsetpip install apache-superset -i https://pypi.douban.com/simple/## 初始化Supetset数据库superset db upgrade## 创建管理员用户export FLASK_APP=supersetflask fab create-admin## Superset初始化superset init 3 启动superset 1 安装gunicorn 1pip install gunicorn -i https://pypi.douban.com/simple/ 2 启动superset（确保环境为superset） 1234567gunicorn --workers 5 --timeout 120 --bind hadoop102:8787 superset:app --daemon 说明：--workers：指定进程个数--timeout：worker进程超时时间，超时会自动重启--bind：绑定本机地址，即为Superset访问地址--daemon：后台运行 3 停止superseet 1234ps -ef | awk &#x27;/gunicorn/ &amp;&amp; !/awk/&#123;print $2&#125;&#x27; | xargs kill -9退出环境conda deactivate 4 登录supserset 1默认的端口为8787 3 Superset的使用","categories":[{"name":"安装","slug":"安装","permalink":"http://kiedeng.github.io/categories/%E5%AE%89%E8%A3%85/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"azkaban安装部署","slug":"azkaban安装部署","date":"2020-05-15T09:12:43.000Z","updated":"2020-05-16T10:01:11.193Z","comments":true,"path":"2020/05/15/azkaban安装部署/","link":"","permalink":"http://kiedeng.github.io/2020/05/15/azkaban%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/","excerpt":"","text":"生成密钥库所出现的问题补充：keytool -genkey -alias tomcat1 -keyalg RSA -keystore second.keystore 参考链接：链接 配置邮箱1234567# mail settingsmail.sender=572245955@qq.commail.host=smtp.qq.commail.user=572245955@qq.commail.password=(填写qq的授权码)job.failure.email=job.success.email=","categories":[{"name":"Azkaba","slug":"Azkaba","permalink":"http://kiedeng.github.io/categories/Azkaba/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"常见命令行操作","slug":"常见命令行操作","date":"2020-05-13T08:25:07.000Z","updated":"2020-05-13T09:40:20.395Z","comments":true,"path":"2020/05/13/常见命令行操作/","link":"","permalink":"http://kiedeng.github.io/2020/05/13/%E5%B8%B8%E8%A7%81%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/","excerpt":"","text":"Kafka常见命令1234567启动生产者bin/kafka-console-producer.sh \\--broker-list hadoop102:9092 --topic first启动消费者（zookeeper存储offset）bin/kafka-console-consumer.sh --topic atguigu --zookeeper hadoop102:2181启动消费者（本地存储offset）bin/kafka-console-consumer.sh --topic shangguigu --bootstrap-server hadoop102:9092","categories":[{"name":"大数据","slug":"大数据","permalink":"http://kiedeng.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"主题的美化","slug":"主题的美化","date":"2020-05-05T11:28:19.000Z","updated":"2020-05-06T13:22:42.344Z","comments":true,"path":"2020/05/05/主题的美化/","link":"","permalink":"http://kiedeng.github.io/2020/05/05/%E4%B8%BB%E9%A2%98%E7%9A%84%E7%BE%8E%E5%8C%96/","excerpt":"","text":"一，增加天气效果打开心知天气的官网，注册账号，并开通免费版服务。之后在/handsome/component/headnav.php中搜索，在搜索到的地方的前一行添加如下代码，并把其中的公钥和私钥修改为你自己的即可。 12345678910111213141516171819&lt;!-- 知心天气--&gt; &lt;div id=&quot;tp-weather-widget&quot; class=&quot;navbar-form navbar-form-sm navbar-left shift&quot;&gt;&lt;/div&gt;&lt;script&gt;(function(T,h,i,n,k,P,a,g,e)&#123;g=function()&#123;P=h.createElement(i);a=h.getElementsByTagName(i)[0];P.src=k;P.charset=&quot;utf-8&quot;;P.async=1;a.parentNode.insertBefore(P,a)&#125;;T[&quot;ThinkPageWeatherWidgetObject&quot;]=n;T[n]||(T[n]=function()&#123;(T[n].q=T[n].q||[]).push(arguments)&#125;);T[n].l=+new Date();if(T.attachEvent)&#123;T.attachEvent(&quot;onload&quot;,g)&#125;else&#123;T.addEventListener(&quot;load&quot;,g,false)&#125;&#125;(window,document,&quot;script&quot;,&quot;tpwidget&quot;,&quot;//widget.seniverse.com/widget/chameleon.js&quot;))&lt;/script&gt;&lt;script&gt;tpwidget(&quot;init&quot;, &#123; &quot;flavor&quot;: &quot;slim&quot;, &quot;location&quot;: &quot;WX4FBXXFKE4F&quot;, &quot;geolocation&quot;: &quot;enabled&quot;, &quot;language&quot;: &quot;auto&quot;, &quot;unit&quot;: &quot;c&quot;, &quot;theme&quot;: &quot;chameleon&quot;, &quot;container&quot;: &quot;tp-weather-widget&quot;, &quot;bubble&quot;: &quot;enabled&quot;, &quot;alarmType&quot;: &quot;badge&quot;, &quot;color&quot;: &quot;#C6C6C6&quot;, &quot;uid&quot;: &quot;你的公钥&quot;, &quot;hash&quot;: &quot;你的私钥&quot;&#125;);tpwidget(&quot;show&quot;);&lt;/script&gt;&lt;!-- 心知结束--&gt; 二，美化123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118/* * 文章一二三四级标题美化 */#post-content h1 &#123; font-size: 30px&#125;#post-content h2 &#123; position: relative; margin: 20px 0 32px!important; font-size: 1.55em;&#125;#post-content h3 &#123; font-size: 20px&#125;#post-content h4 &#123; font-size: 15px&#125;#post-content h2::after &#123; transition: all .35s; content: &quot;&quot;; position: absolute; background: linear-gradient(#3c67bd8c 30%,#3c67bd 70%); width: 1em; left: 0; box-shadow: 0 3px 3px rgba(32,160,255,.4); height: 3px; bottom: -8px;&#125;#post-content h2::before &#123; content: &quot;&quot;; width: 100%; border-bottom: 1px solid #eee; bottom: -7px; position: absolute&#125;#post-content h2:hover::after &#123; width: 2.5em;&#125;#post-content h1,#post-content h2,#post-content h3,#post-content h4,#post-content h5,#post-content h6 &#123; color: #666; line-height: 1.4; font-weight: 700; margin: 30px 0 10px 0&#125; /* * 首页文章列表悬停上浮 */.blog-post .panel:not(article) &#123; transition: all 0.3s;&#125;.blog-post .panel:not(article):hover &#123; transform: translateY(-10px); box-shadow: 0 8px 10px rgba(73, 90, 47, 0.47);&#125;/* * 文章内头图和文章图片悬停放大并将超出范围隐藏 */.entry-thumbnail &#123; overflow: hidden;&#125;#post-content img &#123; border-radius: 10px; transition: 0.5s;&#125;#post-content img:hover &#123; transform: scale(1.05);&#125;/* *首页文章图片获取焦点放大 */.item-thumb &#123; cursor: pointer; transition: all 0.6s;&#125;.item-thumb:hover &#123; transform: scale(1.05);&#125;.item-thumb-small &#123; cursor: pointer; transition: all 0.6s;&#125;.item-thumb-small:hover &#123; transform: scale(1.05);&#125;/*文章内打赏图标跳动*/.btn - pay &#123; animation: star 0.5s ease - in-out infinite alternate;&#125;@keyframes star &#123; from &#123; transform: scale(1); &#125; to &#123; transform: scale(1.1); &#125;&#125;/* *修改字体 */*&#123;font-family: &#x27;Noto Serif SC&#x27;, serif;font-family: &#x27;Fira Code&#x27;, monospace;&#125; 三，头像呼吸光环和鼠标悬停旋转放大1234567891011121314151617181920212223242526272829303132.img-full &#123; width: 100px; border-radius: 50%; animation: light 4s ease-in-out infinite; transition: 0.5s;&#125;.img-full:hover &#123; transform: scale(1.15) rotate(720deg);&#125;@keyframes light &#123; 0% &#123; box-shadow: 0 0 4px #f00; &#125; 25% &#123; box-shadow: 0 0 16px #0f0; &#125; 50% &#123; box-shadow: 0 0 4px #00f; &#125; 75% &#123; box-shadow: 0 0 16px #0f0; &#125; 100% &#123; box-shadow: 0 0 4px #f00; &#125;&#125; 四，qq头像的图片链接1https://q1.qlogo.cn/g?b=qq&amp;nk=572245955&amp;s=640 五，自定义js美化1234567/*彩色标签云*/let tags = document.querySelectorAll(&quot;#tag_cloud-2 a&quot;);let colorArr = [&quot;#428BCA&quot;, &quot;#AEDCAE&quot;, &quot;#ECA9A7&quot;, &quot;#DA99FF&quot;, &quot;#FFB380&quot;, &quot;#D9B999&quot;];tags.forEach(tag =&gt; &#123; tagsColor = colorArr[Math.floor(Math.random() * colorArr.length)]; tag.style.backgroundColor = tagsColor;&#125;); 六，博主介绍特效12&lt;!--博主介绍的闪字特效--&gt;&lt;span class=&quot;text-muted text-xs block&quot;&gt;&lt;div id=&quot;chakhsu&quot;&gt;&lt;/div&gt; &lt;script&gt; var chakhsu = function (r) &#123;function t() &#123;return b[Math.floor(Math.random() * b.length)]&#125; function e() &#123;return String.fromCharCode(94 * Math.random() + 33)&#125; function n(r) &#123;for (var n = document.createDocumentFragment(), i = 0; r &gt; i; i++) &#123; var l = document.createElement(&quot;span&quot;); l.textContent = e(), l.style.color = t(), n.appendChild(l) &#125; return n&#125;function i() &#123;var t = o[c.skillI]; c.step ? c.step-- : (c.step = g, c.prefixP &lt; l.length ? (c.prefixP &gt;= 0 &amp;&amp; (c.text += l[c.prefixP]), c.prefixP++) : &quot;forward&quot; === c.direction ? c.skillP &lt; t.length ? (c.text += t[c.skillP], c.skillP++) : c.delay ? c.delay-- : (c.direction = &quot;backward&quot;, c.delay = a) : c.skillP &gt; 0 ? (c.text = c.text.slice(0, -1), c.skillP--) : (c.skillI = (c.skillI + 1) % o.length, c.direction = &quot;forward&quot;)), r.textContent = c.text, r.appendChild(n(c.prefixP &lt; l.length ? Math.min(s, s + c.prefixP) : Math.min(s, t.length - c.skillP))), setTimeout(i, d) &#125; /*以下内容自定义修改*/ var l = &quot;&quot;, o = [&quot;Keep Fighting&quot; ].map(function (r) &#123;return r + &quot;&quot;&#125;), a = 2, g = 1, s = 5, d = 75, b = [&quot;rgb(110,64,170)&quot;, &quot;rgb(150,61,179)&quot;, &quot;rgb(191,60,175)&quot;, &quot;rgb(228,65,157)&quot;, &quot;rgb(254,75,131)&quot;, &quot;rgb(255,94,99)&quot;, &quot;rgb(255,120,71)&quot;, &quot;rgb(251,150,51)&quot;, &quot;rgb(226,183,47)&quot;, &quot;rgb(198,214,60)&quot;, &quot;rgb(175,240,91)&quot;, &quot;rgb(127,246,88)&quot;, &quot;rgb(82,246,103)&quot;, &quot;rgb(48,239,130)&quot;, &quot;rgb(29,223,163)&quot;, &quot;rgb(26,199,194)&quot;, &quot;rgb(35,171,216)&quot;, &quot;rgb(54,140,225)&quot;, &quot;rgb(76,110,219)&quot;, &quot;rgb(96,84,200)&quot;], c = &#123;text: &quot;&quot;, prefixP: -s, skillI: 0, skillP: 0, direction: &quot;forward&quot;, delay: a, step: g&#125;; i() &#125;; chakhsu(document.getElementById(&#x27;chakhsu&#x27;)); &lt;/script&gt; &lt;/span&gt; &lt;/span&gt; 七，倒计时12&lt;style&gt; .gn_box&#123; border: none; border-radius: 15px; &#125; .gn_box &#123; padding: 10px 14px; margin: 10px; margin-bottom: 20px; text-align: center; background-color: #fff; &#125; #t_d&#123; color: #982585; font-size: 18px; &#125; #t_h&#123; color: #8f79c1; font-size: 18px; &#125; #t_m&#123; color: #65b4b5; font-size: 18px; &#125; #t_s&#123; color: #83caa3; font-size: 18px; &#125; &lt;/style&gt; &lt;div class=&quot;gn_box&quot;&gt; &lt;h1 style=&quot;font-size:1em;&quot;&gt;&lt;font color=&quot;#E80017&quot;&gt;2&lt;/font&gt; &lt;font color=&quot;#D1002E&quot;&gt;0&lt;/font&gt; &lt;font color=&quot;#BA0045&quot;&gt;2&lt;/font&gt; &lt;font color=&quot;#A3005C&quot;&gt;0&lt;/font&gt; &lt;font color=&quot;#8C0073&quot;&gt;年&lt;/font&gt; &lt;font color=&quot;#75008A&quot;&gt;-&lt;/font&gt; &lt;font color=&quot;#5E00A1&quot;&gt;秋&lt;/font&gt; &lt;font color=&quot;#4700B8&quot;&gt;招&lt;/font&gt; &lt;font color=&quot;#3000CF&quot;&gt;倒&lt;/font&gt; &lt;font color=&quot;#1900E6&quot;&gt;计&lt;/font&gt; &lt;font color=&quot;#0200FD&quot;&gt;时&lt;/font&gt;&lt;/h1&gt; &lt;center&gt; &lt;div id=&quot;CountMsg&quot; class=&quot;HotDate&quot;&gt; &lt;span id=&quot;t_d&quot;&gt; 天&lt;/span&gt; &lt;span id=&quot;t_h&quot;&gt; 时&lt;/span&gt;&lt;span id=&quot;t_m&quot;&gt; 分&lt;/span&gt; &lt;span id=&quot;t_s&quot;&gt; 秒&lt;/span&gt; &lt;/div&gt; &lt;/center&gt; &lt;script type=&quot;text/javascript&quot;&gt; function getRTime() &#123; var EndTime = new Date(&#x27;2020/09/1 00:00:00&#x27;); var NowTime = new Date(); var t = EndTime.getTime() - NowTime.getTime(); var d = Math.floor(t / 1000 / 60 / 60 / 24); var h = Math.floor(t / 1000 / 60 / 60 % 24); var m = Math.floor(t / 1000 / 60 % 60); var s = Math.floor(t / 1000 % 60); var day = document.getElementById(&quot;t_d&quot;); if (day != null) &#123; day.innerHTML = d + &quot; 天&quot;; &#125; var hour = document.getElementById(&quot;t_h&quot;); if (hour != null) &#123; hour.innerHTML = h + &quot; 时&quot;; &#125; var min = document.getElementById(&quot;t_m&quot;); if (min != null) &#123; min.innerHTML = m + &quot; 分&quot;; &#125; var sec = document.getElementById(&quot;t_s&quot;); if (sec != null) &#123; sec.innerHTML = s + &quot; 秒&quot;; &#125; &#125; setInterval(getRTime, 1000); &lt;/script&gt; &lt;/div&gt; &lt;!--首页输出文章--&gt; 八，疫情图1&lt;iframe src=&quot;https://www.lovestu.com/api/project/cnmapyinqing/obj.php&quot; height=&quot;500&quot; frameborder=&quot;no&quot; border=&quot;0&quot; width=&quot;100%&quot;&gt; &lt;/iframe&gt; 参考链接：https://wanghongfeng.cn/handsome-diy.html https://rehtt.com/index.php/archives/193","categories":[{"name":"typecho","slug":"typecho","permalink":"http://kiedeng.github.io/categories/typecho/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"ArrayList源码分析","slug":"ArrayList源码分析","date":"2020-04-20T04:22:21.000Z","updated":"2020-04-20T04:22:21.542Z","comments":true,"path":"2020/04/20/ArrayList源码分析/","link":"","permalink":"http://kiedeng.github.io/2020/04/20/ArrayList%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"","categories":[{"name":"未分类","slug":"未分类","permalink":"http://kiedeng.github.io/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"hql常见函数","slug":"hql常见函数","date":"2020-03-19T07:27:35.000Z","updated":"2020-03-19T07:28:42.883Z","comments":true,"path":"2020/03/19/hql常见函数/","link":"","permalink":"http://kiedeng.github.io/2020/03/19/hql%E5%B8%B8%E8%A7%81%E5%87%BD%E6%95%B0/","excerpt":"","text":"常用日期函数 unix_timestamp:返回当前或指定时间的时间戳 from_unixtime：将时间戳转为日期格式 current_date：当前日期 current_timestamp：当前的日期加时间 to_date：抽取日期部分 year：获取年 month：获取月 day：获取日 hour：获取时 minute：获取分 second：获取秒 weekofyear：当前时间是一年中的第几周 dayofmonth：当前时间是一个月中的第几天 months_between： 两个日期间的月份 add_months：日期加减月 datediff：两个日期相差的天数 date_add：日期加天数 date_sub：日期减天数 last_day：日期的当月的最后一天 常用取整函数 round： 四舍五入 ceil： 向上取整 floor： 向下取整 常用字符串操作函数 upper： 转大写 lower： 转小写 length： 长度 trim： 前后去空格 lpad： 向左补齐，到指定长度 rpad： 向右补齐，到指定长度 regexp_replace： SELECT regexp_replace(‘100-200’, ‘(\\d+)’, ‘num’) ； ​ 使用正则表达式匹配目标字符串，匹配成功后替换！ 集合操作 size： 集合中元素的个数 map_keys： 返回map中的key map_values: 返回map中的value array_contains: 判断array中是否包含某个元素 sort_array： 将array中的元素排序","categories":[{"name":"未分类","slug":"未分类","permalink":"http://kiedeng.github.io/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"HQL常见错误","slug":"HQL常见错误","date":"2020-03-18T14:03:47.000Z","updated":"2020-03-18T15:13:59.298Z","comments":true,"path":"2020/03/18/HQL常见错误/","link":"","permalink":"http://kiedeng.github.io/2020/03/18/HQL%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/","excerpt":"","text":"行转列过程中分组的时候只写了group，忘记了by 子查询不能写分号 列名错误：1Error: Error while compiling statement: FAILED: SemanticException [Error 10004]: Line 3:6 Invalid table alias or column reference &#x27;orderdata&#x27;: (possible column names are: name, orderdate, cost) (state=42000,code=10004)","categories":[{"name":"hive","slug":"hive","permalink":"http://kiedeng.github.io/categories/hive/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"简介","slug":"简介","date":"2020-03-18T14:03:31.000Z","updated":"2021-03-05T14:09:02.462Z","comments":true,"path":"2020/03/18/简介/","link":"","permalink":"http://kiedeng.github.io/2020/03/18/%E7%AE%80%E4%BB%8B/","excerpt":"","text":"教育2017.09.09 - 2021.06.06 郑州轻工业大学 本科 计算机科学与技术 ACM集训队成员 工作数据的离线数据处理和实时处理 hadoop, spark,linux, scala 附录github：github QQ:572245955","categories":[{"name":"成长","slug":"成长","permalink":"http://kiedeng.github.io/categories/%E6%88%90%E9%95%BF/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"mongdb导入json文件","slug":"mongdb导入json文件","date":"2020-03-18T04:22:00.000Z","updated":"2020-03-18T04:29:16.461Z","comments":true,"path":"2020/03/18/mongdb导入json文件/","link":"","permalink":"http://kiedeng.github.io/2020/03/18/mongdb%E5%AF%BC%E5%85%A5json%E6%96%87%E4%BB%B6/","excerpt":"","text":"1 安装mongdb使用宝塔安装mongdb 2 配置mongdb 注：开放27017端口 3 导入数据mongoimport –db mall –collection userData –file /Users/yeo/Desktop/userdata.json 注：执行命令在/www/server/mongodb/bin","categories":[{"name":"mongdb","slug":"mongdb","permalink":"http://kiedeng.github.io/categories/mongdb/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"hive使用mysql编码问题","slug":"hive使用mysql编码问题","date":"2020-03-17T10:58:22.000Z","updated":"2020-03-17T11:07:52.618Z","comments":true,"path":"2020/03/17/hive使用mysql编码问题/","link":"","permalink":"http://kiedeng.github.io/2020/03/17/hive%E4%BD%BF%E7%94%A8mysql%E7%BC%96%E7%A0%81%E9%97%AE%E9%A2%98/","excerpt":"","text":"修改my.cnf文件123456# 添加init_connect=&#x27;SET collation_connection = utf8_unicode_ci&#x27;init_connect=&#x27;SET NAMES utf8&#x27;character-set-server=utf8collation-server=utf8_unicode_ciskip-character-set-client-handshake 重启mysql 启动使用bin/hive脚本 修改metastore数据库12show variables like &#x27;char%&#x27;;show variables like &quot;colla%&quot;; 使用上面两个命令，查看是否都为utf8的编码，（除character_set _filesystem为binary） 如果没有：则 12345678修改表字段注解和表注解alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8;alter table TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;修改分区字段注解：alter table PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;alter table PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set utf8;修改索引注解：alter table INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8; 这样就可以显示中文字符的，不过在hive中插入中文字符还有些问题，只能显示上传文档为中文字符和描述为中文字符的情况","categories":[{"name":"hive","slug":"hive","permalink":"http://kiedeng.github.io/categories/hive/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"mysql安装","slug":"mysql安装","date":"2020-03-16T10:35:24.000Z","updated":"2020-03-16T11:32:24.560Z","comments":true,"path":"2020/03/16/mysql安装/","link":"","permalink":"http://kiedeng.github.io/2020/03/16/mysql%E5%AE%89%E8%A3%85/","excerpt":"","text":"检查是否安装Mysql12rpm -qa|grep mysql; // 查询是否存在mysqlrpm -e --nodeps mysql-libs; // 卸载mysql rpm 是一个包管理工具 -e 卸载程序 -qa 查询安装的软件 –nodeps 不验证软件包的依赖 -i ,–install 安装软件包 -v， –verbose 提供更多的详细信息输出 -h ，–hash 软件包安装的时候列出哈希标记 安装Mysql123456tar -xf mysql-5.7.28-1.el6.x86_64.rpm-bundle.tar // 解压// 安装[root@hadoop102 software]$ rpm -ivh mysql-community-common-5.7.28-1.el6.x86_64.rpm[root@hadoop102 software]$ rpm -ivh mysql-community-libs-5.7.28-1.el6.x86_64.rpm[root@hadoop102 software]$ rpm -ivh mysql-community-client-5.7.28-1.el6.x86_64.rpm[root@hadoop102 software]$ rpm -ivh mysql-community-server-5.7.28-1.el6.x86_64.rpm 修改/etc/my.cnf12[mysqld]explicit_defaults_for_timestamp=true //显示指定默认值为timestamp类型的字段 删除/etc/my.cnf文件中datadir指向的目录下的所有内容: 启动Mysql1234567891011121314// 初始化mysqld --initialize --user=mysql// 查看临时密码cat /var/log/mysqld.log // 启动mysql服务service mysqld start// 登陆mysql mysql -uroot -pEnter password:// 修改密码set password = password(&quot;新密码&quot;)// 修改root用户支持任意IP连接（在user表中）update user set host= ‘%’ where user = ‘root’;flush privileges ; // 刷新配置","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kiedeng.github.io/categories/Linux/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"大数据相关单词","slug":"大数据相关单词","date":"2020-03-16T10:21:50.000Z","updated":"2020-03-16T16:37:30.491Z","comments":true,"path":"2020/03/16/大数据相关单词/","link":"","permalink":"http://kiedeng.github.io/2020/03/16/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3%E5%8D%95%E8%AF%8D/","excerpt":"","text":"单词 翻译 row format delimited fields terminated by “\\t” 行格式分隔的字段，以“ \\ t”结尾 external 外部 comment 评论 partitioned by 分区（分目录） sorted by 排序 clustered by 分桶（分文件） serialize 序列化 deserialize 反序列化 create table test( ​ )","categories":[{"name":"hive","slug":"hive","permalink":"http://kiedeng.github.io/categories/hive/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"Hadoop_HA高可用","slug":"Hadoop-HA高可用","date":"2020-03-08T05:52:37.000Z","updated":"2020-03-08T09:19:37.481Z","comments":true,"path":"2020/03/08/Hadoop-HA高可用/","link":"","permalink":"http://kiedeng.github.io/2020/03/08/Hadoop-HA%E9%AB%98%E5%8F%AF%E7%94%A8/","excerpt":"","text":"包括HDFS的HA和YARN的HA HDFS-HA的工作机制​ 通过双NameNode消除单节点故障 手动故障转移配置：在opt目录下创建ha文件夹，将hadoop复制到ha文件夹下 配置hadoop-env.sh 1export JAVA_HOME=/opt/module/jdk1.8.0_144 配置core-site.xml 1234567891011121314151617181920&lt;configuration&gt;&lt;!-- 把两个NameNode）的地址组装成一个集群mycluster --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://mycluster&lt;/value&gt; &lt;/property&gt; &lt;!-- 声明journalnode服务器存储目录--&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/opt/ha/hadoop-2.7.2/data/jn&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/ha/hadoop-2.7.2/data/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置hdfs-site.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&lt;configuration&gt; &lt;!-- 完全分布式集群名称 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;mycluster&lt;/value&gt; &lt;/property&gt; &lt;!-- 集群中NameNode节点都有哪些 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;hadoop102:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;hadoop103:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;hadoop102:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;hadoop103:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定NameNode元数据在JournalNode上的存放位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://hadoop102:8485;hadoop103:8485;hadoop104:8485/mycluster&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;!-- 使用隔离机制时需要ssh无秘钥登录--&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/atguigu/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;!-- 关闭权限检查--&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.enable&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;!-- 访问代理类：client，mycluster，active配置失败自动切换实现方式--&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 拷贝到其他节点 启动集群 各个集群启动JournalNode节点 1sbin/hadoop-daemon.sh start journalnode 在[nn1]上，对其格式化，并启动 12bin/hdfs namenode -formatsbin/hadoop-daemon.sh start namenode 在[nn2]上，同步nn1的元数据信息 1bin/hdfs namenode -bootstrapStandby 启动[nn2] 1sbin/hadoop-daemon.sh start namenode 将[nn1]切换为Active 1bin/hdfs haadmin -transitionToActive nn1 启动datanode 1sbin/hadoop-daemons.sh start datanode 查看是否Active 1bin/hdfs haadmin -getServiceState nn1 自动故障转移具体配置 （1）在hdfs-site.xml中增加 1234&lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; （2）在core-site.xml文件中增加 1234&lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop102:2181,hadoop103:2181,hadoop104:2181&lt;/value&gt;&lt;/property&gt; 启动 （1）关闭所有HDFS服务：sbin/stop-dfs.sh （2）启动Zookeeper集群：bin/zkServer.sh start （3）初始化HA在Zookeeper中状态：bin/hdfs zkfc -formatZK （4）启动HDFS服务：sbin/start-dfs.sh 集群规划： hadoop102 hadoop103 hadoop104 NameNode NameNode ZKFC ZKFC JournalNode JournalNode JournalNode DataNode DataNode DataNode ZK ZK ZK ResourceManager NodeManager NodeManager NodeManager YARN-HA配置yarn-site.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!--启用resourcemanager ha--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--声明两台resourcemanager的地址--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;cluster-yarn1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;hadoop102&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;hadoop103&lt;/value&gt; &lt;/property&gt; &lt;!--指定zookeeper集群的地址--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;hadoop102:2181,hadoop103:2181,hadoop104:2181&lt;/value&gt; &lt;/property&gt; &lt;!--启用自动恢复--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--指定resourcemanager的状态信息存储在zookeeper集群--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 启动YARN （1）在hadoop102中执行：sbin/start-yarn.sh （2）在hadoop103中执行：sbin/yarn-daemon.sh start resourcemanager","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://kiedeng.github.io/categories/Hadoop/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"shell脚本文件","slug":"shell脚本文件","date":"2020-03-07T16:39:20.000Z","updated":"2020-03-07T16:41:48.903Z","comments":true,"path":"2020/03/08/shell脚本文件/","link":"","permalink":"http://kiedeng.github.io/2020/03/08/shell%E8%84%9A%E6%9C%AC%E6%96%87%E4%BB%B6/","excerpt":"","text":"myjps1234567#!/bin/bashfor i in hadoop102 hadoop103 hadoop104do echo &quot;************* $i Jps ***********&quot; ssh $i /opt/module/jdk1.8.0_144/bin/jpsdone zookeeper集群管理脚本1234567891011121314151617181920212223242526#!/bin/bashif [ $# -eq 0 ]then echo &quot;No args Input....&quot;fifor i in hadoop102 hadoop103 hadoop104do case $1 in &quot;start&quot;) echo &quot;*****************Start $i Zookeeper *************&quot; ssh $i /opt/module/zookeeper-3.4.10/bin/zkServer.sh start ;; &quot;stop&quot;) echo &quot;*****************Stop $i Zookeeper *************&quot; ssh $i /opt/module/zookeeper-3.4.10/bin/zkServer.sh stop ;; &quot;status&quot;) echo &quot;*****************Status $i Zookeeper *************&quot; ssh $i /opt/module/zookeeper-3.4.10/bin/zkServer.sh status ;; *) echo &quot;Input Args Error......&quot; esacdone 文件同步脚本12345678910111213141516171819202122232425#!/bin/bash#1 获取输入参数个数，如果没有参数，直接退出pcount=$#if((pcount==0)); thenecho no args;exit;fi#2 获取文件名称p1=$1fname=`basename $p1`echo fname=$fname#3 获取上级目录到绝对路径pdir=`cd -P $(dirname $p1); pwd`echo pdir=$pdir#4 获取当前用户名称user=`whoami`#5 循环for((host=103; host&lt;105; host++)); do echo ------------------- hadoop$host -------------- rsync -rvl $pdir/$fname $user@hadoop$host:$pdirdone","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kiedeng.github.io/categories/Linux/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"shell操作","slug":"shell操作","date":"2020-03-07T09:28:34.000Z","updated":"2020-03-08T05:39:36.259Z","comments":true,"path":"2020/03/07/shell操作/","link":"","permalink":"http://kiedeng.github.io/2020/03/07/shell%E6%93%8D%E4%BD%9C/","excerpt":"","text":"更改权限：chmod 777 [sh文件] 案例案例：创建文件，写入数据 1234#!/bin/bashcd /home/atguigu/zzulitouch chuang.txtecho &quot;kangdong&quot; &gt;&gt; chuang.txt Shell中的变量系统变量1$HOME,$PWD,$SHELL,$USER等 全局变量：export A 自定义变量撤销变量：unset 变量 声明静态变量：readonly变量，注意：不能unset 特殊变量:$ n$n （功能描述：n为数字，$0代表该脚本名称，$1-$9代表第一到第九个参数，十以上的参数，十以上的参数需要用大括号包含，如${10}） 特殊变量：$获取所有输入参数个数，常用于循环 特殊变量：$ *、$ @​ $ * （功能描述：这个变量代表命令行中所有的参数，$*把所有的参数看成一个整体） ​ $ @ （功能描述：这个变量也代表命令行中所有的参数，不过$@把每个参数区分对待） 特殊变量：$ ?判断最后一次执行的命令的返回状态，0位成功，非0为失败 运算符 “$ ((运算式))”或“$[运算式]” expr + , - , *, /, % 加，减，乘，除，取余 注意：expr运算符间要有空格 条件判断格式：[condition] 判断条件： 两个整数比较 -lt 小于（less than） -le 小于或等于（less equal） -eq等于（equal） -gt大于（greater than） -ge大于等于（greater equal） -ne不等于（Not equal） 按照文件权限进行判断 -r 读 -w写 -x 执行（execute） 按照文件类型进行判断 -f 文件存在并且是一个常规文件 -e 文件存在 -d为目录 流程控制1 if判断12345678#!/bin/bashif [ $1 -eq &quot;2&quot; ]then echo &quot;2&quot;elif [ $1 -eq &quot;3&quot; ]then echo &quot;3&quot;fi 2 case语句注意事项： case行尾必须为单词“in”，每一个模式匹配必须以右括号“）”结束。 双分号“**;;**”表示命令序列结束，相当于java中的break。 最后的“*）”表示默认模式，相当于java中的default。 案例： 123456789101112131415#!/bin/bashcase $1 in&quot;1&quot;) echo &quot;1 kangdong&quot;;;&quot;2&quot;) echo &quot;2 kangdong&quot;;;&quot;3&quot;) echo &quot;3 kangdong&quot;;;*) echo &quot;* kangdong&quot;;;esac 3 for循环1234567#/bin/bashs=0for((i=0;i&lt;=100;i++))do s=$[$s+$i]doneecho $s 4 比较$ *与$ @未被双引号包含时，都分开输出，当被双引号包含时，“$*”会将所有参数作为一个整体输出 123456789101112#!/bin/bashfor i in &quot;$*&quot;do echo &quot;I am $i&quot;donefor i in &quot;$@&quot;do echo &quot;My name $i&quot;done 5 while循环12345678910#!/bin/bashs=0i=0while [ $i -le 100 ]do s=$[$i+$s] i=$[$i+1]doneecho $s read读取控制台输入read(选项)(参数) ​ 选项： -p：指定读取值时的提示符； -t：指定读取值时等待的时间（秒）。 参数 ​ 变量：指定读取值的变量名 12read -t 9 -p &quot;输入值:&quot; nameecho $name 函数1 系统函数basename [string / pathname] [suffix] 求文件名 dirname ：求文件的目录 2 自定义函数12345678910111213#!/bin/bashfunction sum()&#123; s=0 s=$[$1+$2] echo $s&#125;read -p &quot;输入s1：&quot; n1;read -p &quot;输入s2: &quot; n2;sum $n1 $n2 Shell工具cutcut [选项参数] filename -f 列号 -d 分隔符 sedawk一个强大的文本分析工具，把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分再进行分析处理。 sort企业真实面试题（重点）《未完，，待续》","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kiedeng.github.io/categories/Linux/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"Zookeeper","slug":"Zookeeper","date":"2020-03-04T05:09:50.000Z","updated":"2020-03-08T05:30:50.554Z","comments":true,"path":"2020/03/04/Zookeeper/","link":"","permalink":"http://kiedeng.github.io/2020/03/04/Zookeeper/","excerpt":"","text":"​ Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目。多作为集群提供服务的中间件。 ​ Zookeeper从设计模式角度来理解，是一个基于观察者模式设计的分布式服务管理框架。 应用场景​ 提供的服务包括：统一命名服务，统一配置管理，统一集群管理，服务器节点动态上下线，软负载均衡等。 1 统一命名服务​ 在分布式环境下，经常需要对应用/服务进行统一命名，便于识别。 2 统一配置管理​ 1）分布式环境下，配置文件同步非常常见 ​ 2）配置管理可交由ZooKeeper实现 3 统一集群管理​ 1）分布式环境下，实时掌握节点的状态是必要的 ​ 2）ZooKeeper可以实现实时监控节点状态变化 4 服务器动态上下线​ 客户端能实时洞察服务器上下线的变化 5 软负载均衡​ 在Zookeeper中记录每台服务器的访问数，让访问数最少的服务器去处理最新的客户端请求 Zookeeper安装本地模式安装部署1 安装jdk，拷贝Zookeeper，解压到指定目录1[atguigu@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/ 2 配置修改 将conf路径下的zoo_sample.cfg修改为zoo.cfg 修改dataDir路径，改为：dataDir=/opt/module/zookeeper-3.4.10/zkData 创建zkDataa文件夹 3 操作Zookeeper 启动：bin/zkServer.sh start 查看进程是否启动：jps 查看状态：bin/zkServer.sh status 启动客户端：bin/zkCli.sh 退出：quit 停止：bin/zkServer.sh stop 四字命令 ruok 测试服务是否处于正确状态，如果确实如此，那么服务返回 imok ,否则不做任何响应。 conf 3.3.0版本引入的，打印出服务相关配置的详细信息 cons 列出所有连接到这台服务器的客户端全部会话详细信息。包括 接收/发送的包数量，会话id，操作延迟、最后的操作执行等等信息 crst 重置所有连接的连接和会话统计信息 dump 列出那些比较重要的会话和临时节点。这个命令只能在leader节点上有用 envi 打印出服务环境的详细信息 命令格式：nc localhost 2181 注: 使用之前，需要先安装nc，可以使用yum方式进行安装. 配置参数解读zoo.cfg参数含义： tickTime：通信心跳数，Zookeeper服务器与客户端心跳时间，单位 initLimit=10:LF初始通信时限（第一次连接的超时时间） syncLimit=5：LF同步通信时限（最大响应时间单位） clientPort=2181：客户端连接端口 Zookeeper内容原理选举机制半数机制：集群中半数以上机器存活，集群可用。 Zookeeper虽然在配置中并没有指定Master和Slave。但是，Zookeeper工作时，是有一个节点为Leader，其他则为Follower，Leader是通过内部的选举机制临时产生的。 节点类型持久：客户端和服务器断开连接后，创建的节点不删除 短暂：客户端和服务器断开连接后，创建的节点自己删除 监听器原理监听器原理详解： 首先要有一个main（）线程 创建Zookeeper客户端，两个线程，一个负责网络连接通信（connect），一个负责监听（listener） 通过connect线程将注册的监听事件发送给Zookeeper。 在Zookeeper的注册监听器列表将注册的监听事件添加到列表中 监听数据或路径变化，就会将这个消息发送给listener线程 listener线程内部调用了process（）方法 写数据流程 Client发送一个写请求 如果Server不是Leader，在转给Leader，请求广播给各个Server 当Leader收到大多数Server数据写成功了，就说明数据写成功了 server通知Client数据写成功了 Zookeeper实战1 集群规划在hadoop102,103,104三个节点上部署Zookeeper 2 解压安装（1）解压到/opt/module目录下 （2）同步到hadoop103,104 3 配置服务器编号（1）在zookeeper目录下创建zkData （2）在zkData目录下创建一个myid的文件 （3）编写myid文件，添加编号 （4）拷贝配置好的zookeeper到其他机器上 4 配置zoo.cfg文件（1）重命名zoo_sample.cfg为zoo.cfg （2）打开zoo.cfg文件 修改数据存储路径配置，添加配置 1234#######################cluster##########################server.2=hadoop102:2888:3888server.3=hadoop103:2888:3888server.4=hadoop104:2888:3888 （3）同步zoo.cfg配置文件 （4）配置参数解读： server.A=B:C:D A是一个数字，代表这个是几号服务器 B是这个服务器的ip地址 C是这个服务器与集群中的Leader服务器交换信息的端口 D是集群服务器挂了，此端口执行服务器相互通信端口 5 集群操作（1）分别启动Zookeeper：bin/zkServer.sh start （2）查看状态：bin/zkServer.sh status 客户端命令行操作 命令基本语法 功能描述 help 显示所有操作命令 ls path [watch] 使用 ls 命令来查看当前znode中所包含的内容 ls2 path [watch] 查看当前节点数据并能看到更新次数等数据 create 普通创建 -s 含有序列 -e 临时（重启或者超时消失） get path [watch] 获得节点的值 set 设置节点的具体值 stat 查看节点状态 delete 删除节点 rmr 递归删除节点 启动客户端：bin/zkCli.sh 创短暂节点：create -e /sanguo “zhouyu” 创建带序号的节点：create -s /sanguo “kang” 监听节点数据：get /sang watch 监听节点数目：ls /sanguo watch 删除节点：delete rmr /san 查看节点状态：stat /san API应用pom文件 123456789101112131415161718&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.zookeeper/zookeeper --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.10&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; log4j.properties文件 需要在项目的src/main/resources目录下，新建一个文件，命名为“log4j.properties”，在文件中填入 12345678log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n log4j.appender.logfile=org.apache.log4j.FileAppender log4j.appender.logfile.File=target/spring.log log4j.appender.logfile.layout=org.apache.log4j.PatternLayout log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 创建Zookeeper客户端12345678910111213141516171819202122232425private static String connectString = &quot;hadoop102:2181,hadoop103:2181,hadoop104:2181&quot;; private static int sessionTimeout = 2000; private ZooKeeper zkClient = null; @Before public void init() throws Exception &#123; zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; // 收到事件通知后的回调函数（用户的业务逻辑） System.out.println(event.getType() + &quot;--&quot; + event.getPath()); // 再次启动监听 try &#123; zkClient.getChildren(&quot;/&quot;, true); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; 创建子节点1234567// 创建子节点@Testpublic void create() throws Exception &#123; // 参数1：要创建的节点的路径； 参数2：节点数据 ； 参数3：节点权限 ；参数4：节点的类型 String nodeCreated = zkClient.create(&quot;/atguigu&quot;, &quot;jinlian&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);&#125; 获取子节点并监听节点变化12345678910111213// 获取子节点@Testpublic void getChildren() throws Exception &#123; List&lt;String&gt; children = zkClient.getChildren(&quot;/&quot;, true); for (String child : children) &#123; System.out.println(child); &#125; // 延时阻塞 Thread.sleep(Long.MAX_VALUE);&#125; 判断Znode是否存在123456789// 判断znode是否存在@Testpublic void exist() throws Exception &#123; Stat stat = zkClient.exists(&quot;/eclipse&quot;, false); System.out.println(stat == null ? &quot;not exist&quot; : &quot;exist&quot;);&#125; 监听服务器节点动态上下线案例需求：某分布式系统中，主节点可以有多台，可以动态上下线，任意一台客户端都能实时感知到主节点服务器的上下线。 先在集群上创建/servers节点 12[zk: localhost:2181(CONNECTED) 10] create /servers &quot;servers&quot;Created /servers 服务器端注册代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.atguigu.zookeeper1;import java.io.IOException;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooDefs.Ids;import org.apache.zookeeper.ZooKeeper;public class DistributeServer &#123; public static void main(String[] args) throws Exception &#123; DistributeServer server = new DistributeServer(); // 1 连接zookeepeer集群 server.getConnect(); // 2 注册节点 server.regist(args[0]); // 3 业务逻辑处理 server.business(); &#125; private void business() throws InterruptedException &#123; Thread.sleep(Long.MAX_VALUE); &#125; private void regist(String hostname) throws KeeperException, InterruptedException &#123; String path = zkClient.create(&quot;/servers/server&quot;, hostname.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); System.out.println(hostname +&quot;is online!!&quot;); &#125; private String connectString=&quot;hadoop102:2181,hadoop103:2181,hadoop104:2181&quot;; private int sessionTimeout=2000; private ZooKeeper zkClient; private void getConnect() throws IOException &#123; zkClient=new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; public void process(WatchedEvent arg0) &#123; &#125; &#125;); &#125;&#125; 客户端代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374package com.atguigu.zookeeper1;import java.io.IOException;import java.util.ArrayList;import java.util.List;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;public class DistributeClient &#123; public static void main(String[] args) throws Exception &#123; // 1 获取zookeeper集群连接 DistributeClient client = new DistributeClient(); client.getConnect(); // 2 注册监听 client.getChlidren(); // 3 业务逻辑处理 client.business(); &#125; private void business() throws InterruptedException &#123; Thread.sleep(Long.MAX_VALUE); &#125; private void getChlidren() throws KeeperException, InterruptedException &#123; List&lt;String&gt; children = zkClient.getChildren(&quot;/servers&quot;, true); ArrayList&lt;String&gt; hosts = new ArrayList&lt;String&gt;(); for (String child : children) &#123; byte[] data = zkClient.getData(&quot;/servers/&quot;+child, false, null); hosts.add(new String(data)); &#125; System.out.println(hosts); &#125; private String connectString=&quot;hadoop102:2181,hadoop103:2181,hadoop104:2181&quot;; private int sessionTimeout=2000; private ZooKeeper zkClient; private void getConnect() &#123; try &#123; zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; public void process(WatchedEvent event) &#123; try &#123; getChlidren(); &#125; catch (KeeperException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125;); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;&#125;","categories":[{"name":"Zookeeper","slug":"Zookeeper","permalink":"http://kiedeng.github.io/categories/Zookeeper/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"数据压缩","slug":"数据压缩","date":"2020-03-01T04:39:39.000Z","updated":"2020-03-01T09:04:22.605Z","comments":true,"path":"2020/03/01/数据压缩/","link":"","permalink":"http://kiedeng.github.io/2020/03/01/%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9/","excerpt":"","text":"1 概念：​ 压缩技术能够有效减少底层存储系统读写字节数。在运行MR程序是，I/O操作，网络数据传输，Shuffle和Merge要花大量的时间 ​ 压缩是提高Hadoop运行效率的一种优化策略 ​ 注：运行密集型的job，少用压缩；IO密集型的job，多用压缩 2 MR支持的压缩编码 压缩格式 hadoop自带？ 算法 文件扩展名 是否可切分 换成压缩格式后，原来的程序是否需要修改 DEFLATE 是，直接使用 DEFLATE .deflate 否 和文本处理一样，不需要修改 Gzip 是，直接使用 DEFLATE .gz 否 和文本处理一样，不需要修改 bzip2 是，直接使用 bzip2 .bz2 是 和文本处理一样，不需要修改 LZO 否，需要安装 LZO .lzo 是 需要建索引，还需要指定输入格式 Snappy 否，需要安装 Snappy .snappy 否 和文本处理一样，不需要修改 Hadoop引入了编码/解码器，如： 压缩格式 对应的编码/解码器 DEFLATE org.apache.hadoop.io.compress.DefaultCodec gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec LZO com.hadoop.compression.lzo.LzopCodec Snappy org.apache.hadoop.io.compress.SnappyCodec 3 压缩方式选择Gzip压缩优点：压缩率比较高，压缩速度比较快，本身支持，在应用中处理Gzip格式的文件和直接处理文本一样，大部分Linux系统自带； 缺点：不支持Split 应用场景：一个块大小内的数据，比如一天或者一个小时的日志信息 Bzip2压缩优点：支持Split，具有很高的压缩率，比Gzip压缩率要高，自带，使用方便 缺点：压缩/解压速度比较慢 应用场景：适合对速度要求不高，但需要很高压缩率的时候； Lzo压缩优点：压缩/解压速度比较快，合理的压缩率；支持Split,是Hadoop中最流行的压缩格式； 缺点：压缩率比Gzip低一些；需要安装，为了支持Split需要建索引，还需要指令InputFormat为Lzo格式 应用场景：一个很大的文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，Lzo优点越明显 Snappy压缩优点：高速压缩速度和合理的压缩率 缺点：不支持Split 应用场景：Map输出数据比较大的时候，作为Map到Reduce的中间数据的压缩格式；或者MapReduce的输出和另外一个MapReduce的输入 4 压缩位置选择1 输入端采用压缩 2 Mapper输出采用压缩 3 Reduce输出采用压缩 5 压缩参数配置 参数 默认值 阶段 建议 io.compression.codecs （在core-site.xml中配置） org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec 输入压缩 Hadoop使用文件扩展名判断是否支持某种编解码器 mapreduce.map.output.compress（在mapred-site.xml中配置） false mapper输出 这个参数设为true启用压缩 mapreduce.map.output.compress.codec（在mapred-site.xml中配置） org.apache.hadoop.io.compress.DefaultCodec mapper输出 企业多使用LZO或Snappy编解码器在此阶段压缩数据 mapreduce.output.fileoutputformat.compress（在mapred-site.xml中配置） false reducer输出 这个参数设为true启用压缩 mapreduce.output.fileoutputformat.compress.codec（在mapred-site.xml中配置） org.apache.hadoop.io.compress. DefaultCodec reducer输出 使用标准工具或者编解码器，如gzip和bzip2 mapreduce.output.fileoutputformat.compress.type（在mapred-site.xml中配置） RECORD reducer输出 SequenceFile输出使用的压缩类型：NONE和BLOCK 6 压缩案例压缩解压案例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package com.atguigu.mapreduce.compress;import java.io.File;import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.FileOutputStream;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.io.compress.CompressionCodec;import org.apache.hadoop.io.compress.CompressionCodecFactory;import org.apache.hadoop.io.compress.CompressionInputStream;import org.apache.hadoop.io.compress.CompressionOutputStream;import org.apache.hadoop.util.ReflectionUtils;public class TestCompress &#123; public static void main(String[] args) throws Exception &#123; compress(&quot;e:/hello.txt&quot;,&quot;org.apache.hadoop.io.compress.BZip2Codec&quot;);// decompress(&quot;e:/hello.txt.bz2&quot;); &#125; // 1、压缩 private static void compress(String filename, String method) throws Exception &#123; // （1）获取输入流 FileInputStream fis = new FileInputStream(new File(filename)); Class codecClass = Class.forName(method); CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, new Configuration()); // （2）获取输出流 FileOutputStream fos = new FileOutputStream(new File(filename + codec.getDefaultExtension())); CompressionOutputStream cos = codec.createOutputStream(fos); // （3）流的对拷 IOUtils.copyBytes(fis, cos, 1024*1024*5, false); // （4）关闭资源 cos.close(); fos.close();fis.close(); &#125; // 2、解压缩 private static void decompress(String filename) throws FileNotFoundException, IOException &#123; // （0）校验是否能解压缩 CompressionCodecFactory factory = new CompressionCodecFactory(new Configuration()); CompressionCodec codec = factory.getCodec(new Path(filename)); if (codec == null) &#123; System.out.println(&quot;cannot find codec for file &quot; + filename); return; &#125; // （1）获取输入流 CompressionInputStream cis = codec.createInputStream(new FileInputStream(new File(filename))); // （2）获取输出流 FileOutputStream fos = new FileOutputStream(new File(filename + &quot;.decoded&quot;)); // （3）流的对拷 IOUtils.copyBytes(cis, fos, 1024*1024*5, false); // （4）关闭资源 cis.close(); fos.close(); &#125;&#125; Map输出端采用压缩12345// 在Driver添加// 开启map端输出压缩configuration.setBoolean(&quot;mapreduce.map.output.compress&quot;, true);// 设置map端输出压缩方式configuration.setClass(&quot;mapreduce.map.output.compress.codec&quot;, BZip2Codec.class, CompressionCodec.class); Reduce输出采用压缩1234// 设置reduce端输出压缩开启FileOutputFormat.setCompressOutput(job, true);// 设置压缩的方式FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class);","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://kiedeng.github.io/categories/Hadoop/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"MapReduce框架原理","slug":"MapReduce框架原理","date":"2020-02-29T09:08:56.000Z","updated":"2020-02-29T13:57:55.988Z","comments":true,"path":"2020/02/29/MapReduce框架原理/","link":"","permalink":"http://kiedeng.github.io/2020/02/29/MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86/","excerpt":"","text":"总结MapReduce框架原理 1 InputFotmat数据输入待写 2 MapReduce工作流程待写 3 Shuffle机制3.1 Shuffle机制介绍Map方法之后，Reduce方法之前的数据称之为Shuffle 3.2 主要功能 Partition分区，WritableComparable排序，Combiner合并，GroupingComparator分组（辅助排序） 4 MapTask工作机制​ （1）Read阶段：MapTask通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value。 ​ （2）Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value。 ​ （3）Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner），并写入一个环形内存缓冲区中。 ​ （4）Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。 ​ 溢写阶段详情： ​ 步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号Partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。 ​ 步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。 ​ 步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中。 ​ （5）Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。 ​ 当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。 ​ 在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor（默认10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。 ​ 让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。 5 RedeceTask工作机制​ （1）Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。 ​ （2）Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。 ​ （3）Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。 ​ （4）Reduce阶段：reduce()函数将计算结果写到HDFS上。 6 OutputFormat数据输出​ OutputFormat是MapReduce输出的基类，所有实现MapReduce输出都实现了OutputFormat接口。 下面我们介绍几种常见的OutputFormat实现类。 ​ 1.文本输出TextOutputFormat默认的输出格式是TextOutputFormat，它把每条记录写为文本行。它的键和值可以是任意类型，因为TextOutputFormat调用toString0方法把它们转换为字符串。 ​ 2.SequenceFileOutputFormat将SequenceFileOutputFormat输出作为后续MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑，很容易被压缩。 ​ 3.自定义OutputFormat根据用户需求，自定义实现输出。 6.1 自定义OutputFomat步骤​ （1）自定义一个类继承FileOutputFormat。 1234567891011121314151617package com.atguigu.mapreduce.outputformat;import java.io.IOException;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.RecordWriter;import org.apache.hadoop.mapreduce.TaskAttemptContext;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class FilterOutputFormat extends FileOutputFormat&lt;Text, NullWritable&gt;&#123; @Override public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter(TaskAttemptContext job) throws IOException, InterruptedException &#123; // 创建一个RecordWriter return new FilterRecordWriter(job); &#125;&#125; ​ （2）改写RecordWriter，具体改写输出数据的方法write。 ​ RecordWriter格式： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.atguigu.mapreduce.outputformat;import java.io.IOException;import org.apache.hadoop.fs.FSDataOutputStream;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.RecordWriter;import org.apache.hadoop.mapreduce.TaskAttemptContext;public class FilterRecordWriter extends RecordWriter&lt;Text, NullWritable&gt; &#123; FSDataOutputStream atguiguOut = null; FSDataOutputStream otherOut = null; public FilterRecordWriter(TaskAttemptContext job) &#123; // 1 获取文件系统 FileSystem fs; try &#123; fs = FileSystem.get(job.getConfiguration()); // 2 创建输出文件路径 Path atguiguPath = new Path(&quot;e:/atguigu.log&quot;); Path otherPath = new Path(&quot;e:/other.log&quot;); // 3 创建输出流 atguiguOut = fs.create(atguiguPath); otherOut = fs.create(otherPath); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void write(Text key, NullWritable value) throws IOException, InterruptedException &#123; // 判断是否包含“atguigu”输出到不同文件 if (key.toString().contains(&quot;atguigu&quot;)) &#123; atguiguOut.write(key.toString().getBytes()); &#125; else &#123; otherOut.write(key.toString().getBytes()); &#125; &#125; @Override public void close(TaskAttemptContext context) throws IOException, InterruptedException &#123; // 关闭资源IOUtils.closeStream(atguiguOut); IOUtils.closeStream(otherOut); &#125;&#125; 注意：记得将自定义输出格式设置到job中 12// 要将自定义的输出格式组件设置到job中job.setOutputFormatClass(FilterOutputFormat.class); 7 Join多种应用7.1 工作原理​ Map端的主要工作：为来自不同表或文件的key/value对，打标签以区别不同来源的记录。然后用连接字段作为key，其余部分和新加的标志作为value，最后进行输出。 ​ Reduce端的主要工作：在Reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录（在Map阶段已经打标志分开，最后进行合并就ok了。 7.2 Reduce join​ 在Mapper阶段使得连接属性为key，其余属性为value（自定义bean对象，记得序列化），再用一个标记属性标记来自于哪个文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package com.atguigu.mapreduce.table;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.FileSplit;public class TableMapper extends Mapper&lt;LongWritable, Text, Text, TableBean&gt;&#123;String name; TableBean bean = new TableBean(); Text k = new Text(); @Override protected void setup(Context context) throws IOException, InterruptedException &#123; // 1 获取输入文件切片 FileSplit split = (FileSplit) context.getInputSplit(); // 2 获取输入文件名称 name = split.getPath().getName(); &#125; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取输入数据 String line = value.toString(); // 2 不同文件分别处理 if (name.startsWith(&quot;order&quot;)) &#123;// 订单表处理 // 2.1 切割 String[] fields = line.split(&quot;\\t&quot;); // 2.2 封装bean对象 bean.setOrder_id(fields[0]); bean.setP_id(fields[1]); bean.setAmount(Integer.parseInt(fields[2])); bean.setPname(&quot;&quot;); bean.setFlag(&quot;order&quot;); k.set(fields[1]); &#125;else &#123;// 产品表处理 // 2.3 切割 String[] fields = line.split(&quot;\\t&quot;); // 2.4 封装bean对象 bean.setP_id(fields[0]); bean.setPname(fields[1]); bean.setFlag(&quot;pd&quot;); bean.setAmount(0); bean.setOrder_id(&quot;&quot;); k.set(fields[0]); &#125; // 3 写出 context.write(k, bean); &#125;&#125; ​ 在Reduce阶段，输出连接成功的bean对象 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.atguigu.mapreduce.table;import java.io.IOException;import java.util.ArrayList;import org.apache.commons.beanutils.BeanUtils;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class TableReducer extends Reducer&lt;Text, TableBean, TableBean, NullWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;TableBean&gt; values, Context context) throws IOException, InterruptedException &#123; // 1准备存储订单的集合 ArrayList&lt;TableBean&gt; orderBeans = new ArrayList&lt;&gt;(); // 2 准备bean对象 TableBean pdBean = new TableBean(); for (TableBean bean : values) &#123; if (&quot;order&quot;.equals(bean.getFlag())) &#123;// 订单表 // 拷贝传递过来的每条订单数据到集合中 TableBean orderBean = new TableBean(); try &#123; BeanUtils.copyProperties(orderBean, bean); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; orderBeans.add(orderBean); &#125; else &#123;// 产品表 try &#123; // 拷贝传递过来的产品表到内存中 BeanUtils.copyProperties(pdBean, bean); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; // 3 表的拼接 for(TableBean bean:orderBeans)&#123; bean.setPname (pdBean.getPname()); // 4 数据写出去 context.write(bean, NullWritable.get()); &#125; &#125;&#125; 缺点：这种方式中，合并的操作是在Reduce阶段完成，Reduce端的处理压力太大，Map节点的运算负载则很低，资源利用率不高，且在Reduce阶段极易产生数据倾斜。 map join使用场景：Map Join适用于一张表十分小、一张表很大的场景。 具体办法：采用DistributedCache ​ （1）在Mapper的setup阶段，将文件读取到缓存集合中。 ​ （2）在驱动函数中加载缓存。 ​ // 缓存普通文件到Task运行节点。 ​ job.addCacheFile(new URI(“file://e:/cache/pd.txt”)); 注：简单说就是把一个小表用map表示，用大表的连接属性直接映射出小表的属性 对于驱动模块Driver来说 1234// 6 加载缓存数据job.addCacheFile(new URI(&quot;file:///e:/input/inputcache/pd.txt&quot;)); // 7 Map端Join的逻辑不需要Reduce阶段，设置reduceTask数量为0job.setNumReduceTasks(0); 对于mapper来说，先在map前读取缓存数据 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package test;import java.io.BufferedReader;import java.io.FileInputStream;import java.io.IOException;import java.io.InputStreamReader;import java.util.HashMap;import java.util.Map;import org.apache.commons.lang.StringUtils;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class DistributedCacheMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123; Map&lt;String, String&gt; pdMap = new HashMap&lt;&gt;(); @Override protected void setup(Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; // 1 获取缓存的文件 URI[] cacheFiles = context.getCacheFiles(); String path = cacheFiles[0].getPath().toString(); BufferedReader reader = new BufferedReader(new InputStreamReader(new FileInputStream(path), &quot;UTF-8&quot;)); String line; while(StringUtils.isNotEmpty(line = reader.readLine()))&#123; // 2 切割 String[] fields = line.split(&quot;\\t&quot;); // 3 缓存数据到集合 pdMap.put(fields[0], fields[1]); &#125; // 4 关流 reader.close(); &#125; Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 截取 String[] fields = line.split(&quot;\\t&quot;); // 3 获取产品id String pId = fields[1]; // 4 获取商品名称 String pdName = pdMap.get(pId); // 5 拼接 k.set(line + &quot;\\t&quot;+ pdName); // 6 写出 context.write(k, NullWritable.get()); &#125;&#125; 8 计数器应用与数据清洗（ETL）​ Hadoop为每个作业维护若干内置计数器，以描述多项指标。例如，某些计数器记录已处理的字节数和记录数，使用户可监控已处理的输入数据量和已产生的输出数据量。 使用方法： 1context.getCounter(&quot;map&quot;,&quot;失败&quot;).increment(1); 9 MapRedece开发总结1. 输入数据接口：InputFormat（1）默认使用的实现类是：TextInputFormat （2）TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回。 （3）KeyValueTextlnputFomat每一行均为一条记录，被分隔符分割为key，value。默认分隔符是tab（\\t）。（4）NlinelnputFormat按照指定的行数N来划分切片。 （5）CombineTextlnputFormat可以把多个小文件合并成一个切片处理，提高处理效率。 （6）用户还可以自定义ImputFormat。 2.逻辑处理接口：Mapper用户根据业务需求实现其中三个方法：map）setup）cleanup） 3.Partitioner分区（1）有默认实现HashPartitioner，逻辑是根据key的哈希值和numReduces来返回一个分区号；key hashCode（）&amp;Integer.MAXVALUE%numReduces （2）如果业务上有特别的需求，可以自定义分区。 4.Comparable排序（1）当我们用自定义的对象作为key来输出时，就必须要实现WritableComparable接口，重写其中的compareTo0方法。 （2）部分排序：对最终输出的每一个文件进行内部排序。 （3）全排序：对所有数据进行排序，通常只有一个Reduce。 （4）二次排序：排序的条件有两个。 5.Combiner合并Combiner合并可以提高程序执行效率，减少I0传输。但是使用时必须不能影响原有的业务处理结果。 6.Reduce端分组：GroupingComparator在Reduce端对key进行分组。应用于：在接收的key为bean对象时，想让一个或几个字段相同（全部字段比较不相同）的key进入到同一个reduce方法时，可以采用分组排序。 7.逻辑处理接口：Reducer用户根据业务需求实现其中三个方法：reduce();setup();cleanup() 8.输出数据接口：OutputFormat（1）默认实现类是TextOutputFormat，功能逻辑是：将每一个KV对，向目标文本文件输出一行。 （2）将SequenceFileOutputFormat输出作为后续MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑，很容易被压缩。 （3）用户还可以自定义OutputFormat。","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://kiedeng.github.io/categories/Hadoop/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"bean对象没有序列化","slug":"bean对象没有序列化","date":"2020-02-29T08:39:08.000Z","updated":"2020-02-29T08:54:00.899Z","comments":true,"path":"2020/02/29/bean对象没有序列化/","link":"","permalink":"http://kiedeng.github.io/2020/02/29/bean%E5%AF%B9%E8%B1%A1%E6%B2%A1%E6%9C%89%E5%BA%8F%E5%88%97%E5%8C%96/","excerpt":"","text":"在使用bean对象进行数据传输的过程中，一定要注意序列化，不然将出现map输出收集器的错误 12345678910111213142020-02-29 16:36:31,063 WARN [org.apache.hadoop.mapred.MapTask] - Unable to initialize MapOutputCollector org.apache.hadoop.mapred.MapTask$MapOutputBufferjava.lang.NullPointerException at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.init(MapTask.java:1011) at org.apache.hadoop.mapred.MapTask.createSortingCollector(MapTask.java:402) at org.apache.hadoop.mapred.MapTask.access$100(MapTask.java:81) at org.apache.hadoop.mapred.MapTask$NewOutputCollector.&lt;init&gt;(MapTask.java:698) at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:770) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341) at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:748) 出错位置：Reduce Join案例","categories":[{"name":"常见错误","slug":"常见错误","permalink":"http://kiedeng.github.io/categories/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"MapReduce流程图","slug":"MapReduce流程图","date":"2020-02-28T14:22:07.000Z","updated":"2020-02-28T16:11:22.122Z","comments":true,"path":"2020/02/28/MapReduce流程图/","link":"","permalink":"http://kiedeng.github.io/2020/02/28/MapReduce%E6%B5%81%E7%A8%8B%E5%9B%BE/","excerpt":"","text":"","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://kiedeng.github.io/categories/Hadoop/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"文档资料","slug":"文档资料","date":"2020-02-28T12:58:31.000Z","updated":"2020-02-28T14:57:22.875Z","comments":true,"path":"2020/02/28/文档资料/","link":"","permalink":"http://kiedeng.github.io/2020/02/28/%E6%96%87%E6%A1%A3%E8%B5%84%E6%96%99/","excerpt":"","text":"HDFS: https://books.kiedeng.site/hadoop/hdfs.pdf MapReduce: https://books.kiedeng.site/hadoop/mapreduce.pdf","categories":[{"name":"工具","slug":"工具","permalink":"http://kiedeng.github.io/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"在线预览文件pdfJS","slug":"在线预览文件pdfJS","date":"2020-02-28T12:31:38.000Z","updated":"2020-02-28T12:53:52.987Z","comments":true,"path":"2020/02/28/在线预览文件pdfJS/","link":"","permalink":"http://kiedeng.github.io/2020/02/28/%E5%9C%A8%E7%BA%BF%E9%A2%84%E8%A7%88%E6%96%87%E4%BB%B6pdfJS/","excerpt":"","text":"1 下载pdf.js插件地址: http://mozilla.github.io/pdf.js/ 下载的时候选择Stable（稳定版），下载完成以后进行解压， 2 部署在tomcat的webapps文件夹下新建一个目录比如books，进解压的文件传入。 运行tomcat即可以进行访问 http://localhost:8080/books/web/viewer.html 3 添加文档访问自己需要访问的pdf文档，就可以先在books下新建一个目录，比如hadoop，将自己的1.pdf文档放入hadoop目录下，就可以通过http://localhost:8080/books/hadoop/1.pdf访问。 4 云服务器将localhost换成服务器的ip或者域名即可","categories":[{"name":"Tomcat","slug":"Tomcat","permalink":"http://kiedeng.github.io/categories/Tomcat/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"GroupingComparator错误","slug":"GroupingComparator错误","date":"2020-02-28T10:43:08.000Z","updated":"2020-02-28T11:00:25.025Z","comments":true,"path":"2020/02/28/GroupingComparator错误/","link":"","permalink":"http://kiedeng.github.io/2020/02/28/GroupingComparator%E9%94%99%E8%AF%AF/","excerpt":"","text":"在进行重写的时候千万要小心，一不小心就会导致出错，注意参数！ 错误处： 12345@Overridepublic int compare(Object a, Object b) &#123; // TODO Auto-generated method stub return super.compare(a, b);&#125; 改正： 123456789101112131415@Overridepublic int compare(WritableComparable a, WritableComparable b) &#123; // 要求只要id相同，就认为是相同的key OrderBean aBean = (OrderBean) a; OrderBean bBean = (OrderBean) b; int result; if (aBean.getOrder_id() &gt; bBean.getOrder_id()) &#123; result = 1; &#125;else if(aBean.getOrder_id() &lt; bBean.getOrder_id())&#123; result = -1; &#125;else &#123; result = 0; &#125; return result;&#125; 出错案例：统计order的top1","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://kiedeng.github.io/categories/Hadoop/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"typora与typecho的问题","slug":"typora与typecho的问题","date":"2020-02-27T15:59:39.000Z","updated":"2020-02-27T16:38:41.704Z","comments":true,"path":"2020/02/27/typora与typecho的问题/","link":"","permalink":"http://kiedeng.github.io/2020/02/27/typora%E4%B8%8Etypecho%E7%9A%84%E9%97%AE%E9%A2%98/","excerpt":"","text":"标题问题​ 为了能使typecho的“文章目录”能够正确的显示，在typora设置的标题必须是连续的； 比如：#","categories":[{"name":"未分类","slug":"未分类","permalink":"http://kiedeng.github.io/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"Combiner合并","slug":"Combiner合并","date":"2020-02-27T09:47:40.000Z","updated":"2020-02-27T09:47:40.129Z","comments":true,"path":"2020/02/27/Combiner合并/","link":"","permalink":"http://kiedeng.github.io/2020/02/27/Combiner%E5%90%88%E5%B9%B6/","excerpt":"","text":"","categories":[{"name":"未分类","slug":"未分类","permalink":"http://kiedeng.github.io/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"map值类型不匹配","slug":"map值类型不匹配","date":"2020-02-27T07:48:50.000Z","updated":"2020-02-27T08:47:02.567Z","comments":true,"path":"2020/02/27/map值类型不匹配/","link":"","permalink":"http://kiedeng.github.io/2020/02/27/map%E5%80%BC%E7%B1%BB%E5%9E%8B%E4%B8%8D%E5%8C%B9%E9%85%8D/","excerpt":"","text":"map值类型不匹配 123java.lang.Exception: java.io.IOException: Type mismatch in value from map: expected com.atguigu.mr.sort.FlowBean, received org.apache.hadoop.io.Text at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462) at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522) 错误位置：进行排序案例的时候发生的错误 12job.setMapOutputKeyClass(FlowBean.class);job.setMapOutputValueClass(FlowBean.class);//应该为Text.class 注：写Driver驱动的时候，要特别注意类型错误问题 因为不太懂这方面的错，所以","categories":[{"name":"常见错误","slug":"常见错误","permalink":"http://kiedeng.github.io/categories/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"WritableComparable排序","slug":"WritableComparable排序","date":"2020-02-27T04:37:15.000Z","updated":"2020-02-28T07:01:59.398Z","comments":true,"path":"2020/02/27/WritableComparable排序/","link":"","permalink":"http://kiedeng.github.io/2020/02/27/WritableComparable%E6%8E%92%E5%BA%8F/","excerpt":"","text":"排序的分类 部分排序 MapReduce根据输入记录的键对数据集排序，保证输出的每个文件内部有序 全排序 最终输出结果为一个文件，且文件内部有序 辅助排序：（GroupingComparator分组) 在Reduece端对key进行分组。应用于key为bean对象时，想让一个或几个字段相同的key 进入到同一个reduce方法时，可以采用分组排序 二次排序 在自定义排序过程中，如果compareTo中的判断添加为两个即为二次排序 自定义排序WritableComparable1.原理分析 ​ bean对象作为key传输，需要实现WritableComparable接口重写compareTo方法，就可以实现排序 12345678910111213@Overridepublic int compareTo(FlowBean o) &#123; int result; // 按照总流量大小，倒序排列 if (sumFlow &gt; bean.getSumFlow()) &#123; result = -1; &#125;else if (sumFlow &lt; bean.getSumFlow()) &#123; result = 1; &#125;else &#123; result = 0; &#125; return result;&#125; WritableComparable排序案例实操（全排序） 需求：对总流量进行排序 代码实现 使用以前的Flowcount就可以实现，代码就可以实现，略 排序案例（区内排序）需求：要求每个省份手机号输出的文件中按照总流量内部排序 注意点：区间排序中，需要添加的是自定义的Patitioner分区类与在驱动类中添加分区类，设置Reducetask个数 12345678910111213141516171819202122232425262728package com.atguigu.mapreduce.sort;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Partitioner;public class ProvincePartitioner extends Partitioner&lt;FlowBean, Text&gt; &#123; @Override public int getPartition(FlowBean key, Text value, int numPartitions) &#123; // 1 获取手机号码前三位 String preNum = value.toString().substring(0, 3); int partition = 4; // 2 根据手机号归属地设置分区 if (&quot;136&quot;.equals(preNum)) &#123; partition = 0; &#125;else if (&quot;137&quot;.equals(preNum)) &#123; partition = 1; &#125;else if (&quot;138&quot;.equals(preNum)) &#123; partition = 2; &#125;else if (&quot;139&quot;.equals(preNum)) &#123; partition = 3; &#125; return partition; &#125;&#125; 1234// 加载自定义分区类job.setPartitionerClass(ProvincePartitioner.class);// 设置Reducetask个数job.setNumReduceTasks(5); GroupingComparator分组（辅助排序）对Reduce阶段的数据根据某一个或几个字段进行分组 自定义继承WritableComparator 重写compare()方法 12345@Overridepublic int compare(WritableComparable a, WritableComparable b) &#123; // 比较的业务逻辑 return result;&#125; 创建一个构造将比较对象的类传给父类 123protected OrderGroupingComparator() &#123; super(OrderBean.class, true);&#125; GroupingComparator分组案例实操 需求：有如下订单数据； 表4-2 订单数据 订单id 商品id 成交金额 0000001 Pdt_01 222.8 Pdt_02 33.8 0000002 Pdt_03 522.8 Pdt_04 122.4 Pdt_05 722.4 0000003 Pdt_06 232.8 Pdt_02 33.8 现在需要求出每一个订单中最贵的商品。 期望输出数据* 1 222.8 2 722.4 3 232.8 需求分析 利用“订单id和成交金额”作为key，可以将Map阶段读取到的所有订单数据按照id升序排序，如果id相同按照金额降序排序，发送到Reduce 在Reduce端利用groupComparator将订单id相同的kv合成为组，然后取第一个即是该订单中最贵商品 代码实现 定义订单信息OrderBean类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package com.atguigu.mapreduce.order;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.WritableComparable;public class OrderBean implements WritableComparable&lt;OrderBean&gt; &#123; private int order_id; // 订单id号 private double price; // 价格 public OrderBean() &#123; super(); &#125; public OrderBean(int order_id, double price) &#123; super(); this.order_id = order_id; this.price = price; &#125; @Override public void write(DataOutput out) throws IOException &#123; out.writeInt(order_id); out.writeDouble(price); &#125; @Override public void readFields(DataInput in) throws IOException &#123; order_id = in.readInt(); price = in.readDouble(); &#125; @Override public String toString() &#123; return order_id + &quot;\\t&quot; + price; &#125; public int getOrder_id() &#123; return order_id; &#125; public void setOrder_id(int order_id) &#123; this.order_id = order_id; &#125; public double getPrice() &#123; return price; &#125; public void setPrice(double price) &#123; this.price = price; &#125; // 二次排序 @Override public int compareTo(OrderBean o) &#123; int result; if (order_id &gt; o.getOrder_id()) &#123; result = 1; &#125; else if (order_id &lt; o.getOrder_id()) &#123; result = -1; &#125; else &#123; // 价格倒序排序 result = price &gt; o.getPrice() ? -1 : 1; &#125; return result; &#125;&#125; 编写OrderSortMapper类 12345678910111213141516171819202122232425262728package com.atguigu.mapreduce.order;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class OrderMapper extends Mapper&lt;LongWritable, Text, OrderBean, NullWritable&gt; &#123; OrderBean k = new OrderBean(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 截取 String[] fields = line.split(&quot;\\t&quot;); // 3 封装对象 k.setOrder_id(Integer.parseInt(fields[0])); k.setPrice(Double.parseDouble(fields[2])); // 4 写出 context.write(k, NullWritable.get()); &#125;&#125; 编写OrderSortGroupingComparator类 12345678910111213141516171819202122232425262728package com.atguigu.mapreduce.order;import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;public class OrderGroupingComparator extends WritableComparator &#123; protected OrderGroupingComparator() &#123; super(OrderBean.class, true); &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; OrderBean aBean = (OrderBean) a; OrderBean bBean = (OrderBean) b; int result; if (aBean.getOrder_id() &gt; bBean.getOrder_id()) &#123; result = 1; &#125; else if (aBean.getOrder_id() &lt; bBean.getOrder_id()) &#123; result = -1; &#125; else &#123; result = 0; &#125; return result; &#125;&#125; 编写OrderSortReducer类 12345678910111213package com.atguigu.mapreduce.order;import java.io.IOException;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.Reducer;public class OrderReducer extends Reducer&lt;OrderBean, NullWritable, OrderBean, NullWritable&gt; &#123; @Override protected void reduce(OrderBean key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; context.write(key, NullWritable.get()); &#125;&#125; 编写OrderSortDriver类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.atguigu.mapreduce.order;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class OrderDriver &#123; public static void main(String[] args) throws Exception, IOException &#123;// 输入输出路径需要根据自己电脑上实际的输入输出路径设置 args = new String[]&#123;&quot;e:/input/inputorder&quot; , &quot;e:/output1&quot;&#125;; // 1 获取配置信息 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2 设置jar包加载路径 job.setJarByClass(OrderDriver.class); // 3 加载map/reduce类 job.setMapperClass(OrderMapper.class); job.setReducerClass(OrderReducer.class); // 4 设置map输出数据key和value类型 job.setMapOutputKeyClass(OrderBean.class); job.setMapOutputValueClass(NullWritable.class); // 5 设置最终输出数据的key和value类型 job.setOutputKeyClass(OrderBean.class); job.setOutputValueClass(NullWritable.class); // 6 设置输入数据和输出数据路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 8 设置reduce端的分组 job.setGroupingComparatorClass(OrderGroupingComparator.class); // 7 提交 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125;","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://kiedeng.github.io/categories/Hadoop/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"Partition分区","slug":"Partition分区","date":"2020-02-26T15:51:54.000Z","updated":"2020-02-26T16:46:55.670Z","comments":true,"path":"2020/02/26/Partition分区/","link":"","permalink":"http://kiedeng.github.io/2020/02/26/Partition%E5%88%86%E5%8C%BA/","excerpt":"","text":"问题引出：将结果按照条件输出到不同文件（分区）中 默认的分区是根据key的hashCode对ReduceTasks个数取模得到的。 自定义Partitioner步骤 自定义继承Partitioner,重写getPartition()方法 在job驱动中，设置自定义Partitioner 1job.setPatitionerClass(CustonPartitioner.class) 自定义Partition后，要根据自定义Partition的逻辑设置相对应的ReduceTask 1job.setNumReduceTasd(); 分区总结 （1）如果Reduce Task的数量&gt;getPartition的结果数，则会多产生几个空的输出文件part-r-000xx； （2）如果1&lt;ReduceTask的数量&lt;getPartition的结果数，则有一部分分区数据无处安放，会Exception；（3）如果ReduceTask的数量=1，则不管MapTask端输出多少个分区文件，最终结果都交给这一个Reduce Task，最终也就只会产生一个结果文件part-r-00000；（4）分区号必须从零开始，逐一累加。 Partition案例实操题目：将统计结果按照手机归属地不同省份输出到不同文件中（分区） 添加分区类 12345678910111213141516171819202122232425262728package com.atguigu.mapreduce.flowsum;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Partitioner;public class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; &#123; @Override public int getPartition(Text key, FlowBean value, int numPartitions) &#123; // 1 获取电话号码的前三位 String preNum = key.toString().substring(0, 3); int partition = 4; // 2 判断是哪个省 if (&quot;136&quot;.equals(preNum)) &#123; partition = 0; &#125;else if (&quot;137&quot;.equals(preNum)) &#123; partition = 1; &#125;else if (&quot;138&quot;.equals(preNum)) &#123; partition = 2; &#125;else if (&quot;139&quot;.equals(preNum)) &#123; partition = 3; &#125; return partition; &#125;&#125; 在驱动函数中增加自定义数据分区设置和ReduceTask设置 1234// 8 指定自定义数据分区job.setPartitionerClass(ProvincePartitioner.class);// 9 同时指定相应数量的reduce taskjob.setNumReduceTasks(5);","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://kiedeng.github.io/categories/Hadoop/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"Text导包错误","slug":"Text导包错误","date":"2020-02-26T05:52:34.000Z","updated":"2020-02-26T05:58:21.071Z","comments":true,"path":"2020/02/26/Text导包错误/","link":"","permalink":"http://kiedeng.github.io/2020/02/26/Text%E5%AF%BC%E5%8C%85%E9%94%99%E8%AF%AF/","excerpt":"","text":"123456789101112131415java.lang.ClassCastException: class com.sun.jersey.core.impl.provider.entity.XMLJAXBElementProvider$Text at java.lang.Class.asSubclass(Class.java:3404) at org.apache.hadoop.mapred.JobConf.getOutputKeyComparator(JobConf.java:887) at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.init(MapTask.java:1004) at org.apache.hadoop.mapred.MapTask.createSortingCollector(MapTask.java:402) at org.apache.hadoop.mapred.MapTask.access$100(MapTask.java:81) at org.apache.hadoop.mapred.MapTask$NewOutputCollector.&lt;init&gt;(MapTask.java:698) at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:770) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341) at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:748) 错误的导入了Text包， 1234错误： import com.sun.jersey.core.impl.provider.entity.XMLJAXBElementProvider.Text;正确的包： import org.apache.hadoop.io.Text;","categories":[{"name":"常见错误","slug":"常见错误","permalink":"http://kiedeng.github.io/categories/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"LeetCode计划表","slug":"LeetCode计划表","date":"2020-02-21T05:26:42.000Z","updated":"2020-02-21T05:33:01.684Z","comments":true,"path":"2020/02/21/LeetCode计划表/","link":"","permalink":"http://kiedeng.github.io/2020/02/21/LeetCode%E8%AE%A1%E5%88%92%E8%A1%A8/","excerpt":"","text":"题目序号 题目名称 总结 1 Two Sum 4 Median of Two Sorted Arrays 11 Container With Most Water 17 Letter Combinations of a Phone Number 21 Merge Two Sorted Lists 23 Merge k Sorted Lists 37 Sudoku Solver 39 Combination Sum 40 Combination Sum II 51 N-Queens 53 Maximum Subarray 56 Merge Intervals 57 Insert Interval 62 Unique Paths 63 Unique Paths II 64 Minimum Path Sum 69 sqrt(x) 70 Climbing Stairs 72 Edit Distance 78 Subsets 79 Word Search 91 Decode Ways 98 Validate Binary Search Tree 100 Same Tree 102 Binary Tree Level Order Traversal 110 Balanced Binary Tree 112 Path Sum 113 Path Sum II 115 Distinct Subsequences 120 Triangle 121 Best Time to Buy and Sell Stock 124 Binary Tree Maximum Path Sum 126 Word Ladder II 127 Word Ladder 128 Longest Consecutive Sequence 139 Word Break (revisit) 140 Word Break II 141 Linked List Cycle 145 Binary Tree Postorder Traversal 146 LRU Cache 148 Sort List 149 Max Points on a Line 153 Find Minimum in Rotated Sorted Array 154 Find Minimum in Rotated Sorted Array II 169 Majority Element 174 Dungeon Game 198 House Robber 200 Number of Islands 204 Count Primes 207 Course Schedule 208 Implement Trie (Prefix Tree) 210 Course Schedule II 216 Combination Sum III 218 The Skyline Problem 221 Maximal Square 239 Sliding Window Maximum 241 Different Ways to Add Parentheses 263 Ugly Number 264 Ugly Number II 268 Missing Number 282 Expression Add Operators 289 Game of Life 295 Find Median from Data Stream 297 Serialize and Deserialize Binary Tree 300 Longest Increasing Subsequence 301 Remove Invalid Parentheses 303 Range Sum Query - Immutable 304 Range Sum Query 2D - Immutable 309 Best Time to Buy and Sell Stock with Cooldown 312 Burst Balloons 315 Count of Smaller Numbers After Self 321 Create Maximum Number 322 Coin Change 329 Longest Increasing Path in a Matrix 332 Reconstruct Itinerary 347 Top K Frequent Elements 377 Combination Sum IV 380 Insert Delete GetRandom O(1) 381 Insert Delete GetRandom O(1) - Duplicates allowed 391 Perfect Rectangle 399 Evaluate Division 404 Sum of Left Leaves 409 Longest Palindrome 410 Split Array Largest Sum 416 Partition Equal Subset Sum 417 Pacific Atlantic Water Flow 432 All O`one Data Structure 438 Find All Anagrams in a String 449 Serialize and Deserialize BST 450 Delete Node in a BST 451 Sort Characters By Frequency 452 Minimum Number of Arrows to Burst Balloons 455 Assign Cookies 460 LFU Cache 461 Hamming Distance 463 Island Perimeter 464 Can I Win 470 Implement Rand10() Using Rand7() 476 Number Complement 477 Total Hamming Distance 480 Sliding Window Median 486 Predict the Winner 488 Zuma Game 494 Target Sum2 494 Target Sum 504 Base 7 516 Longest Palindromic Subsequence 525 Contiguous Array 530 Minimum Absolute Difference in BST 540 Single Element in a Sorted Array 543 Diameter of Binary Tree 546 Remove Boxes 547 Friend Circles 551 Student Attendance Record I 560 Subarray Sum Equals K 561 Array Partition I 566 Reshape the Matrix 567 Permutation in String 576 Out of Boundary Paths 606 Construct String from Binary Tree 611 Valid Triangle Number 617 Merge Two Binary Trees 621 Task Scheduler 628 Maximum Product of Three Numbers 633 Sum of Square Numbers 636 Exclusive Time of Functions 637 Average of Levels in Binary Tree 639 Decode Ways II 652 Find Duplicate Subtrees 654 Maximum Binary Tree 655 Print Binary Tree 657 Judge Route Circle 664 Strange Printer 668 Kth Smallest Number in Multiplication Table 669 Trim a Binary Search Tree 671 Second Minimum Node In a Binary Tree 673 Number of Longest Increasing Subsequence 674 Longest Continuous Increasing Subsequence 675 Cut Off Trees for Golf Event 676 Implement Magic Dictionary 677 Map Sum Pairs 678 Valid Parenthesis String 680 Valid Palindrome II 681 Next Closest Time 682 Baseball Game 683 K Empty Slots 684 Redundant Connection 685 Redundant Connection II 687 Longest Univalue Path 688 Knight Probability in Chessboard 690 Employee Importance 692 Top K Frequent Words 699 Falling Squares 707 Design Linked List 712 Minimum ASCII Delete Sum for Two Strings 715 Range Module 719 Find K-th Smallest Pair Distance 720 Longest Word in Dictionary 724 Find Pivot Index 725 Split Linked List in Parts 726 Number of Atoms 728 Self Dividing Numbers 729 My Calendar I 730 Count Different Palindromic Subsequences 731 My Calendar II 732 My Calendar III 733 Flood Fill 734 Sentence Similarity 735 Asteroid Collision 736 Parse Lisp Expression 737 Sentence Similarity II 740 Delete and Earn 741 Cherry Pickup 742 Closest Leaf in a Binary Tree 743 Network Delay Time 744 Find Smallest Letter Greater Than Target 745 Prefix and Suffix Search 746 Min Cost Climbing Stairs 748 Shortest Completing Word 749 Contain Virus 752 Open the Lock 753 Cracking the Safe 754 Reach a Number 755 Pour Water 758 Bold Words in String 759 Employee Free Time 762 Prime Number of Set Bits in Binary Representation 763 Partition Labels 769 Max Chunks To Make Sorted 773 Sliding Puzzle 775 Global and Local Inversions 778 Swim in Rising Water 784 Letter Case Permutation 786 K-th Smallest Prime Fraction 787 Cheapest Flights Within K Stops 790 Domino and Tromino Tiling 792 Number of Matching Subsequences 799 Champagne Tower 801 Minimum Swaps To Make Sequences Increasing 802 Find Eventual Safe States 803 Bricks Falling When Hit 813 Largest Sum of Averages 815 Bus Routes 817 Linked List Components 818 Race Car2 818 Race Car 823 Binary Trees With Factors 826 Most Profit Assigning Work 827 Making A Large Island 841 Keys and Rooms 847 Shortest Path Visiting All Nodes 848 Shifting Letters 856 Score of Parentheses 863 All Nodes Distance K in Binary Tree 864 Shortest Path to Get All Keys 865 Smallest Subtree with all the Deepest Nodes 871 Minimum Number of Refueling Stops 873 Length of Longest Fibonacci Subsequence 877 Stone Game 879 Profitable Schemes 882 Reachable Nodes In Subdivided Graph 886 Possible Bipartition 889 Construct Binary Tree from Preorder and Postorder Traversal 891 Sum of Subsequence Widths 894 All Possible Full Binary Trees 895 Maximum Frequency Stack 898 Bitwise ORs of Subarrays 901 Online Stock Span 902 Numbers At Most N Given Digit Set 923 3Sum With Multiplicity 926 Flip String to Monotone Increasing 934 Shortest Bridge 935 Knight Dialer 936 Stamping The Sequence 943 Find the Shortest Superstring 952 Largest Component Size by Common Factor 956 Tallest Billboard 959 Regions Cut By Slashes 964 Least Operators to Express Number 967 Numbers With Same Consecutive Differences 972 Equal Rational Numbers 973 K Closest Points to Origin 975 Odd Even Jump 979 Distribute Coins in Binary Tree 980 Unique Paths III 1000 Minimum Cost to Merge Stones 1017 Convert to Base -2 1019 Next Greater Node In Linked List 1024 Video Stitching 1043 Partition Array for Maximum Sum 1092 Shortest Common Supersequenc 1105 Filling Bookcase Shelves 1106 Parsing A Boolean Expression 1124 Longest Well-Performing Interval 1125 Smallest Sufficient Team 1129 Shortest Path with Alternating Colors 的","categories":[{"name":"算法","slug":"算法","permalink":"http://kiedeng.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"LeetCode","slug":"LeetCode","permalink":"http://kiedeng.github.io/tags/LeetCode/"}]},{"title":"MapReduce工作流程","slug":"MapReduce工作流程","date":"2020-02-20T13:39:47.000Z","updated":"2020-02-20T13:39:47.432Z","comments":true,"path":"2020/02/20/MapReduce工作流程/","link":"","permalink":"http://kiedeng.github.io/2020/02/20/MapReduce%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B/","excerpt":"","text":"","categories":[{"name":"未分类","slug":"未分类","permalink":"http://kiedeng.github.io/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"Shuffle机制","slug":"Shuffle机制","date":"2020-02-20T13:38:49.000Z","updated":"2020-02-20T13:38:49.220Z","comments":true,"path":"2020/02/20/Shuffle机制/","link":"","permalink":"http://kiedeng.github.io/2020/02/20/Shuffle%E6%9C%BA%E5%88%B6/","excerpt":"","text":"","categories":[{"name":"未分类","slug":"未分类","permalink":"http://kiedeng.github.io/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"InputFormat数据输入","slug":"InputFormat数据输入","date":"2020-02-20T13:38:16.000Z","updated":"2020-02-28T02:10:46.494Z","comments":true,"path":"2020/02/20/InputFormat数据输入/","link":"","permalink":"http://kiedeng.github.io/2020/02/20/InputFormat%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5/","excerpt":"","text":"切片与MapTask并行度决定机制数据块：Block是HDFS物理上把数据分成一块一块。 数据切片：数据切片只是逻辑上对输入进行分片，并不会再磁盘上将其切分成片进行存储。 Job提交流程源码和切片源码详解 Job提交流程源码详解 12345678910111213141516171819202122232425262728293031323334waitForCompletion()submit();// 1建立连接 connect(); // 1）创建提交Job的代理 new Cluster(getConfiguration()); // （1）判断是本地yarn还是远程 initialize(jobTrackAddr, conf); // 2 提交jobsubmitter.submitJobInternal(Job.this, cluster) // 1）创建给集群提交数据的Stag路径 Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf); // 2）获取jobid ，并创建Job路径 JobID jobId = submitClient.getNewJobID(); // 3）拷贝jar包到集群copyAndConfigureFiles(job, submitJobDir); rUploader.uploadFiles(job, jobSubmitDir);// 4）计算切片，生成切片规划文件writeSplits(job, submitJobDir); maps = writeNewSplits(job, jobSubmitDir); input.getSplits(job);// 5）向Stag路径写XML配置文件writeConf(conf, submitJobFile); conf.writeXml(out);// 6）提交Job,返回提交状态status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials()); FileInputFormat切片机制1 切片机制 简单的按照文件的内容长度进行切片 切片大小，默认等于Block大小 切片时不考虑数据集整体，而是逐个针对每一个文件单独切片 注：每次切片，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片 2 源码中计算机切片大小的公式 Math. max(minSize, Math.min(maxSize, blockSize)); maprecduce.input.fileinputformat.split.minsize=1默认值为1 mapreduce.input.fileinputformat.split maxsize=LongMAXValue 默认值Long.MAXValue,因此，默认情况下，切片大小=blocksize 3 获取切片信息API1234//获取切片的文件名称String name=inputSplit.getPath（）.getName（）：//根据文件类型获取切片信息FileSplit inputSplit =（FileSplit）context.get InputSplit（）； CombineTextInputFormat切片机制​ 默认的TextInputFormat切片机制是对任务按文件规划切片，会产生大量小文件，产生大量的MapTask，处理效率极其低下，故CombineTextInputFormat来处理大量小文件的情况。 1 虚拟存储切片最大值设置1CombineTextInputFormat. setMaxInputSplitSize(job,4194304)://4m- 2 切片机制生成切片过程包括：虚拟存储过程和切片过程二部分 虚拟存储过程 将目录下的所有文件与setMaxInputSize比较，小于则切分为一块，大于且小于两倍，则平分这一块，大于两倍，则切出一块setMaxInputSize的块 切片过程 判断虚拟存储文件大小是否大于setMaxInputSplitSize值，大于或等于则单独形成一个切片，否则和下一个切片进行合并，形成一个切片 CombineTextInputFormat案例实操需求：将输入的大量小文件合并成一个切片统一处理（以WordCount为基础），准备四个文件。 只需要在Driver中添加输入格式即可： 12345// 如果不设置InputFormat，它默认用的是TextInputFormat.classjob.setInputFormatClass(CombineTextInputFormat.class);//虚拟存储切片最大值设置4mCombineTextInputFormat.setMaxInputSplitSize(job, 4194304); FileInputFormat实现类​ FileInputFormat 常见的接口实现类包括：TextinputFormat、KeyValue TextInputFormat、NLinelnputFormat、CombineTextinputFormat，自定义InputFormat等。 1 TextInputFormatFileInputFile默认的实现类，按行读，键为字节偏移量，LongWritable类型 2 KeyValueTextInputFormat通过分隔符，分为key，value，课通过设置 1234// 设置切割符conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, &quot; &quot;);// 设置输入格式job.setInputFormatClass(KeyValueTextInputFormat.class); 3 NlineInputFormat按照行数N来划分，即切片数=输入文件总行数/N 1234// 7设置每个切片InputSplit中划分三条记录NLineInputFormat.setNumLinesPerSplit(job, 3); // 8使用NLineInputFormat处理记录数 job.setInputFormatClass(NLineInputFormat.class); 4 自定义InputFormat 需要自定义一个类继承FileInputFormat 改写RecordReader，实现封装为KV 输出是使用SequenceFileOutFormat输出合并文件","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://kiedeng.github.io/categories/Hadoop/"}],"tags":[{"name":"MapReduce","slug":"MapReduce","permalink":"http://kiedeng.github.io/tags/MapReduce/"}]},{"title":"序列化案例","slug":"序列化案例","date":"2020-02-16T07:08:00.000Z","updated":"2020-02-20T11:27:43.180Z","comments":true,"path":"2020/02/16/序列化案例/","link":"","permalink":"http://kiedeng.github.io/2020/02/16/%E5%BA%8F%E5%88%97%E5%8C%96%E6%A1%88%E4%BE%8B/","excerpt":"","text":"需求：统计每一个手机号耗费的总上行流量，下行流量，总流量 编写MapReduce程序： 编写流量统计的Bean对象 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package com.atguigu.mapreduce.flowsum;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.Writable;// 1 实现writable接口public class FlowBean implements Writable&#123; private long upFlow; private long downFlow; private long sumFlow; //2 反序列化时，需要反射调用空参构造函数，所以必须有 public FlowBean() &#123; super(); &#125; public FlowBean(long upFlow, long downFlow) &#123; super(); this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = upFlow + downFlow; &#125; //3 写序列化方法 @Override public void write(DataOutput out) throws IOException &#123; out.writeLong(upFlow); out.writeLong(downFlow); out.writeLong(sumFlow); &#125; //4 反序列化方法 //5 反序列化方法读顺序必须和写序列化方法的写顺序必须一致 @Override public void readFields(DataInput in) throws IOException &#123; this.upFlow = in.readLong(); this.downFlow = in.readLong(); this.sumFlow = in.readLong(); &#125; // 6 编写toString方法，方便后续打印到文本 @Override public String toString() &#123; return upFlow + &quot;\\t&quot; + downFlow + &quot;\\t&quot; + sumFlow; &#125; public long getUpFlow() &#123; return upFlow; &#125; public void setUpFlow(long upFlow) &#123; this.upFlow = upFlow; &#125; public long getDownFlow() &#123; return downFlow; &#125; public void setDownFlow(long downFlow) &#123; this.downFlow = downFlow; &#125; public long getSumFlow() &#123; return sumFlow; &#125; public void setSumFlow(long sumFlow) &#123; this.sumFlow = sumFlow; &#125;&#125; 编写Mapper类 1234567891011121314151617181920212223242526272829303132333435package com.atguigu.mapreduce.flowsum;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class FlowCountMapper extends Mapper&lt;LongWritable, Text, Text, FlowBean&gt;&#123; FlowBean v = new FlowBean(); Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 切割字段 String[] fields = line.split(&quot;\\t&quot;); // 3 封装对象 // 取出手机号码 String phoneNum = fields[1]; // 取出上行流量和下行流量 long upFlow = Long.parseLong(fields[fields.length - 3]); long downFlow = Long.parseLong(fields[fields.length - 2]); k.set(phoneNum); v.set(downFlow, upFlow); // 4 写出 context.write(k, v); &#125;&#125; 编写Reducer类 1234567891011121314151617181920212223242526package com.atguigu.mapreduce.flowsum;import java.io.IOException;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class FlowCountReducer extends Reducer&lt;Text, FlowBean, Text, FlowBean&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;FlowBean&gt; values, Context context)throws IOException, InterruptedException &#123; long sum_upFlow = 0; long sum_downFlow = 0; // 1 遍历所用bean，将其中的上行流量，下行流量分别累加 for (FlowBean flowBean : values) &#123; sum_upFlow += flowBean.getUpFlow(); sum_downFlow += flowBean.getDownFlow(); &#125; // 2 封装对象 FlowBean resultBean = new FlowBean(sum_upFlow, sum_downFlow); // 3 写出 context.write(key, resultBean); &#125;&#125; 编写Driver驱动类 1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.atguigu.mapreduce.flowsum;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class FlowsumDriver &#123; public static void main(String[] args) throws IllegalArgumentException, IOException, ClassNotFoundException, InterruptedException &#123; // 输入输出路径需要根据自己电脑上实际的输入输出路径设置args = new String[] &#123; &quot;e:/input/inputflow&quot;, &quot;e:/output1&quot; &#125;; // 1 获取配置信息，或者job对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 6 指定本程序的jar包所在的本地路径 job.setJarByClass(FlowsumDriver.class); // 2 指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(FlowCountMapper.class); job.setReducerClass(FlowCountReducer.class); // 3 指定mapper输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(FlowBean.class); // 4 指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); // 5 指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125;","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://kiedeng.github.io/categories/Hadoop/"}],"tags":[{"name":"MapReduce","slug":"MapReduce","permalink":"http://kiedeng.github.io/tags/MapReduce/"}]},{"title":"序列化概述","slug":"序列化概述","date":"2020-02-16T07:06:59.000Z","updated":"2020-02-20T10:58:32.980Z","comments":true,"path":"2020/02/16/序列化概述/","link":"","permalink":"http://kiedeng.github.io/2020/02/16/%E5%BA%8F%E5%88%97%E5%8C%96%E6%A6%82%E8%BF%B0/","excerpt":"","text":"序列化​ 序列化就是把内存中的对象，转换为字节序列（或者其他数据传输协议）以便于存储到磁盘（持久化）和网络传输。 ​ 反序列化就是将收到字节序列或者是磁盘的持久化数据，转换为内存中的对象 原因：一般来说，“活的对象”只生产在内存里，关机断电就没有了，并且不能发送，然而序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机。 由于java序列化是一个重量级序列化框架（Serializable），序列化的时候，会附带很多额外的信息（各种校验信息，Header，继承体系等），不便于在网络中高效传输。故Hadoop开发了一套序列化机制(Writable)。 Hadoop序列化特点： 紧凑：高效实用存储空间 快速：读写数据的额外开销小 可扩展：随着通信协议的升级而升级 互操作：支持多语言的交互 实现序列化接口（Writable） 必须实现Writable接口 反序列化时，需要反射调用空参构造函数，所以必须要有空参构造 123public FlowBean()&#123; super();&#125; 重写序列化方法 123456@Overridepublic void write(DataOutput out) throws IOException &#123; out.writeLong(upFlow); out.writeLong(downFlow); out.writeLong(sumFlow);&#125; 重写反序列化方法 123456@Overridepublic void readFields(DataInput in) throws IOException &#123; upFlow = in.readLong(); downFlow = in.readLong(); sumFlow = in.readLong();&#125; 注意反序列化的顺序和序列化的顺序完全一致 重写toString（），可用“\\t”分开，方便后续用 如果需要将自定义的bean放在key中传输，则还需要实现Comparable接口，因为MapReduce框中的Shuffle过程要求对key必须能排序。 12345@Overridepublic int compareTo(FlowBean o) &#123; // 倒序排列，从大到小 return this.sumFlow &gt; o.getSumFlow() ? -1 : 1;&#125;","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://kiedeng.github.io/categories/Hadoop/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"centos安装MySQL","slug":"centos安装MySQL","date":"2020-02-16T02:14:41.000Z","updated":"2020-02-16T02:15:28.523Z","comments":true,"path":"2020/02/16/centos安装MySQL/","link":"","permalink":"http://kiedeng.github.io/2020/02/16/centos%E5%AE%89%E8%A3%85MySQL/","excerpt":"","text":"","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://kiedeng.github.io/categories/MySQL/"}],"tags":[{"name":"部署","slug":"部署","permalink":"http://kiedeng.github.io/tags/%E9%83%A8%E7%BD%B2/"}]},{"title":"WordCount案例","slug":"WordCount案例","date":"2020-02-15T07:23:09.000Z","updated":"2020-02-15T07:24:46.955Z","comments":true,"path":"2020/02/15/WordCount案例/","link":"","permalink":"http://kiedeng.github.io/2020/02/15/WordCount%E6%A1%88%E4%BE%8B/","excerpt":"","text":"需求：在给定的文本文件中输出每个单词出现的总次数 创建maven工程 pom.xml文件中添加一下依赖 123456789101112131415161718192021222324252627&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 在项目的资源目录下/src/main/resources目录下，新建一个文件，命名为“log4j.properties” 12345678log4j.rootLogger=INFO, stdoutlog4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%nlog4j.appender.logfile=org.apache.log4j.FileAppenderlog4j.appender.logfile.File=target/spring.loglog4j.appender.logfile.layout=org.apache.log4j.PatternLayoutlog4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 编写程序 编写mapper 1234567891011121314151617181920212223242526272829package com.atguigu.mapreduce;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class WordcountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123; Text k = new Text(); IntWritable v = new IntWritable(1); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 切割 String[] words = line.split(&quot; &quot;); // 3 输出 for (String word : words) &#123; k.set(word); context.write(k, v); &#125; &#125;&#125; 编写Reduce类 12345678910111213141516171819202122232425package com.atguigu.mapreduce.wordcount;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class WordcountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123;int sum;IntWritable v = new IntWritable(); @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException &#123; // 1 累加求和 sum = 0; for (IntWritable count : values) &#123; sum += count.get(); &#125; // 2 输出 v.set(sum); context.write(key,v); &#125;&#125; 编写Driver驱动类 12345678910111213141516171819202122232425262728293031323334353637383940414243package com.atguigu.mapreduce.wordcount;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class WordcountDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1 获取配置信息以及封装任务 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2 设置jar加载路径 job.setJarByClass(WordcountDriver.class); // 3 设置map和reduce类 job.setMapperClass(WordcountMapper.class); job.setReducerClass(WordcountReducer.class); // 4 设置map输出 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 5 设置最终输出kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // 6 设置输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 提交 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 测试 直接在eclipse/Idea上测试 集群上测试 maven打jar包，需要添加打包插件依赖 12345678910111213141516171819202122232425262728293031323334&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.3.2&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin &lt;/artifactId&gt; &lt;configuration&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;com.atguigu.mr.WordcountDriver&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 如果工程显示红叉，项目右键-&gt;maven-&gt;update project即可 maven install，将jar包放到Hadoop集群 执行WordCount 12hadoop jar wc.jar com.atguigu.wordcount.WordcountDriver /user/atguigu/input /user/atguigu/output","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://kiedeng.github.io/categories/Hadoop/"},{"name":"Mapreduce","slug":"Hadoop/Mapreduce","permalink":"http://kiedeng.github.io/categories/Hadoop/Mapreduce/"}],"tags":[{"name":"案例","slug":"案例","permalink":"http://kiedeng.github.io/tags/%E6%A1%88%E4%BE%8B/"}]},{"title":"HDFS_2.X新特性","slug":"HDFS-2-X新特性","date":"2020-02-13T17:46:00.000Z","updated":"2020-02-14T17:34:24.151Z","comments":true,"path":"2020/02/14/HDFS-2-X新特性/","link":"","permalink":"http://kiedeng.github.io/2020/02/14/HDFS-2-X%E6%96%B0%E7%89%B9%E6%80%A7/","excerpt":"","text":"集群间的数据拷贝 scp实现两个远程主机之间的文件复制 ​ scp -r hello.txt root@hadoop103:/user/atguigu/hello.txt // 推 push ​ scp -r [root@hadoop103:/user/atguigu/hello.txt hello.txt](mailto:root@hadoop103:/user/atguigu/hello.txt hello.txt) // 拉 pull ​ scp -r root@hadoop103:/user/atguigu/hello.txt root@hadoop104:/user/atguigu //是通过本地主机中转实现两个远程主机的文件复制；如果在两个远程主机之间ssh没有配置的情况下可以使用该方式。 采用distcp命令实现两个Hadoop集群之间的递归数据复制 12 bin/hadoop distcphdfs://haoop102:9000/user/atguigu/hello.txt hdfs://hadoop103:9000/user/atguigu/hello.txt 小文件存档 弊端 ​ 每个文件按块存储，每个块的数据在NameNode的内存中，因此HDFS存储小文件会非常低效。因为大量小文件会用尽NameNode的大部分内存。 解决小文件的办法之一 ​ HDFS存档文件或Har文件，是一个更高效的文件存档工具。HDFS存档文件对内还是一个一个独立文件，对NameNode而言是一个整体，减少NameNode内存 实例 需要启动YARN进程 归档文件 ​ 把/user/atguigu/input目录里面的所有文件归档成一个叫input.har的归档文件，并把归档后文件存储到/user/atguigu/output路径下。 1bin/hadoop archive -archiveName input.har –p /user/atguigu/input /user/atguigu/output 查看归档 12hadoop fs -lsr /user/atguigu/output/input.harhadoop fs -lsr har:///user/atguigu/output/input.har 解归档文件 1hadoop fs -cp har:/// user/atguigu/output/input.har/* /user/atguigu 回收站 开启回收站功能参数说明 默认值fs.trash.interval=0,0表示禁用回收站 默认值fs.trash.checkpoint.interval=0,检查回收站的间隔时间。如果为0，则该值和fs.tarsh.interval的参数相等 要求fs.trash.checkpoint.interval&lt;=fs.trash.interval 启动回收站 修改core-site.xml,配置垃圾回收时间为1分钟 1234&lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt; 查看回收站 回收站在集群中的路径：/user/atguigu/.Trash/…. 修改访问垃圾回收站用户名称 进入垃圾回收站用户名称，默认是dr.who，修改为atguigu用户 1234&lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;atguigu&lt;/value&gt;&lt;/property&gt; 调用moveToTrash（）才进入回收站 12Trash trash = New Trash(conf);trash.moveToTrash(path); 恢复回收站数据 12hadoop fs -mv/user/atguigu/.Trash/Current/user/atguigu/input /user/atguigu/input 清空回收站 1hadoop fs -expunge 快照管理12345678hdfs dfsadmin -allowSnapshot 路径（功能描述：开启指定目录的快照功能）hdfs dfsadmin -disallowSnapshot 路径（功能描述：禁用指定目录的快照功能，默认是禁用）hdfs dfs -createSnapshot 路径（功能描述：对目录创建快照）hdfs dfs -createSnapshot 路径名称（功能描述：指定名称创建快照）hdfs dfs -renameSnapshot 略径旧名称新名称（功能描述：重命名快照）hdfs lsSnapshottableDir（功能描述：列出当前用户所有可快照目录）hdfs snapshotDiff 路径1 路径2（功能描述：比较两个快照目录的不同之处）hdfs dfs -deleteSnapshot&lt;path&gt;&lt;snapshotName&gt;（功能描述：删除快照） 开启/禁用指定目录的快照功能 12hdfs dfsadmin -allowSnapshot /user/atguigu/inputhdfs dfsadmin -disallowSnapshot /user/atguigu/input 对目录创建快照 12hdfs dfs -createSnapshot /user/atguigu/inputhdfs dfs -lsr /user/atguigu/input/.snapshot/ 指定名称创建快照 1hdfs dfs -createSnapshot /user/atguigu/input miao170508 快照重命名 1hdfs dfs -renameSnapshot /user/atguigu/input/ miao170508 atguigu170508 列出当前用户所有可快照目录 1hdfs lsSnapshottableDir 比较两个快照目录的不同之处 12hdfs snapshotDiff /user/atguigu/input/ . .snapshot/atguigu170508 恢复快照 1hdfs dfs -cp /user/atguigu/input/.snapshot/s20170708-134303.027 /user","categories":[{"name":"HDFS","slug":"HDFS","permalink":"http://kiedeng.github.io/categories/HDFS/"},{"name":"Hadoop","slug":"HDFS/Hadoop","permalink":"http://kiedeng.github.io/categories/HDFS/Hadoop/"}],"tags":[{"name":"原理","slug":"原理","permalink":"http://kiedeng.github.io/tags/%E5%8E%9F%E7%90%86/"}]},{"title":"NN与DataNode","slug":"NN与DataNode","date":"2020-02-13T17:21:19.000Z","updated":"2020-02-20T06:52:17.345Z","comments":true,"path":"2020/02/14/NN与DataNode/","link":"","permalink":"http://kiedeng.github.io/2020/02/14/NN%E4%B8%8EDataNode/","excerpt":"","text":"NN和2NN工作机制FsImage:磁盘中备份元数据的文件 Edits：每当元数据有更新或者添加元数据时，修改内存中元数据并追加到Edits，为防止该文件数据过大影响效率，因此需要定期进行FsImage和Edits合并，引入一个节点SecondaryNamenode,专门用于FsImage和Edits的合并 第一阶段：NameNode启动 第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。 客户端对元数据进行增删改的请求。 NameNode记录操作日志，更新滚动日志。 NameNode在内存中对数据进行增删改。 第二阶段：Secondary NameNode工作 Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。 Secondary NameNode请求执行CheckPoint。 NameNode滚动正在写的Edits日志。 将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。 Secondary NameNode加载编辑日志和镜像文件到内存，并合并。 生成新的镜像文件fsimage.chkpoint。 拷贝fsimage.chkpoint到NameNode。 NameNode将fsimage.chkpoint重新命名成fsimage。 *** opt/module/hadoop-2.7.2/data/tmp/dfs/name/current *** oiv查看Fsimage语法：hdfs oiv -p 文件类型 -i镜像文件 -o 转换后文件输出路径 1hdfs oiv -p XML -i fsimage_0000000000000000025 -o /opt/module/hadoop-2.7.2/fsimage.xml oev查看Edits文件语法：hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径 1hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-2.7.2/edits.xml CheckPoint时间设置 通常情况下，SecondaryNameNade每隔一个小时执行一次，在【hdfs-default.xml】中设置 12345&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;3600&lt;/value&gt;&lt;/property&gt; 一分钟检查一次操作次数，3当操作次数达到1百万时，SecondaryNameNode执行一次。 123456789101112&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.txns&lt;/name&gt; &lt;value&gt;1000000&lt;/value&gt;&lt;description&gt;操作动作次数&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.check.period&lt;/name&gt; &lt;value&gt;60&lt;/value&gt;&lt;description&gt; 1分钟检查一次操作次数&lt;/description&gt;&lt;/property &gt; NameNode故障处理方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录 kill -9 NameNode过程 删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name） 拷贝SecondaryNameNode中数据到原NameNode存储数据目录 1scp -r atguigu@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* ./name/ 重新启动NameNode 方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中。 修改hdfs-site.xml中的 12345678910&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;120&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp/dfs/name&lt;/value&gt;&lt;/property&gt; kill -9 NameNode进程 删除NameNode存储的数据 将2NN存储数据的目录拷贝到NN存储数据的评级目录，并删除in_use.lock文件 导入检查点数据 sbin/hadoop-daemon.sh start namenode 启动NameNode sbin/hadoop-daemon.sh start namenode 安全模式（1）bin/hdfs dfsadmin -safemode get （功能描述：查看安全模式状态） （2）bin/hdfs dfsadmin -safemode enter （功能描述：进入安全模式状态） （3）bin/hdfs dfsadmin -safemode leave （功能描述：离开安全模式状态） （4）bin/hdfs dfsadmin -safemode wait （功能描述：等待安全模式状态） DataNodeDataNode工作机制 一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。 DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。 心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。 集群运行中可以安全加入和退出一些机器。 保证DataNode的数据完整性1）当DataNode读取Block的时候，它会计算CheckSum。 2）如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。 3）Client读取其他DataNode上的Block。 4）DataNode在其文件创建后周期验证CheckSum 掉线时限参数设置 如果定义超时时间为TimeOut，则超时时长的计算公式为：TimeOut =2dfs.namenode.heartbeat.recheck-interval+10dfs.heartbeat interval。而默认的dfs.namenode.heartbeat.recheck-interval 大小为5分钟，dfs.heartbeatinterval默认为3秒。 配置位置：hdfs-site.xml，配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。 123456789&lt;property&gt; &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;/name&gt; &lt;value&gt;300000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.heartbeat.interval&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt; 服役新节点 环境准备 ​ （1）在hadoop104主机上再克隆一台hadoop105主机 ​ （2）修改IP地址和主机名称 ​ （3）删除原来HDFS文件系统留存的文件（/opt/module/hadoop-2.7.2/data和log） ​ （4）source一下配置文件 12sbin/hadoop-daemon.sh start datanodesbin/yarn-daemon.sh start nodemanager 添加白名单 在NameNode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts文件，将允许的集群IP写入 在NameNode的hdfs-site.xml配置文件中增加dfs.hosts属性 1234&lt;property&gt; &lt;name&gt;dfs.hosts&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts&lt;/value&gt;&lt;/property&gt; 配置文件分发 刷新namenode：hdfs dfsadmin -refreshNodes 更新ResourceManager节点：yarn rmadmin -refreshNodes 数据如果不平衡，则：start-balancer.sh 添加黑名单（不能同时有节点出现在两个中） 在NameNode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts.exclude文件 在NameNode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性 1234&lt;property&gt;&lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude&lt;/value&gt;&lt;/property&gt; 刷新namenode和Resourcemanager Datanode多目录配置 DataNode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本 hdfs-site.xml 1234&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;&lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2&lt;/value&gt;&lt;/property&gt;","categories":[{"name":"HDFS","slug":"HDFS","permalink":"http://kiedeng.github.io/categories/HDFS/"},{"name":"Hadoop","slug":"HDFS/Hadoop","permalink":"http://kiedeng.github.io/categories/HDFS/Hadoop/"}],"tags":[{"name":"原理","slug":"原理","permalink":"http://kiedeng.github.io/tags/%E5%8E%9F%E7%90%86/"}]},{"title":"HDFS的数据流","slug":"HDFS的数据流","date":"2020-02-13T10:03:05.000Z","updated":"2020-02-13T17:20:15.945Z","comments":true,"path":"2020/02/13/HDFS的数据流/","link":"","permalink":"http://kiedeng.github.io/2020/02/13/HDFS%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B5%81/","excerpt":"","text":"HDFS写数据流程 客户端通过Distributed FileSysetm模块向NameNode请求上传文件，NameNode检查文件是否已存在，父目录是否存在 NameNode返回是否可以上传 客户端请求第一个Block上传哪几个DataNode服务器上 NameNode返回三个DataNode节点，分别为dn1，dn2，dn3 客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成 dn1，dn2，dn3逐级应答客户端 客户端开始往dn1上传第一个Block（先从磁盘读取数据收到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传个dn3，dn1没传一个packet会放入一个应答队列等待应答 当一个Bloc传输完成之后，客户端再次请求NameNode上传第二个Block服务器 节点距离：两个节点到达最近的共同祖先的距离总和。机架感知 对于常见情况，当复制因子为3时，HDFS的放置策略是将一个副本放在本地机架中的一个节点上，另一个放在本地机架中的另一个节点上，最后一个放在不同机架中的另一个节点上 HDFS读数据流程 客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。 挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据 DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。 客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。","categories":[{"name":"HDFS","slug":"HDFS","permalink":"http://kiedeng.github.io/categories/HDFS/"},{"name":"Hadoop","slug":"HDFS/Hadoop","permalink":"http://kiedeng.github.io/categories/HDFS/Hadoop/"}],"tags":[{"name":"原理","slug":"原理","permalink":"http://kiedeng.github.io/tags/%E5%8E%9F%E7%90%86/"}]},{"title":"HDFS客户端操作","slug":"HDFS客户端操作","date":"2020-02-13T03:16:18.000Z","updated":"2020-02-28T02:03:37.072Z","comments":true,"path":"2020/02/13/HDFS客户端操作/","link":"","permalink":"http://kiedeng.github.io/2020/02/13/HDFS%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%93%8D%E4%BD%9C/","excerpt":"","text":"客户端环境准备1 将编译hadoop jar包放在路径下（比如：D:\\environment\\hadoop-2.7.2）2 配置HADOOP_HOME环境变量 HADOOP_HOME : D:\\environment\\hadoop-2.7.2 3 配置Path环境变量 %HADOOP_HOME%\\bin 4 创建一个Maven工程5 导入相应的依赖坐标+日志添加123456789101112131415161718192021222324252627282930313233343536373839404142&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.atguigu&lt;/groupId&gt; &lt;artifactId&gt;HDFS-0529&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;jdk.tools&lt;/groupId&gt; &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;JAVA_HOME&#125;/lib/tools.jar&lt;/systemPath&gt; &lt;/dependency&gt;&lt;/dependencies&gt; &lt;/project&gt; 需要再项目的src/main/resources目录下，新建一个文件，命令为”log4j.properties”,在文件中填写 log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n log4j.appender.logfile=org.apache.log4j.FileAppender log4j.appender.logfile.File=target/spring.log log4j.appender.logfile.layout=org.apache.log4j.PatternLayout log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 6 创建包名：com.atguigu.hdfs7 创建HdfsClient类12345678910111213141516171819public class HdfsClient&#123; @Testpublic void testMkdirs() throws IOException, InterruptedException, URISyntaxException&#123; // 1 获取文件系统 Configuration configuration = new Configuration(); // 配置在集群上运行 // configuration.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop102:9000&quot;); // FileSystem fs = FileSystem.get(configuration); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;atguigu&quot;); // 2 创建目录 fs.mkdirs(new Path(&quot;/1108/daxian/banzhang&quot;)); // 3 关闭资源 fs.close(); &#125;&#125; 8 执行程序 客户端去操作HDFS时，是有一个用户身份的。 默认情况下，HDFS客户端API会从JVM中获取一个参数来作为自己的用户身份：-DHADOOP_USER_NAME=atguigu，atguigu为用户名称。 1234567891011public static void main(String[] args) throws Exception &#123; // 配置文件 Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop102:9000&quot;); // 获取客户端对象 FileSystem fs = FileSystem.get(conf); fs.mkdirs(new Path(&quot;/kangdong&quot;)); // 关闭客户端 fs.close(); System.out.println(&quot;Over!&quot;); &#125; HDFS的API操作HDFS文件上传（测试参数优先级）1 编写源代码1234567891011public static void main(String[] args) throws Exception &#123; // 配置文件 Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop102:9000&quot;); // 获取客户端对象 FileSystem fs = FileSystem.get(conf); fs.mkdirs(new Path(&quot;/kangdong&quot;)); // 关闭客户端 fs.close(); System.out.println(&quot;Over!&quot;); &#125; 2 将hdfs-site.xml拷贝到项目的根目录（即资源目录下）12345678910&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3 参数优先级 参数优先级排序：（1）客户端代码中设置的值 &gt;（2）ClassPath下的用户自定义配置文件 &gt;（3）然后是服务器的默认配置 HDFS文件的下载123456// boolean delSrc 指是否将原文件删除// Path src 指要下载的文件路径// Path dst 指将文件下载到的路径// boolean useRawLocalFileSystem 是否开启文件校验fs.copyToLocalFile(false, new Path(&quot;/banzhang.txt&quot;), new Path(&quot;e:/banhua.txt&quot;), true); HDFS文件的删除1fs.delete(new Path(&quot;/0508/&quot;), true);//第一参数是路径，第二个参数是判断是否递归删除 HDFS文件的改名1fs.rename(new Path(&quot;/banzhang.txt&quot;), new Path(&quot;/banhua.txt&quot;)); HDFS文件详情的查看123456789101112131415161718192021222324252627282930313233343536373839404142@Testpublic void testListFiles() throws IOException, InterruptedException, URISyntaxException&#123; // 1获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;atguigu&quot;); // 2 获取文件详情 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path(&quot;/&quot;), true); while(listFiles.hasNext())&#123; LocatedFileStatus status = listFiles.next(); // 输出详情 // 文件名称 System.out.println(status.getPath().getName()); // 长度 System.out.println(status.getLen()); // 权限 System.out.println(status.getPermission()); // 分组 System.out.println(status.getGroup()); // 获取存储的块信息 BlockLocation[] blockLocations = status.getBlockLocations(); for (BlockLocation blockLocation : blockLocations) &#123; // 获取块存储的主机节点 String[] hosts = blockLocation.getHosts(); for (String host : hosts) &#123; System.out.println(host); &#125; &#125; System.out.println(&quot;-----------班长的分割线----------&quot;); &#125;// 3 关闭资源fs.close();&#125; HDFS的I/O流操作HDFS文件上传12345678910111213@Test public void IOcopyFromLocal() throws Exception&#123; Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), conf, &quot;atguigu&quot;); FileInputStream fis = new FileInputStream(new File(&quot;d:/kiedeng.txt&quot;)); FSDataOutputStream fout = fs.create(new Path(&quot;/kg.txt&quot;)); IOUtils.copyBytes(fis, fout, conf); IOUtils.closeStream(fis); IOUtils.closeStream(fout); fs.close(); &#125; HDFS文件下载1234567891011121314151617181920212223// 文件下载@Testpublic void getFileFromHDFS() throws IOException, InterruptedException, URISyntaxException&#123; // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;atguigu&quot;); // 2 获取输入流 FSDataInputStream fis = fs.open(new Path(&quot;/banhua.txt&quot;)); // 3 获取输出流 FileOutputStream fos = new FileOutputStream(new File(&quot;e:/banhua.txt&quot;)); // 4 流的对拷 IOUtils.copyBytes(fis, fos, configuration); // 5 关闭资源 IOUtils.closeStream(fos); IOUtils.closeStream(fis); fs.close();&#125; 定位文件读取 下载第一块 123456789101112131415161718192021222324252627@Testpublic void readFileSeek1() throws IOException, InterruptedException, URISyntaxException&#123; // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;atguigu&quot;); // 2 获取输入流 FSDataInputStream fis = fs.open(new Path(&quot;/hadoop-2.7.2.tar.gz&quot;)); // 3 创建输出流 FileOutputStream fos = new FileOutputStream(new File(&quot;e:/hadoop-2.7.2.tar.gz.part1&quot;)); // 4 流的拷贝 byte[] buf = new byte[1024]; for(int i =0 ; i &lt; 1024 * 128; i++)&#123; fis.read(buf); fos.write(buf); &#125; // 5关闭资源 IOUtils.closeStream(fis); IOUtils.closeStream(fos);fs.close();&#125; 下载第二块 123456789101112131415161718192021222324@Testpublic void readFileSeek2() throws IOException, InterruptedException, URISyntaxException&#123; // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;atguigu&quot;); // 2 打开输入流 FSDataInputStream fis = fs.open(new Path(&quot;/hadoop-2.7.2.tar.gz&quot;)); // 3 定位输入数据位置 fis.seek(1024*1024*128); // 4 创建输出流 FileOutputStream fos = new FileOutputStream(new File(&quot;e:/hadoop-2.7.2.tar.gz.part2&quot;)); // 5 流的对拷 IOUtils.copyBytes(fis, fos, configuration); // 6 关闭资源 IOUtils.closeStream(fis); IOUtils.closeStream(fos);&#125; 合并文件 ​ 在Window命令窗口中进入到目录E:\\，然后执行如下命令，对数据进行合并 ​ type hadoop-2.7.2.tar.gz.part2 &gt;&gt; hadoop-2.7.2.tar.gz.part1 合并完成后，将hadoop-2.7.2.tar.gz.part1重新命名为hadoop-2.7.2.tar.gz。解压发现该tar包非常完整。","categories":[{"name":"HDFS","slug":"HDFS","permalink":"http://kiedeng.github.io/categories/HDFS/"},{"name":"Hadoop","slug":"HDFS/Hadoop","permalink":"http://kiedeng.github.io/categories/HDFS/Hadoop/"}],"tags":[{"name":"代码","slug":"代码","permalink":"http://kiedeng.github.io/tags/%E4%BB%A3%E7%A0%81/"}]},{"title":"Hadoop编译源码","slug":"Hadoop编译源码","date":"2020-02-12T03:29:53.000Z","updated":"2020-02-28T00:55:28.921Z","comments":true,"path":"2020/02/12/Hadoop编译源码/","link":"","permalink":"http://kiedeng.github.io/2020/02/12/Hadoop%E7%BC%96%E8%AF%91%E6%BA%90%E7%A0%81/","excerpt":"","text":"jar包准备 hadoop-2.7.2-src.tar.gz jdk-8u144-linux-x64.tar.gz apache-ant-1.9.9-bin.tar.gz（build工具，打包用的） apache-maven-3.0.5-bin.tar.gz protobuf-2.5.0.tar.gz（序列化的框架） jar包安装1 JDK解压，配置环境变量JAVA_HOME和PATH #JAVA_HOME： export JAVA_HOME=/opt/module/jdk1.8.0_144 export PATH=$PATH:$JAVA_HOME/bin source /etc/profile进行生效 2 Maven解压，配置MAVEN_HOME和PATH [root@hadoop101 software]# tar -zxvf apache-maven-3.0.5-bin.tar.gz -C /opt/module/ [root@hadoop101 apache-maven-3.0.5]# vi conf/settings.xml ​ ​ nexus-aliyun ​ central ​ Nexus aliyun ​ http://maven.aliyun.com/nexus/content/groups/public ​ [root@hadoop101 apache-maven-3.0.5]# vi /etc/profile #MAVEN_HOME export MAVEN_HOME=/opt/module/apache-maven-3.0.5 export PATH=$PATH:$MAVEN_HOME/bin [root@hadoop101 software]#source /etc/profile 验证命令：mvn-version 2.1 ant解压、配置 ANT _HOME和PATH [root@hadoop101 software]# tar -zxvf apache-ant-1.9.9-bin.tar.gz -C /opt/module/ [root@hadoop101 apache-ant-1.9.9]# vi /etc/profile #ANT_HOME export ANT_HOME=/opt/module/apache-ant-1.9.9 export PATH=$PATH:$ANT_HOME/bi [root@hadoop101 software]#source /etc/profile 2.2 安装 glibc-headers 和 g++ 命令如下 [root@hadoop101 apache-ant-1.9.9]# yum install glibc-headers [root@hadoop101 apache-ant-1.9.9]# yum install gcc-c++ 2.3 安装make和cmake [root@hadoop101 apache-ant-1.9.9]# yum install make [root@hadoop101 apache-ant-1.9.9]# yum install cmake 2.4 解压protobuf ，进入到解压后protobuf主目录，/opt/module/protobuf-2.5.0，然后相继执行命令 [root@hadoop101 software]# tar -zxvf protobuf-2.5.0.tar.gz -C /opt/module/ [root@hadoop101 opt]# cd /opt/module/protobuf-2.5.0/ [root@hadoop101 protobuf-2.5.0]#./configure [root@hadoop101 protobuf-2.5.0]# make [root@hadoop101 protobuf-2.5.0]# make check [root@hadoop101 protobuf-2.5.0]# make install [root@hadoop101 protobuf-2.5.0]# ldconfig [root@hadoop101 hadoop-dist]# vi /etc/profile #LD_LIBRARY_PATH export LD_LIBRARY_PATH=/opt/module/protobuf-2.5.0 export PATH=$PATH:$LD_LIBRARY_PATH [root@hadoop101 software]#source /etc/profile 2.5 安装openssl库 [root@hadoop101 software]#yum install openssl-devel 2.6 安装 ncurses-devel库 [root@hadoop101 software]#yum install ncurses-devel 编译源码3.1 解压源码到/opt/目录 tar -zxvf hadoop-2.7.2-src.tar.gz -C /opt/ 3.2 进入源码主目录3.3 通过maven执行编译命令 mvn package -Pdist,native -DskipTests -Dtar 3.4 成功的64位hadoop包在/opt/hadoop-2.7.2-src/hadoop-dist/target下","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://kiedeng.github.io/categories/Hadoop/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"软件推荐","slug":"软件推荐","date":"2020-02-11T10:48:17.000Z","updated":"2020-02-11T11:25:52.268Z","comments":true,"path":"2020/02/11/软件推荐/","link":"","permalink":"http://kiedeng.github.io/2020/02/11/%E8%BD%AF%E4%BB%B6%E6%8E%A8%E8%8D%90/","excerpt":"","text":"TinyTask自动化操作工具 Ditto复制神器 SecureCRT一般大数据使用的远程工具 PanDownload网盘下载 天若OCR文字识别用于文字识别 Snipaste截图贴图工具 Typoramarkdown编辑器 Notepad++编辑器，可以进行文件夹文字查找 Sublime编辑器，可以远程服务器 Bandizip解压工具 VMware虚拟机 Xshell/Xftp远程连接工具 Adobe Acrobat DCpdf阅读，编辑 Anki记忆工具 Everything查看文件 PotPlayer播放器 Chrome浏览器","categories":[{"name":"工具","slug":"工具","permalink":"http://kiedeng.github.io/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"自动化","slug":"自动化","permalink":"http://kiedeng.github.io/tags/%E8%87%AA%E5%8A%A8%E5%8C%96/"}]},{"title":"Linux使用手册","slug":"Linux使用手册","date":"2020-02-10T08:57:11.000Z","updated":"2020-02-11T04:56:13.605Z","comments":true,"path":"2020/02/10/Linux使用手册/","link":"","permalink":"http://kiedeng.github.io/2020/02/10/Linux%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/","excerpt":"","text":"用户管理命令 #添加 useradd kiedeng 或者 useradd -g [组名] 用户名 #设置密码 useradd 用户名 #查看用户是否存在 id 用户名 #查看创建的用户 cat /etc/passwd #切换用户 su 用户名（没有获得环境变量） su - 用户名 （获得环境变量） #删除用户 userdel 用户名 userdel -r 用户名 （用户和用户目录全部删除） #设置root权限 vim /etc/sudoers 在root下中添加一行 用户名 ALL=(ALL) ALL 用户名 ALL=(ALL) NOPASSWDALL （不需要密码） 修改usermod 修改用户 usemod -g 用户组 用户名 用户组管理 添加：groupadd 组名 删除：groupdel 组名 修改组：groupmod -n 新组名 老组名 查看组：cat /etc/group 文件权限类 总共10位：0~9 0位：-表示文件，d代表文件，l代表链接文档 1-3位确定属主的该文件权限 4-6位确定属组的该文件的权限 7-9位确定其他用户改文件的权限 修改文件权限 chmod 777 a.txt chmod -R 777 xiyou （递归删除） 改变所有者 chown [选项] [最终用户] [文件或者目录] 选项为-R （递归操作） 改变所属组 chgrp [最终用户组] [文件或者目录] 搜索查找类find 查找文件或者目录 基本语法：find [搜索范围] [选项] 选项 -name&lt;查询方式&gt; 按照指定文件名查找 -user&lt;用户名&gt; 指定用户查找 -size&lt;文件大小&gt; 按照文件大小查找 （+为大于，-为小于） 比如：find /home -size +20458 locate快速定位文件路径 更新：updatedb 基本语法：locate 搜索文件 grep 过滤查找及“|”管道符 基本语法：grep 选项 查找内容 源文件 压缩和解压缩gzip/gunzip压缩 只能压缩文件，不能压缩目录 命令：gzip 文件；gunzip 文件. gz zip/unzip压缩 基本语法： zip [选项] xxx.zip 将要压缩的内容 （目录或文件） unzip [选项] xxx.zip 解压文件 选项：-d&lt;目录&gt; 指定压缩后文件存放的目录 tar 打包 tar [选项] xxx.tar.gz 将要打包进去的内容 选项 -c 产生tar打包文件 -v 显示详细信息 -f 指定压缩后的文件名 -z 打包同时压缩 -x 解包.tar文件 压缩：tar -zcvf kie.tar.gz a.txt b.txt 解压：tar -zxvf kie.tar.gz -C /opt 磁盘分区类","categories":[{"name":"使用手册","slug":"使用手册","permalink":"http://kiedeng.github.io/categories/%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kiedeng.github.io/tags/Linux/"}]},{"title":"hadoop集群搭建","slug":"hadoop集群搭建","date":"2020-02-10T06:11:15.000Z","updated":"2020-05-16T10:31:43.300Z","comments":true,"path":"2020/02/10/hadoop集群搭建/","link":"","permalink":"http://kiedeng.github.io/2020/02/10/hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/","excerpt":"","text":"克隆虚拟机 配置好的Linux虚拟机-&gt; 管理 -&gt; 克隆 1 删除网卡，复制物理地址：vim /etc/udev/rules.d/70-persistent-net.rules 1删除eht0的那一行，将下一行的eth0改为eth1 2 配置网络：vim /etc/sysconfig/network-scripts/ifcfg-eth0 (删除UUID) 123456IPADDR=192.168.1.101 设置ipONBOOT=yesNM_CONTROLLED=yesB00TPROTO=staticGATEWAY=192.168.1.2DNS1=192.168.1.2 和网关一致 3 修改主机名称：vim /etc/sysconfig/network 12NETWORKING=yesHOSTNAME=hadoopl 01 4 修改主机映射：vim /etc/hosts 5 windows的映射：C:\\Windows\\System32\\drivers\\etc 安装JDK1 通过sftp将jdk放在/opt/software下1tar -zxvf [gz文件名] -C [解压的路径] 2 设置环境路径123#JAVA_HOMEexport JAVA_HOME=/opt/module/jdk1.8.0_144export PATH=$PATH:$JAVA_HOME/bin 3 让修改以后的文件生效1source /etc/profile 安装Hadoop 将hadoop-2.7.2.tar.gz传入，解压 在/etc/profile添加环境变量 1234##HADOOP_HOMEexport HADOOP_HOME=/opt/module/hadoop-2.7.2export PATH=$PATH:$HADOOP_HOME/binexport PATH=$PATH:$HADOOP_HOME/sbin 生效文件：source /etc/profile Hadoop的目录结构 bin目录：存放对Hadoop相关服务（HDFS和YARN）进行操作的脚本 etc目录：Hadoop的配置目录，存放Hadoop的配置文件 lib目录：存放Hadoop的本地库（对数据进行压缩解压功能） sbin目录：存放启动或者停止Hadoop相关服务的脚本 share目录：存放hadoop的jar包，官方文档和案例 Hadoop的运行模式​ 本地模式，伪分布式模式，完成分布式模式 一 本地模式官方Grep案例 在hadoop-2.7.2文件下面创建一个input文件夹 复制文件到input文件夹里面 执行share文件夹下的MapReduce程序 1bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output &#x27;dfs[a-z.]+&#x27; 查看输出结果 1cat output/* 官方WordCount案例（计算单词个数） 创建文件夹（wcinput） 在该文件夹创建并编辑文件内容 执行 1hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount wcinput wcoutput 查看结果 1hadoop-2.7.2]$ cat wcoutput/part-r-00000 二 伪分布式运行模式启动HDFS并运行MapReduce程序1 配置集群 hadoop-env.sh(添加jdk路径) 1export JAVA_HOME=/opt/module/jdk1.8.0_144 core-site.xml 123456789101112131415161718192021&lt;!-- 指定HDFS中NameNode的地址 --&gt;&lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt;&lt;value&gt;hdfs://hadoop101:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;&lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;&lt;/property&gt; hdfs-site.xml 123456789&lt;!-- 指定HDFS副本的数量 --&gt;&lt;property&gt;&lt;name&gt;dfs.replication&lt;/name&gt;&lt;value&gt;1&lt;/value&gt;&lt;/property&gt; 2 启动集群 格式化NameNode（第一次启动时格式化，以后就不要格式化 bin/hdfs namenode -format 启动NameNode sbin/hadoop-daemon.sh start namenode 启动DataNode sbin/hadoop-daemon.sh start datanode 3 查看集群 web访问默认端口：50070 打开失败：https://www.cnblogs.com/zlslch/p/6604189.html log日志：/opt/module/hadoop-2.7.2/logs 不能一直格式化NameNode原因 会产生新的集群id，导致NameNode和DataNode的集群id不一致，集群找不到数据，所以格式化前先删除data和log日志 4 操作集群 （wordcount）执行操作和本地模式一样 启动YARN并运行MapReduce程序1 配置集群 yarn-env.sh(添加路径) 1export JAVA_HOME=/opt/module/jdk1.8.0_144 配置yarn-site.xml 123456789101112131415161718192021&lt;!-- Reducer获取数据的方式 --&gt;&lt;property&gt;&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;&lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定YARN的ResourceManager的地址 --&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;&lt;value&gt;hadoop101&lt;/value&gt;&lt;/property&gt; mapred-env.sh(添加jdk路径) (对mapred-site.xml.template重新命名为) mapred-site.xml 123456789&lt;!-- 指定MR运行在YARN上 --&gt;&lt;property&gt;&lt;name&gt;mapreduce.framework.name&lt;/name&gt;&lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; 2 启动集群 启动前必须先启动NameNode和DataNode 启动ResourceManager 1sbin/yarn-daemon.sh start resourcemanager 启动NodeManager 1sbin/yarn-daemon.sh start nodemanager 3 集群操作 yarn访问端口：8088 删除文件系统的ouput文件 bin/hdfs dfs -rm -R /user/kiedong/output 执行MapReduce程序 bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/atguigu/input /user/atguigu/output 配置历史服务器 配置mapred-site.xml,添加 12345678910111213141516171819&lt;!-- 历史服务器端地址 --&gt;&lt;property&gt;&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;&lt;value&gt;hadoop101:10020&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史服务器web端地址 --&gt;&lt;property&gt;&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;&lt;value&gt;hadoop101:19888&lt;/value&gt;&lt;/property&gt; 启动历史服务器 sbin/mr-jobhistory-daemon.sh start historyserver 端口：19888 配置日志的聚集日志聚集的概念：应用程序运行完成以后，程序运行日志上传到HDFS系统上 日志聚集的好处：方便开发调试 注意：开启此功能，需要重新启动NodeManager，ResourceManager和HistoryManager yarn-site.xml 123456789101112131415161718192021&lt;!-- 日志聚集功能使能 --&gt;&lt;property&gt;&lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 日志保留时间设置7天 --&gt;&lt;property&gt;&lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;&lt;value&gt;604800&lt;/value&gt;&lt;/property&gt; 关闭,然后NodeManager 、ResourceManager和HistoryManager 执行WordCount案例 查看日志端口：19888 三 完全分布式运行模式（重点）主要步骤： 准备三台客户机（关闭防火墙，静态ip，主机名称） 安装JDK 配置环境变量 安装Hadoop 配置环境变量 配置集群 单点启动 配置ssh 群起并测试集群 编写集群分发脚本xsync scp（secure copy）安全拷贝 scp可以实现服务器与服务器之间的数据拷贝 语法:将hadoop101传给hadoop102 scp -r /opt/module root@hadoop102:/opt/module rsync远程同步工具 主要用于备份和镜像。 与scp区别：rsync只对差异性文件做更新。scp是复制所有文件 rsync -rvl /opt/software root@hadoop102:/opt/software -r 递归 -v显示复制过程 -l拷贝符号连接 xsync集群分发脚本（本集群使用） 需求：循环复制文件到所有节点的相同目录下 在home/用户名/bin这个目录下存放脚本，这样次用户可以在系统任何地方直接执行 编写代码：vim xsync 12345678910111213141516171819202122232425#!/bin/bash#1 获取输入参数个数，如果没有参数，直接退出pcount=$#if((pcount==0)); thenecho no args;exit;fi#2 获取文件名称p1=$1fname=`basename $p1`echo fname=$fname#3 获取上级目录到绝对路径pdir=`cd -P $(dirname $p1); pwd`echo pdir=$pdir#4 获取当前用户名称user=`whoami`#5 循环for((host=103; host&lt;105; host++)); do echo ------------------- hadoop$host -------------- rsync -rvl $pdir/$fname $user@hadoop$host:$pdirdone 修改脚本xsync具有执行权限 chomod 777 xsync 配置集群 核心配置文件 core-site.xml 123456789101112131415161718192021&lt;!-- 指定HDFS中NameNode的地址 --&gt;&lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt;&lt;value&gt;hdfs://hadoop102:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;&lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;&lt;/property&gt; HDFS配置文件 hadoop-en.sh：配置环境变量 hdfs-site.xml 1234567891011121314151617&lt;property&gt;&lt;name&gt;dfs.replication&lt;/name&gt;&lt;value&gt;3&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;&lt;value&gt;hadoop104:50090&lt;/value&gt;&lt;/property&gt; YARN配置文件 yarn-env.sh：配置环境变量 yarn-site.xml 123456789101112131415161718192021&lt;!-- Reducer获取数据的方式 --&gt;&lt;property&gt;&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;&lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定YARN的ResourceManager的地址 --&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;&lt;value&gt;hadoop103&lt;/value&gt;&lt;/property&gt; MapReduce配置文件 mapred-env.sh：配置环境变量 配置mapred-env.sh 1234567891011cp mapred-site.xml.template mapred-site.xml&lt;!-- 指定MR运行在Yarn上 --&gt;&lt;property&gt;&lt;name&gt;mapreduce.framework.name&lt;/name&gt;&lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; 集群单点启动 第一次启动，需要格式化NameNode hadoop namenode -format 在hadoop102上启动NameNode hadoop-daemon.sh start namenode 在hadoop102,103,104分别启动DataNode ssh无密码登陆配置 原理 将A服务器生成的公钥拷贝给B服务器，当A将数据用私钥A加密， 则B用A的公钥解密，并将数据用公钥加密给A 生成公钥和私钥 ssh-keygen -t rsa 将公钥拷贝到免密的目标机器上 ssh-copy-id hadoop102 ssh-copy-id hadoop103 ssh-copy-id hadoop104 注意：root用户需要再进行一次拷贝，在hadoop103生成公私钥拷贝给其他机器 .ssh文件夹下文件功能解释 known_hosts:记录ssh访问过计算机的公钥 id_rsa : 生成的私钥 id_rsa.pub:生成的公钥 authorized_keys ： 存放授权过得无密登录服务器公钥 群起集群 配置slaves，并同步slaves hadoop102 hadoop103 hadoop104 启动集群 第一次先格式化NameNode（先停止以前运行的namenode和datanode，然后删除data和log数据） 启动 sbin/start-dfs.sh (hadoop102上) sbin/start-yarn.sh（hadoop03上）","categories":[{"name":"部署","slug":"部署","permalink":"http://kiedeng.github.io/categories/%E9%83%A8%E7%BD%B2/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://kiedeng.github.io/tags/Hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kiedeng.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"Linux安装与配置","slug":"Linux安装与配置","date":"2020-02-10T05:35:12.000Z","updated":"2020-02-28T02:13:53.516Z","comments":true,"path":"2020/02/10/Linux安装与配置/","link":"","permalink":"http://kiedeng.github.io/2020/02/10/Linux%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/","excerpt":"","text":"安装时注意事项(用VMware安装centos[大数据虚拟机]) 检查BIOS虚拟化支持 内存默认设置为2048MB 最大磁盘大小默认为20GB 是否对CD媒体进行测试，直接跳过Skip 创建自定义分区(都是标准分区) 123boot 默认： 100MBwap 默认：2048MB/ 默认：15360 自定义系统软件 1234基本系统：兼容程序库；基本应用程序：互联网浏览器桌面：除了KDE,其余都选语言支持：中文支持 Kdump去掉 查看网络IP和网关编辑 -&gt; 虚拟网络编辑器 -&gt; NAT模式 即可看到子网IP NET设置可以看到网关 设置IP自动获取登录后，通过界面来设置自动获取 指定固定ip​ 直接修改配置文件来指定IP,并可以连接到外网(程序员推荐)，编辑 vi /etc/sysconfig/network-scripts/ifcfg-eth0​ 要求：将ip地址配置的静态的，ip地址为192.168.xxx.xxx 123456789101112131415DEVICE=eth0 #接口名（设备,网卡）HWADDR=00:0C:2x:6x:0x:xx #MAC地址 TYPE=Ethernet #网络类型（通常是Ethemet）UUID=926a57ba-92c6-4231-bacb-f27e5e6a9f44 #随机id#系统启动的时候网络接口是否有效（yes/no）ONBOOT=yes # IP的配置方法[none|static|bootp|dhcp]（引导时不使用协议|静态分配IP|BOOTP协议|DHCP协议）BOOTPROTO=static #IP地址IPADDR=192.168.189.130 #网关 GATEWAY=192.168.189.2 #域名解析器DNS1=192.168.189.2 重启网络服务或者重启系统生效：service network restart 、reboot 修改主机名查看当前主机名：hostname 修改主机名：/etc/hostname 修改主机映射文件：vim /etc/sysconfig/network 1234NETWORKING=yesNETWORKING_IPV6=noHOSTNAME= hadoop //写入新的主机名注意：主机名称不要有“_”下划线 修改ip与主机的映射：/etc/hosts 1192.168.102.130 hadoop Windows设置本地dns解析 C:\\Windows\\System32\\drivers\\etc\\hosts 添加内容：192.168.102.130 hadoop","categories":[{"name":"部署","slug":"部署","permalink":"http://kiedeng.github.io/categories/%E9%83%A8%E7%BD%B2/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kiedeng.github.io/tags/Linux/"},{"name":"VMware","slug":"VMware","permalink":"http://kiedeng.github.io/tags/VMware/"},{"name":"centos","slug":"centos","permalink":"http://kiedeng.github.io/tags/centos/"}]},{"title":"git的使用","slug":"git的使用","date":"2020-02-10T03:44:17.000Z","updated":"2020-02-10T05:13:33.539Z","comments":true,"path":"2020/02/10/git的使用/","link":"","permalink":"http://kiedeng.github.io/2020/02/10/git%E7%9A%84%E4%BD%BF%E7%94%A8/","excerpt":"","text":"Git的优势： 大部分操作在本地完成，不需要联网 完整性保证 尽可能添加数据而不是删除或修改数据 分支操作非常快捷流畅 与Linux命令全面兼容 代码托管中心 代码托管中心的任务：维护远程库 局域网环境：GitLab服务器 外网环境：GitHub，码云 Git命令行操作 本库初始化（选择文件夹进行初始化） 1git init 设置签名 作用：区分不同开发者的身份 辨析：这里设置的签名和登录远程库(代码托管中心)的账号、密码没有任何关系。 命令： 123456789项目级别/仓库级别：仅在当前本地库范围内有效git config user.name kiedenggit config user.email kiedeng@qq.com信息保存位置：./.git/config 文件系统用户级别：登录当前操作系统的用户范围git config --global user.name tom_glbgit config --global goodMorning_pro@atguigu.com信息保存位置：~/.gitconfig 文件 基本操作 123456789101112# 状态查看git status# 添加 (将工作区的文件或目录提交到暂存区)git [filename]# 提交 (将暂存区的文件提交的本地库)git commit -m &quot;commit message&quot; [filename]# 查看历史版本git loggit reflog# 版本的前进与后退（基于索引值操作）git reset --hard [局部索引值]git reset --hard a6ace91 分支管理 分支：在版本控制过程中，使用多条线同时推进多个任务。 12345678# 创建分支git branch [分支名]# 查看分支git branch -v# 切换分支git checkout [分支名]# 合并git merge [被和并的分支名] GitHub123456789# 查看所有远程地址别名git remote -v# 创建远程库地址别名git remote add [别名] [远程地址]# 推送 (将本地库上传到github仓库)git push [别名] [分支名]# 克隆(这样克隆：把远程库下载到本地，初始化本地库，创建别名)git origin [远程地址]","categories":[{"name":"使用手册","slug":"使用手册","permalink":"http://kiedeng.github.io/categories/%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://kiedeng.github.io/tags/Git/"},{"name":"GitHub","slug":"GitHub","permalink":"http://kiedeng.github.io/tags/GitHub/"}]},{"title":"hexo搭建","slug":"hexo搭建","date":"2020-02-10T01:10:23.000Z","updated":"2020-02-28T02:07:12.795Z","comments":true,"path":"2020/02/10/hexo搭建/","link":"","permalink":"http://kiedeng.github.io/2020/02/10/hexo%E6%90%AD%E5%BB%BA/","excerpt":"","text":"官方文档： 链接 一，使用Windows完成本地部署1 安装node.js和git,默认安装方式即可2 安装hexo，打开cmd执行1npm install hexo-cli -g 3 cmd移动到选择的一个文件夹，比如：d:\\blog(下面全部假设初始化在本路径),进行hexo的初始化1hexo init blog 4 在此目录安装1npm install 5 启动服务器，访问的默认地址：http://localhost:4000/1hexo s 二，使用GitHub完成远程部署1 注册，登录github2 新建仓库步骤如下： 点击右上角+号，new repository，在Repository name处填 （你的gitusername）.github.io（比如：kiedeng.github.io），然后直接点Create repository 3 在你初始化的路径（比如的d:\\blog）下有一个_config.xml,用记事本打开此文件，最后几行添加github信息12345(对于repo，比如：https://github.com/kiedeng/kiedeng.github.io.git)deploy:type: gitrepo: https://github.com/( yours username)/（your username）.github.io.gitbranck: master 4 将cmd移动到d:\\blog下，安装1npm install hexo-deployer-git --save 5 执行123456# 清理hexo clean# 生成静态文件hexo generate# 上传hexo deploy 在弹出的git窗口中输入你的GitHub邮箱和密码 部署完成，等待一会，使用比如：http://kiedeng.github.io/访问 三，更换hexo主题 找到hexo的主题 推荐主题：链接 选择一款，到达它们的github仓库（如果该主题作者的有文档，按文档即可完成更换） 将该主题下载下来（克隆也行），解压到d:\\blog\\themes,将该文件目录更名，比如：kiedeng 打开d:\\blog_config.xml,将theme: 后面的参数改为 1theme: kiedeng 然后就可以部署和上传了 四，绑定域名 选择一个合适的域名，买下域名 在域名的详细界面，打开解析 设置dns解析 在d:\\blog\\source目录下，新建一个叫CNAME的文件（强调：不能有后缀），里面的内容为你的域名，比如:www.kangdong.store 等待一小会即可进行访问","categories":[{"name":"部署","slug":"部署","permalink":"http://kiedeng.github.io/categories/%E9%83%A8%E7%BD%B2/"}],"tags":[{"name":"教程","slug":"教程","permalink":"http://kiedeng.github.io/tags/%E6%95%99%E7%A8%8B/"},{"name":"hexo","slug":"hexo","permalink":"http://kiedeng.github.io/tags/hexo/"}]}],"categories":[{"name":"生活","slug":"生活","permalink":"http://kiedeng.github.io/categories/%E7%94%9F%E6%B4%BB/"},{"name":"hadoop","slug":"hadoop","permalink":"http://kiedeng.github.io/categories/hadoop/"},{"name":"安装","slug":"安装","permalink":"http://kiedeng.github.io/categories/%E5%AE%89%E8%A3%85/"},{"name":"Azkaba","slug":"Azkaba","permalink":"http://kiedeng.github.io/categories/Azkaba/"},{"name":"大数据","slug":"大数据","permalink":"http://kiedeng.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"typecho","slug":"typecho","permalink":"http://kiedeng.github.io/categories/typecho/"},{"name":"未分类","slug":"未分类","permalink":"http://kiedeng.github.io/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/"},{"name":"hive","slug":"hive","permalink":"http://kiedeng.github.io/categories/hive/"},{"name":"成长","slug":"成长","permalink":"http://kiedeng.github.io/categories/%E6%88%90%E9%95%BF/"},{"name":"mongdb","slug":"mongdb","permalink":"http://kiedeng.github.io/categories/mongdb/"},{"name":"Linux","slug":"Linux","permalink":"http://kiedeng.github.io/categories/Linux/"},{"name":"Hadoop","slug":"Hadoop","permalink":"http://kiedeng.github.io/categories/Hadoop/"},{"name":"Zookeeper","slug":"Zookeeper","permalink":"http://kiedeng.github.io/categories/Zookeeper/"},{"name":"常见错误","slug":"常见错误","permalink":"http://kiedeng.github.io/categories/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"name":"工具","slug":"工具","permalink":"http://kiedeng.github.io/categories/%E5%B7%A5%E5%85%B7/"},{"name":"Tomcat","slug":"Tomcat","permalink":"http://kiedeng.github.io/categories/Tomcat/"},{"name":"算法","slug":"算法","permalink":"http://kiedeng.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"MySQL","slug":"MySQL","permalink":"http://kiedeng.github.io/categories/MySQL/"},{"name":"Mapreduce","slug":"Hadoop/Mapreduce","permalink":"http://kiedeng.github.io/categories/Hadoop/Mapreduce/"},{"name":"HDFS","slug":"HDFS","permalink":"http://kiedeng.github.io/categories/HDFS/"},{"name":"Hadoop","slug":"HDFS/Hadoop","permalink":"http://kiedeng.github.io/categories/HDFS/Hadoop/"},{"name":"使用手册","slug":"使用手册","permalink":"http://kiedeng.github.io/categories/%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/"},{"name":"部署","slug":"部署","permalink":"http://kiedeng.github.io/categories/%E9%83%A8%E7%BD%B2/"}],"tags":[{"name":"小说 工具","slug":"小说-工具","permalink":"http://kiedeng.github.io/tags/%E5%B0%8F%E8%AF%B4-%E5%B7%A5%E5%85%B7/"},{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"},{"name":"LeetCode","slug":"LeetCode","permalink":"http://kiedeng.github.io/tags/LeetCode/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://kiedeng.github.io/tags/MapReduce/"},{"name":"部署","slug":"部署","permalink":"http://kiedeng.github.io/tags/%E9%83%A8%E7%BD%B2/"},{"name":"案例","slug":"案例","permalink":"http://kiedeng.github.io/tags/%E6%A1%88%E4%BE%8B/"},{"name":"原理","slug":"原理","permalink":"http://kiedeng.github.io/tags/%E5%8E%9F%E7%90%86/"},{"name":"代码","slug":"代码","permalink":"http://kiedeng.github.io/tags/%E4%BB%A3%E7%A0%81/"},{"name":"自动化","slug":"自动化","permalink":"http://kiedeng.github.io/tags/%E8%87%AA%E5%8A%A8%E5%8C%96/"},{"name":"Linux","slug":"Linux","permalink":"http://kiedeng.github.io/tags/Linux/"},{"name":"Hadoop","slug":"Hadoop","permalink":"http://kiedeng.github.io/tags/Hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kiedeng.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"VMware","slug":"VMware","permalink":"http://kiedeng.github.io/tags/VMware/"},{"name":"centos","slug":"centos","permalink":"http://kiedeng.github.io/tags/centos/"},{"name":"Git","slug":"Git","permalink":"http://kiedeng.github.io/tags/Git/"},{"name":"GitHub","slug":"GitHub","permalink":"http://kiedeng.github.io/tags/GitHub/"},{"name":"教程","slug":"教程","permalink":"http://kiedeng.github.io/tags/%E6%95%99%E7%A8%8B/"},{"name":"hexo","slug":"hexo","permalink":"http://kiedeng.github.io/tags/hexo/"}]}