{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://kiedeng.github.io","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2020-02-09T14:37:29.946Z","updated":"2020-02-09T14:37:29.946Z","comments":false,"path":"/404.html","permalink":"http://kiedeng.github.io/404.html","excerpt":"","text":""},{"title":"分类","date":"2020-02-09T15:25:56.144Z","updated":"2020-02-09T03:33:37.347Z","comments":false,"path":"categories/index.html","permalink":"http://kiedeng.github.io/categories/index.html","excerpt":"","text":""},{"title":"书单","date":"2020-02-09T06:13:49.098Z","updated":"2020-02-09T03:33:37.347Z","comments":false,"path":"books/index.html","permalink":"http://kiedeng.github.io/books/index.html","excerpt":"","text":""},{"title":"简历","date":"2020-02-09T16:43:45.968Z","updated":"2020-02-09T16:43:45.968Z","comments":false,"path":"about/index.html","permalink":"http://kiedeng.github.io/about/index.html","excerpt":"","text":"应届生求职模板基本信息： x某某 / gender / age&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;现&ensp;在&ensp;地： 山东青岛手&ensp;机&ensp;号： 178 xxxx xxxx&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;邮&emsp;&emsp;箱： congsw@foxmail.com证&emsp;&emsp;书： 英语四/六级、计算机二级&emsp;&emsp;&emsp;&emsp;&ensp;实习经验： x年GitHub： https://github.com/congshengwuCSDN： &ensp;https://blog.csdn.net/c1024197824求职意向期望职位： 安卓开发工程师&emsp;&emsp;&emsp;&emsp;期望薪资： 0k-1000k教育背景xxxx大学&emsp;&emsp;&emsp;&emsp;&emsp;2015.09-2019.06&emsp;&emsp;&emsp;&emsp;&emsp;电子信息工程实习经历公司： xxxxxx信息科技有限公司&emsp;&emsp;职位： 安卓开发工程师&emsp;&emsp;时间： 2018.03-2019.03项目名称： xxxxxxxxxx项目简介： xxxxxxxxxx主要工作：xxxxxxxxxxxx。xxxxxxxxxxxx。xxxxxxxxxxxx。技术栈：基于谷歌AAC框架实现MVVM应用架构网络请求：Retrofit+OkHttp+RxJavaJson解析：谷歌GsonXML/Html解析：Jsoup图片加载：GlideUI及数据加载相关：DataBinding、Paging、自定义View个人作品名称： xxxx&emsp;&emsp;简介： xxxxxxxxxxxxxxxxxxxx。名称： yyyy简介： yyyyyyyyyyyyyyyyyyyy。名称： zzzz简介： zzzzzzzzzzzzzzzzzzzz。技能清单以下均为我熟练使用的技能：编程语言：Java、C安卓开发：熟悉安卓UI设计、布局、自定义控件开发，安卓数据存储SharedPreferences、文件等，微博、微信等第三方SDK集成，处理ANR、OOM等安卓框架：Gson、FastJson、谷歌AAC、EventBus、Glide、Retrofit、RxJava、OkHttp、Jsoup等其他：正则表达式、Git、SVN等以下是我接触并了解的技能：&emsp;&emsp;&emsp;Kotlin（学习中）、Linux简单命令、Java Servlet、Mysql、Html、爬虫等等自我评价&emsp;&emsp;我是一个热爱技术热爱编程的人，大学四年自学编程，做过很多小项目（贪吃蛇、计算器、简单的数据库读取录入系统等等），上架了3款个人开发的安卓小应用，各应用市场累计60多万的下载量，开源了几个小项目并得到了一些star，参与过两个商业安卓应用的开发，其中在开发“猫博”App过程中积累了我安卓开发的大部分经验和学习方法，真是受益匪浅。&emsp;&emsp;我希望在以后的工作中，深度上能学习掌握更多关于安卓开发的高级知识及数据结构算法；广度上能扩宽自己的技术栈，如前端、后端、混合开发以及其他比较新潮有趣的技术等；高度上希望经过未来几年的职业生涯发展，能成为软件架构师级的技术专家。"},{"title":"友情链接","date":"2020-02-09T03:33:37.348Z","updated":"2020-02-09T03:33:37.348Z","comments":true,"path":"links/index.html","permalink":"http://kiedeng.github.io/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2020-02-09T15:26:19.807Z","updated":"2020-02-09T15:26:19.807Z","comments":false,"path":"repository/index.html","permalink":"http://kiedeng.github.io/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2020-02-09T03:33:37.349Z","updated":"2020-02-09T03:33:37.349Z","comments":false,"path":"tags/index.html","permalink":"http://kiedeng.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Combiner合并","slug":"Combiner合并","date":"2020-02-27T09:47:40.000Z","updated":"2020-02-27T09:47:40.129Z","comments":true,"path":"2020/02/27/Combiner合并/","link":"","permalink":"http://kiedeng.github.io/2020/02/27/Combiner%E5%90%88%E5%B9%B6/","excerpt":"","text":"","categories":[{"name":"未分类","slug":"未分类","permalink":"http://kiedeng.github.io/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"map值类型不匹配","slug":"map值类型不匹配","date":"2020-02-27T07:48:50.000Z","updated":"2020-02-27T08:47:02.567Z","comments":true,"path":"2020/02/27/map值类型不匹配/","link":"","permalink":"http://kiedeng.github.io/2020/02/27/map%E5%80%BC%E7%B1%BB%E5%9E%8B%E4%B8%8D%E5%8C%B9%E9%85%8D/","excerpt":"","text":"map值类型不匹配123java.lang.Exception: java.io.IOException: Type mismatch in value from map: expected com.atguigu.mr.sort.FlowBean, received org.apache.hadoop.io.Text at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462) at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)错误位置：进行排序案例的时候发生的错误12job.setMapOutputKeyClass(FlowBean.class);job.setMapOutputValueClass(FlowBean.class);//应该为Text.class注：写Driver驱动的时候，要特别注意类型错误问题因为不太懂这方面的错，所以","categories":[{"name":"常见错误","slug":"常见错误","permalink":"http://kiedeng.github.io/categories/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"WritableComparable排序","slug":"WritableComparable排序","date":"2020-02-27T04:37:15.000Z","updated":"2020-02-27T10:40:34.388Z","comments":true,"path":"2020/02/27/WritableComparable排序/","link":"","permalink":"http://kiedeng.github.io/2020/02/27/WritableComparable%E6%8E%92%E5%BA%8F/","excerpt":"","text":"排序的分类部分排序MapReduce根据输入记录的键对数据集排序，保证输出的每个文件内部有序全排序最终输出结果为一个文件，且文件内部有序辅助排序：（GroupingComparator分组)​ 在Reduece端对key进行分组。应用于key为bean对象时，想让一个或几个字段相同的key 进入到同一个reduce方法时，可以采用分组排序二次排序在自定义排序过程中，如果compareTo中的判断添加为两个即为二次排序自定义排序WritableComparable原理分析​ bean对象作为key传输，需要实现WritableComparable接口重写compareTo方法，就可以实现排序12345678910111213@Overridepublic int compareTo(FlowBean o) &#123; int result; &#x2F;&#x2F; 按照总流量大小，倒序排列 if (sumFlow &gt; bean.getSumFlow()) &#123; result &#x3D; -1; &#125;else if (sumFlow &lt; bean.getSumFlow()) &#123; result &#x3D; 1; &#125;else &#123; result &#x3D; 0; &#125; return result;&#125;WritableComparable排序案例实操（全排序）需求：对总流量进行排序代码实现使用以前的Flowcount就可以实现，代码就可以实现，略排序案例（区内排序）需求：要求每个省份手机号输出的文件中按照总流量内部排序注意点：区间排序中，需要添加的是自定义的Patitioner分区类与在驱动类中添加分区类，设置Reducetask个数12345678910111213141516171819202122232425262728package com.atguigu.mapreduce.sort;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Partitioner;public class ProvincePartitioner extends Partitioner&lt;FlowBean, Text&gt; &#123; @Override public int getPartition(FlowBean key, Text value, int numPartitions) &#123; // 1 获取手机号码前三位 String preNum = value.toString().substring(0, 3); int partition = 4; // 2 根据手机号归属地设置分区 if (\"136\".equals(preNum)) &#123; partition = 0; &#125;else if (\"137\".equals(preNum)) &#123; partition = 1; &#125;else if (\"138\".equals(preNum)) &#123; partition = 2; &#125;else if (\"139\".equals(preNum)) &#123; partition = 3; &#125; return partition; &#125;&#125;1234// 加载自定义分区类job.setPartitionerClass(ProvincePartitioner.class);// 设置Reducetask个数job.setNumReduceTasks(5);GroupingComparator分组（辅助排序）对Reduce阶段的数据根据某一个或几个字段进行分组自定义继承WritableComparator重写compare()方法12345@Overridepublic int compare(WritableComparable a, WritableComparable b) &#123; // 比较的业务逻辑 return result;&#125;创建一个构造将比较对象的类传给父类123protected OrderGroupingComparator() &#123; super(OrderBean.class, true);&#125;GroupingComparator分组案例实操需求：有如下订单数据表4-2 订单数据订单id商品id成交金额0000001Pdt_01222.8Pdt_0233.80000002Pdt_03522.8Pdt_04122.4Pdt_05722.40000003Pdt_06232.8Pdt_0233.8现在需要求出每一个订单中最贵的商品。期望输出数据1 222.82 722.43 232.8需求分析利用“订单id和成交金额”作为key，可以将Map阶段读取到的所有订单数据按照id升序排序，如果id相同按照金额降序排序，发送到Reduce在Reduce端利用groupComparator将订单id相同的kv合成为组，然后取第一个即是该订单中最贵商品代码实现定义订单信息OrderBean类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package com.atguigu.mapreduce.order;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.WritableComparable;public class OrderBean implements WritableComparable&lt;OrderBean&gt; &#123; private int order_id; // 订单id号 private double price; // 价格 public OrderBean() &#123; super(); &#125; public OrderBean(int order_id, double price) &#123; super(); this.order_id = order_id; this.price = price; &#125; @Override public void write(DataOutput out) throws IOException &#123; out.writeInt(order_id); out.writeDouble(price); &#125; @Override public void readFields(DataInput in) throws IOException &#123; order_id = in.readInt(); price = in.readDouble(); &#125; @Override public String toString() &#123; return order_id + \"\\t\" + price; &#125; public int getOrder_id() &#123; return order_id; &#125; public void setOrder_id(int order_id) &#123; this.order_id = order_id; &#125; public double getPrice() &#123; return price; &#125; public void setPrice(double price) &#123; this.price = price; &#125; // 二次排序 @Override public int compareTo(OrderBean o) &#123; int result; if (order_id &gt; o.getOrder_id()) &#123; result = 1; &#125; else if (order_id &lt; o.getOrder_id()) &#123; result = -1; &#125; else &#123; // 价格倒序排序 result = price &gt; o.getPrice() ? -1 : 1; &#125; return result; &#125;&#125;编写OrderSortMapper类12345678910111213141516171819202122232425262728package com.atguigu.mapreduce.order;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class OrderMapper extends Mapper&lt;LongWritable, Text, OrderBean, NullWritable&gt; &#123; OrderBean k = new OrderBean(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 截取 String[] fields = line.split(\"\\t\"); // 3 封装对象 k.setOrder_id(Integer.parseInt(fields[0])); k.setPrice(Double.parseDouble(fields[2])); // 4 写出 context.write(k, NullWritable.get()); &#125;&#125;编写OrderSortGroupingComparator类12345678910111213141516171819202122232425262728package com.atguigu.mapreduce.order;import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;public class OrderGroupingComparator extends WritableComparator &#123; protected OrderGroupingComparator() &#123; super(OrderBean.class, true); &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; OrderBean aBean = (OrderBean) a; OrderBean bBean = (OrderBean) b; int result; if (aBean.getOrder_id() &gt; bBean.getOrder_id()) &#123; result = 1; &#125; else if (aBean.getOrder_id() &lt; bBean.getOrder_id()) &#123; result = -1; &#125; else &#123; result = 0; &#125; return result; &#125;&#125;编写OrderSortReducer类12345678910111213package com.atguigu.mapreduce.order;import java.io.IOException;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.Reducer;public class OrderReducer extends Reducer&lt;OrderBean, NullWritable, OrderBean, NullWritable&gt; &#123; @Override protected void reduce(OrderBean key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; context.write(key, NullWritable.get()); &#125;&#125;编写OrderSortDriver类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.atguigu.mapreduce.order;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class OrderDriver &#123; public static void main(String[] args) throws Exception, IOException &#123;// 输入输出路径需要根据自己电脑上实际的输入输出路径设置 args = new String[]&#123;\"e:/input/inputorder\" , \"e:/output1\"&#125;; // 1 获取配置信息 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2 设置jar包加载路径 job.setJarByClass(OrderDriver.class); // 3 加载map/reduce类 job.setMapperClass(OrderMapper.class); job.setReducerClass(OrderReducer.class); // 4 设置map输出数据key和value类型 job.setMapOutputKeyClass(OrderBean.class); job.setMapOutputValueClass(NullWritable.class); // 5 设置最终输出数据的key和value类型 job.setOutputKeyClass(OrderBean.class); job.setOutputValueClass(NullWritable.class); // 6 设置输入数据和输出数据路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 8 设置reduce端的分组 job.setGroupingComparatorClass(OrderGroupingComparator.class); // 7 提交 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125;","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://kiedeng.github.io/categories/Hadoop/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"Partition分区","slug":"Partition分区","date":"2020-02-26T15:51:54.000Z","updated":"2020-02-26T16:46:55.670Z","comments":true,"path":"2020/02/26/Partition分区/","link":"","permalink":"http://kiedeng.github.io/2020/02/26/Partition%E5%88%86%E5%8C%BA/","excerpt":"","text":"问题引出：将结果按照条件输出到不同文件（分区）中默认的分区是根据key的hashCode对ReduceTasks个数取模得到的。自定义Partitioner步骤自定义继承Partitioner,重写getPartition()方法在job驱动中，设置自定义Partitioner1job.setPatitionerClass(CustonPartitioner.class)自定义Partition后，要根据自定义Partition的逻辑设置相对应的ReduceTask1job.setNumReduceTasd();分区总结（1）如果Reduce Task的数量&gt;getPartition的结果数，则会多产生几个空的输出文件part-r-000xx；（2）如果1&lt;ReduceTask的数量&lt;getPartition的结果数，则有一部分分区数据无处安放，会Exception；（3）如果ReduceTask的数量=1，则不管MapTask端输出多少个分区文件，最终结果都交给这一个Reduce Task，最终也就只会产生一个结果文件part-r-00000；（4）分区号必须从零开始，逐一累加。Partition案例实操题目：将统计结果按照手机归属地不同省份输出到不同文件中（分区）添加分区类12345678910111213141516171819202122232425262728package com.atguigu.mapreduce.flowsum;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Partitioner;public class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; &#123; @Override public int getPartition(Text key, FlowBean value, int numPartitions) &#123; // 1 获取电话号码的前三位 String preNum = key.toString().substring(0, 3); int partition = 4; // 2 判断是哪个省 if (\"136\".equals(preNum)) &#123; partition = 0; &#125;else if (\"137\".equals(preNum)) &#123; partition = 1; &#125;else if (\"138\".equals(preNum)) &#123; partition = 2; &#125;else if (\"139\".equals(preNum)) &#123; partition = 3; &#125; return partition; &#125;&#125;在驱动函数中增加自定义数据分区设置和ReduceTask设置1234// 8 指定自定义数据分区job.setPartitionerClass(ProvincePartitioner.class);// 9 同时指定相应数量的reduce taskjob.setNumReduceTasks(5);","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://kiedeng.github.io/categories/Hadoop/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"背诵知识点","slug":"背诵知识点","date":"2020-02-26T15:21:28.000Z","updated":"2020-02-26T15:26:40.986Z","comments":true,"path":"2020/02/26/背诵知识点/","link":"","permalink":"http://kiedeng.github.io/2020/02/26/%E8%83%8C%E8%AF%B5%E7%9F%A5%E8%AF%86%E7%82%B9/","excerpt":"","text":"1234&#x2F;&#x2F; 键值对修改切割值conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, &quot; &quot;);&#x2F;&#x2F; 7设置每个切片InputSplit中划分三条记录NLineInputFormat.setNumLinesPerSplit(job, 3);","categories":[{"name":"未分类","slug":"未分类","permalink":"http://kiedeng.github.io/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"Text导包错误","slug":"Text导包错误","date":"2020-02-26T05:52:34.000Z","updated":"2020-02-26T05:58:21.071Z","comments":true,"path":"2020/02/26/Text导包错误/","link":"","permalink":"http://kiedeng.github.io/2020/02/26/Text%E5%AF%BC%E5%8C%85%E9%94%99%E8%AF%AF/","excerpt":"","text":"123456789101112131415java.lang.ClassCastException: class com.sun.jersey.core.impl.provider.entity.XMLJAXBElementProvider$Text at java.lang.Class.asSubclass(Class.java:3404) at org.apache.hadoop.mapred.JobConf.getOutputKeyComparator(JobConf.java:887) at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.init(MapTask.java:1004) at org.apache.hadoop.mapred.MapTask.createSortingCollector(MapTask.java:402) at org.apache.hadoop.mapred.MapTask.access$100(MapTask.java:81) at org.apache.hadoop.mapred.MapTask$NewOutputCollector.&lt;init&gt;(MapTask.java:698) at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:770) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341) at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:748)错误的导入了Text包，1234错误： import com.sun.jersey.core.impl.provider.entity.XMLJAXBElementProvider.Text;正确的包： import org.apache.hadoop.io.Text;","categories":[{"name":"常见错误","slug":"常见错误","permalink":"http://kiedeng.github.io/categories/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"LeetCode计划表","slug":"LeetCode计划表","date":"2020-02-21T05:26:42.000Z","updated":"2020-02-21T05:33:01.684Z","comments":true,"path":"2020/02/21/LeetCode计划表/","link":"","permalink":"http://kiedeng.github.io/2020/02/21/LeetCode%E8%AE%A1%E5%88%92%E8%A1%A8/","excerpt":"","text":"题目序号题目名称总结1Two Sum4Median of Two Sorted Arrays11Container With Most Water17Letter Combinations of a Phone Number21Merge Two Sorted Lists23Merge k Sorted Lists37Sudoku Solver39Combination Sum40Combination Sum II51N-Queens53Maximum Subarray56Merge Intervals57Insert Interval62Unique Paths63Unique Paths II64Minimum Path Sum69sqrt(x)70Climbing Stairs72Edit Distance78Subsets79Word Search91Decode Ways98Validate Binary Search Tree100Same Tree102Binary Tree Level Order Traversal110Balanced Binary Tree112Path Sum113Path Sum II115Distinct Subsequences120Triangle121Best Time to Buy and Sell Stock124Binary Tree Maximum Path Sum126Word Ladder II127Word Ladder128Longest Consecutive Sequence139Word Break (revisit)140Word Break II141Linked List Cycle145Binary Tree Postorder Traversal146LRU Cache148Sort List149Max Points on a Line153Find Minimum in Rotated Sorted Array154Find Minimum in Rotated Sorted Array II169Majority Element174Dungeon Game198House Robber200Number of Islands204Count Primes207Course Schedule208Implement Trie (Prefix Tree)210Course Schedule II216Combination Sum III218The Skyline Problem221Maximal Square239Sliding Window Maximum241Different Ways to Add Parentheses263Ugly Number264Ugly Number II268Missing Number282Expression Add Operators289Game of Life295Find Median from Data Stream297Serialize and Deserialize Binary Tree300Longest Increasing Subsequence301Remove Invalid Parentheses303Range Sum Query - Immutable304Range Sum Query 2D - Immutable309Best Time to Buy and Sell Stock with Cooldown312Burst Balloons315Count of Smaller Numbers After Self321Create Maximum Number322Coin Change329Longest Increasing Path in a Matrix332Reconstruct Itinerary347Top K Frequent Elements377Combination Sum IV380Insert Delete GetRandom O(1)381Insert Delete GetRandom O(1) - Duplicates allowed391Perfect Rectangle399Evaluate Division404Sum of Left Leaves409Longest Palindrome410Split Array Largest Sum416Partition Equal Subset Sum417Pacific Atlantic Water Flow432All O`one Data Structure438Find All Anagrams in a String449Serialize and Deserialize BST450Delete Node in a BST451Sort Characters By Frequency452Minimum Number of Arrows to Burst Balloons455Assign Cookies460LFU Cache461Hamming Distance463Island Perimeter464Can I Win470Implement Rand10() Using Rand7()476Number Complement477Total Hamming Distance480Sliding Window Median486Predict the Winner488Zuma Game494Target Sum2494Target Sum504Base 7516Longest Palindromic Subsequence525Contiguous Array530Minimum Absolute Difference in BST540Single Element in a Sorted Array543Diameter of Binary Tree546Remove Boxes547Friend Circles551Student Attendance Record I560Subarray Sum Equals K561Array Partition I566Reshape the Matrix567Permutation in String576Out of Boundary Paths606Construct String from Binary Tree611Valid Triangle Number617Merge Two Binary Trees621Task Scheduler628Maximum Product of Three Numbers633Sum of Square Numbers636Exclusive Time of Functions637Average of Levels in Binary Tree639Decode Ways II652Find Duplicate Subtrees654Maximum Binary Tree655Print Binary Tree657Judge Route Circle664Strange Printer668Kth Smallest Number in Multiplication Table669Trim a Binary Search Tree671Second Minimum Node In a Binary Tree673Number of Longest Increasing Subsequence674Longest Continuous Increasing Subsequence675Cut Off Trees for Golf Event676Implement Magic Dictionary677Map Sum Pairs678Valid Parenthesis String680Valid Palindrome II681Next Closest Time682Baseball Game683K Empty Slots684Redundant Connection685Redundant Connection II687Longest Univalue Path688Knight Probability in Chessboard690Employee Importance692Top K Frequent Words699Falling Squares707Design Linked List712Minimum ASCII Delete Sum for Two Strings715Range Module719Find K-th Smallest Pair Distance720Longest Word in Dictionary724Find Pivot Index725Split Linked List in Parts726Number of Atoms728Self Dividing Numbers729My Calendar I730Count Different Palindromic Subsequences731My Calendar II732My Calendar III733Flood Fill734Sentence Similarity735Asteroid Collision736Parse Lisp Expression737Sentence Similarity II740Delete and Earn741Cherry Pickup742Closest Leaf in a Binary Tree743Network Delay Time744Find Smallest Letter Greater Than Target745Prefix and Suffix Search746Min Cost Climbing Stairs748Shortest Completing Word749Contain Virus752Open the Lock753Cracking the Safe754Reach a Number755Pour Water758Bold Words in String759Employee Free Time762Prime Number of Set Bits in Binary Representation763Partition Labels769Max Chunks To Make Sorted773Sliding Puzzle775Global and Local Inversions778Swim in Rising Water784Letter Case Permutation786K-th Smallest Prime Fraction787Cheapest Flights Within K Stops790Domino and Tromino Tiling792Number of Matching Subsequences799Champagne Tower801Minimum Swaps To Make Sequences Increasing802Find Eventual Safe States803Bricks Falling When Hit813Largest Sum of Averages815Bus Routes817Linked List Components818Race Car2818Race Car823Binary Trees With Factors826Most Profit Assigning Work827Making A Large Island841Keys and Rooms847Shortest Path Visiting All Nodes848Shifting Letters856Score of Parentheses863All Nodes Distance K in Binary Tree864Shortest Path to Get All Keys865Smallest Subtree with all the Deepest Nodes871Minimum Number of Refueling Stops873Length of Longest Fibonacci Subsequence877Stone Game879Profitable Schemes882Reachable Nodes In Subdivided Graph886Possible Bipartition889Construct Binary Tree from Preorder and Postorder Traversal891Sum of Subsequence Widths894All Possible Full Binary Trees895Maximum Frequency Stack898Bitwise ORs of Subarrays901Online Stock Span902Numbers At Most N Given Digit Set9233Sum With Multiplicity926Flip String to Monotone Increasing934Shortest Bridge935Knight Dialer936Stamping The Sequence943Find the Shortest Superstring952Largest Component Size by Common Factor956Tallest Billboard959Regions Cut By Slashes964Least Operators to Express Number967Numbers With Same Consecutive Differences972Equal Rational Numbers973K Closest Points to Origin975Odd Even Jump979Distribute Coins in Binary Tree980Unique Paths III1000Minimum Cost to Merge Stones1017Convert to Base -21019Next Greater Node In Linked List1024Video Stitching1043Partition Array for Maximum Sum1092Shortest Common Supersequenc1105Filling Bookcase Shelves1106Parsing A Boolean Expression1124Longest Well-Performing Interval1125Smallest Sufficient Team1129Shortest Path with Alternating Colors的","categories":[{"name":"算法","slug":"算法","permalink":"http://kiedeng.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"LeetCode","slug":"LeetCode","permalink":"http://kiedeng.github.io/tags/LeetCode/"}]},{"title":"MapReduce工作流程","slug":"MapReduce工作流程","date":"2020-02-20T13:39:47.000Z","updated":"2020-02-20T13:39:47.432Z","comments":true,"path":"2020/02/20/MapReduce工作流程/","link":"","permalink":"http://kiedeng.github.io/2020/02/20/MapReduce%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B/","excerpt":"","text":"","categories":[{"name":"未分类","slug":"未分类","permalink":"http://kiedeng.github.io/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"Shuffle机制","slug":"Shuffle机制","date":"2020-02-20T13:38:49.000Z","updated":"2020-02-20T13:38:49.220Z","comments":true,"path":"2020/02/20/Shuffle机制/","link":"","permalink":"http://kiedeng.github.io/2020/02/20/Shuffle%E6%9C%BA%E5%88%B6/","excerpt":"","text":"","categories":[{"name":"未分类","slug":"未分类","permalink":"http://kiedeng.github.io/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"InputFormat数据输入","slug":"InputFormat数据输入","date":"2020-02-20T13:38:16.000Z","updated":"2020-02-26T03:34:10.043Z","comments":true,"path":"2020/02/20/InputFormat数据输入/","link":"","permalink":"http://kiedeng.github.io/2020/02/20/InputFormat%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5/","excerpt":"","text":"切片与MapTask并行度决定机制数据块：Block是HDFS物理上把数据分成一块一块。数据切片：数据切片只是逻辑上对输入进行分片，并不会再磁盘上将其切分成片进行存储。Job提交流程源码和切片源码详解Job提交流程源码详解12345678910111213141516171819202122232425262728293031323334waitForCompletion()submit();// 1建立连接 connect(); // 1）创建提交Job的代理 new Cluster(getConfiguration()); // （1）判断是本地yarn还是远程 initialize(jobTrackAddr, conf); // 2 提交jobsubmitter.submitJobInternal(Job.this, cluster) // 1）创建给集群提交数据的Stag路径 Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf); // 2）获取jobid ，并创建Job路径 JobID jobId = submitClient.getNewJobID(); // 3）拷贝jar包到集群copyAndConfigureFiles(job, submitJobDir); rUploader.uploadFiles(job, jobSubmitDir);// 4）计算切片，生成切片规划文件writeSplits(job, submitJobDir); maps = writeNewSplits(job, jobSubmitDir); input.getSplits(job);// 5）向Stag路径写XML配置文件writeConf(conf, submitJobFile); conf.writeXml(out);// 6）提交Job,返回提交状态status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());FileInputFormat切片机制切片机制简单的按照文件的内容长度进行切片切片大小，默认等于Block大小切片时不考虑数据集整体，而是逐个针对每一个文件单独切片注：每次切片，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片源码中计算机切片大小的公式Math. max(minSize, Math.min(maxSize, blockSize));maprecduce.input.fileinputformat.split.minsize=1默认值为1mapreduce.input.fileinputformat.split maxsize=LongMAXValue 默认值Long.MAXValue,因此，默认情况下，切片大小=blocksize获取切片信息API1234//获取切片的文件名称String name=inputSplit.getPath（）.getName（）：//根据文件类型获取切片信息FileSplit inputSplit =（FileSplit）context.get InputSplit（）；CombineTextInputFormat切片机制​ 默认的TextInputFormat切片机制是对任务按文件规划切片，会产生大量小文件，产生大量的MapTask，处理效率极其低下，故CombineTextInputFormat来处理大量小文件的情况。虚拟存储切片最大值设置1CombineTextInputFormat. setMaxInputSplitSize(job,4194304)://4m-切片机制生成切片过程包括：虚拟存储过程和切片过程二部分虚拟存储过程将目录下的所有文件与setMaxInputSize比较，小于则切分为一块，大于且小于两倍，则平分这一块，大于两倍，则切出一块setMaxInputSize的块切片过程判断虚拟存储文件大小是否大于setMaxInputSplitSize值，大于或等于则单独形成一个切片，否则和下一个切片进行合并，形成一个切片CombineTextInputFormat案例实操需求：将输入的大量小文件合并成一个切片统一处理（以WordCount为基础），准备四个文件。只需要在Driver中添加输入格式即可：12345// 如果不设置InputFormat，它默认用的是TextInputFormat.classjob.setInputFormatClass(CombineTextInputFormat.class);//虚拟存储切片最大值设置4mCombineTextInputFormat.setMaxInputSplitSize(job, 4194304);FileInputFormat实现类​ FileInputFormat 常见的接口实现类包括：TextinputFormat、KeyValue TextInputFormat、NLinelnputFormat、CombineTextinputFormat，自定义InputFormat等。TextInputFormatFileInputFile默认的实现类，按行读，键为字节偏移量，LongWritable类型KeyValueTextInputFormat通过分隔符，分为key，value，课通过设置1234// 设置切割符conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, \" \");// 设置输入格式job.setInputFormatClass(KeyValueTextInputFormat.class);NlineInputFormat按照行数N来划分，即切片数=输入文件总行数/N1234&#x2F;&#x2F; 7设置每个切片InputSplit中划分三条记录NLineInputFormat.setNumLinesPerSplit(job, 3); &#x2F;&#x2F; 8使用NLineInputFormat处理记录数 job.setInputFormatClass(NLineInputFormat.class);自定义InputFormat需要自定义一个类继承FileInputFormat改写RecordReader，实现封装为KV输出是使用SequenceFileOutFormat输出合并文件","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://kiedeng.github.io/categories/Hadoop/"}],"tags":[{"name":"MapReduce","slug":"MapReduce","permalink":"http://kiedeng.github.io/tags/MapReduce/"}]},{"title":"序列化案例","slug":"序列化案例","date":"2020-02-16T07:08:00.000Z","updated":"2020-02-20T11:27:43.180Z","comments":true,"path":"2020/02/16/序列化案例/","link":"","permalink":"http://kiedeng.github.io/2020/02/16/%E5%BA%8F%E5%88%97%E5%8C%96%E6%A1%88%E4%BE%8B/","excerpt":"","text":"需求：统计每一个手机号耗费的总上行流量，下行流量，总流量编写MapReduce程序：编写流量统计的Bean对象123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package com.atguigu.mapreduce.flowsum;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.Writable;// 1 实现writable接口public class FlowBean implements Writable&#123; private long upFlow; private long downFlow; private long sumFlow; //2 反序列化时，需要反射调用空参构造函数，所以必须有 public FlowBean() &#123; super(); &#125; public FlowBean(long upFlow, long downFlow) &#123; super(); this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = upFlow + downFlow; &#125; //3 写序列化方法 @Override public void write(DataOutput out) throws IOException &#123; out.writeLong(upFlow); out.writeLong(downFlow); out.writeLong(sumFlow); &#125; //4 反序列化方法 //5 反序列化方法读顺序必须和写序列化方法的写顺序必须一致 @Override public void readFields(DataInput in) throws IOException &#123; this.upFlow = in.readLong(); this.downFlow = in.readLong(); this.sumFlow = in.readLong(); &#125; // 6 编写toString方法，方便后续打印到文本 @Override public String toString() &#123; return upFlow + \"\\t\" + downFlow + \"\\t\" + sumFlow; &#125; public long getUpFlow() &#123; return upFlow; &#125; public void setUpFlow(long upFlow) &#123; this.upFlow = upFlow; &#125; public long getDownFlow() &#123; return downFlow; &#125; public void setDownFlow(long downFlow) &#123; this.downFlow = downFlow; &#125; public long getSumFlow() &#123; return sumFlow; &#125; public void setSumFlow(long sumFlow) &#123; this.sumFlow = sumFlow; &#125;&#125;编写Mapper类1234567891011121314151617181920212223242526272829303132333435package com.atguigu.mapreduce.flowsum;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class FlowCountMapper extends Mapper&lt;LongWritable, Text, Text, FlowBean&gt;&#123; FlowBean v = new FlowBean(); Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 切割字段 String[] fields = line.split(\"\\t\"); // 3 封装对象 // 取出手机号码 String phoneNum = fields[1]; // 取出上行流量和下行流量 long upFlow = Long.parseLong(fields[fields.length - 3]); long downFlow = Long.parseLong(fields[fields.length - 2]); k.set(phoneNum); v.set(downFlow, upFlow); // 4 写出 context.write(k, v); &#125;&#125;编写Reducer类1234567891011121314151617181920212223242526package com.atguigu.mapreduce.flowsum;import java.io.IOException;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class FlowCountReducer extends Reducer&lt;Text, FlowBean, Text, FlowBean&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;FlowBean&gt; values, Context context)throws IOException, InterruptedException &#123; long sum_upFlow = 0; long sum_downFlow = 0; // 1 遍历所用bean，将其中的上行流量，下行流量分别累加 for (FlowBean flowBean : values) &#123; sum_upFlow += flowBean.getUpFlow(); sum_downFlow += flowBean.getDownFlow(); &#125; // 2 封装对象 FlowBean resultBean = new FlowBean(sum_upFlow, sum_downFlow); // 3 写出 context.write(key, resultBean); &#125;&#125;编写Driver驱动类1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.atguigu.mapreduce.flowsum;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class FlowsumDriver &#123; public static void main(String[] args) throws IllegalArgumentException, IOException, ClassNotFoundException, InterruptedException &#123; // 输入输出路径需要根据自己电脑上实际的输入输出路径设置args = new String[] &#123; \"e:/input/inputflow\", \"e:/output1\" &#125;; // 1 获取配置信息，或者job对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 6 指定本程序的jar包所在的本地路径 job.setJarByClass(FlowsumDriver.class); // 2 指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(FlowCountMapper.class); job.setReducerClass(FlowCountReducer.class); // 3 指定mapper输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(FlowBean.class); // 4 指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); // 5 指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125;","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://kiedeng.github.io/categories/Hadoop/"}],"tags":[{"name":"MapReduce","slug":"MapReduce","permalink":"http://kiedeng.github.io/tags/MapReduce/"}]},{"title":"序列化概述","slug":"序列化概述","date":"2020-02-16T07:06:59.000Z","updated":"2020-02-20T10:58:32.980Z","comments":true,"path":"2020/02/16/序列化概述/","link":"","permalink":"http://kiedeng.github.io/2020/02/16/%E5%BA%8F%E5%88%97%E5%8C%96%E6%A6%82%E8%BF%B0/","excerpt":"","text":"序列化​ 序列化就是把内存中的对象，转换为字节序列（或者其他数据传输协议）以便于存储到磁盘（持久化）和网络传输。​ 反序列化就是将收到字节序列或者是磁盘的持久化数据，转换为内存中的对象原因：一般来说，“活的对象”只生产在内存里，关机断电就没有了，并且不能发送，然而序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机。由于java序列化是一个重量级序列化框架（Serializable），序列化的时候，会附带很多额外的信息（各种校验信息，Header，继承体系等），不便于在网络中高效传输。故Hadoop开发了一套序列化机制(Writable)。Hadoop序列化特点：紧凑：高效实用存储空间快速：读写数据的额外开销小可扩展：随着通信协议的升级而升级互操作：支持多语言的交互实现序列化接口（Writable）必须实现Writable接口反序列化时，需要反射调用空参构造函数，所以必须要有空参构造123public FlowBean()&#123; super();&#125;重写序列化方法123456@Overridepublic void write(DataOutput out) throws IOException &#123; out.writeLong(upFlow); out.writeLong(downFlow); out.writeLong(sumFlow);&#125;重写反序列化方法123456@Overridepublic void readFields(DataInput in) throws IOException &#123; upFlow = in.readLong(); downFlow = in.readLong(); sumFlow = in.readLong();&#125;注意反序列化的顺序和序列化的顺序完全一致重写toString（），可用“\\t”分开，方便后续用如果需要将自定义的bean放在key中传输，则还需要实现Comparable接口，因为MapReduce框中的Shuffle过程要求对key必须能排序。12345@Overridepublic int compareTo(FlowBean o) &#123; // 倒序排列，从大到小 return this.sumFlow &gt; o.getSumFlow() ? -1 : 1;&#125;","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://kiedeng.github.io/categories/Hadoop/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"centos安装MySQL","slug":"centos安装MySQL","date":"2020-02-16T02:14:41.000Z","updated":"2020-02-16T02:15:28.523Z","comments":true,"path":"2020/02/16/centos安装MySQL/","link":"","permalink":"http://kiedeng.github.io/2020/02/16/centos%E5%AE%89%E8%A3%85MySQL/","excerpt":"","text":"","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://kiedeng.github.io/categories/MySQL/"}],"tags":[{"name":"部署","slug":"部署","permalink":"http://kiedeng.github.io/tags/%E9%83%A8%E7%BD%B2/"}]},{"title":"WordCount案例","slug":"WordCount案例","date":"2020-02-15T07:23:09.000Z","updated":"2020-02-15T07:24:46.955Z","comments":true,"path":"2020/02/15/WordCount案例/","link":"","permalink":"http://kiedeng.github.io/2020/02/15/WordCount%E6%A1%88%E4%BE%8B/","excerpt":"","text":"需求：在给定的文本文件中输出每个单词出现的总次数创建maven工程pom.xml文件中添加一下依赖123456789101112131415161718192021222324252627&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;在项目的资源目录下/src/main/resources目录下，新建一个文件，命名为“log4j.properties”12345678log4j.rootLogger=INFO, stdoutlog4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%nlog4j.appender.logfile=org.apache.log4j.FileAppenderlog4j.appender.logfile.File=target/spring.loglog4j.appender.logfile.layout=org.apache.log4j.PatternLayoutlog4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n编写程序编写mapper1234567891011121314151617181920212223242526272829package com.atguigu.mapreduce;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class WordcountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123; Text k = new Text(); IntWritable v = new IntWritable(1); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 切割 String[] words = line.split(\" \"); // 3 输出 for (String word : words) &#123; k.set(word); context.write(k, v); &#125; &#125;&#125;编写Reduce类12345678910111213141516171819202122232425package com.atguigu.mapreduce.wordcount;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class WordcountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123;int sum;IntWritable v = new IntWritable(); @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException &#123; // 1 累加求和 sum = 0; for (IntWritable count : values) &#123; sum += count.get(); &#125; // 2 输出 v.set(sum); context.write(key,v); &#125;&#125;编写Driver驱动类12345678910111213141516171819202122232425262728293031323334353637383940414243package com.atguigu.mapreduce.wordcount;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class WordcountDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1 获取配置信息以及封装任务 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2 设置jar加载路径 job.setJarByClass(WordcountDriver.class); // 3 设置map和reduce类 job.setMapperClass(WordcountMapper.class); job.setReducerClass(WordcountReducer.class); // 4 设置map输出 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 5 设置最终输出kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // 6 设置输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 提交 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125;测试直接在eclipse/Idea上测试集群上测试maven打jar包，需要添加打包插件依赖12345678910111213141516171819202122232425262728293031323334&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.3.2&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin &lt;/artifactId&gt; &lt;configuration&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;com.atguigu.mr.WordcountDriver&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;如果工程显示红叉，项目右键-&gt;maven-&gt;update project即可maven install，将jar包放到Hadoop集群执行WordCount12hadoop jar wc.jar com.atguigu.wordcount.WordcountDriver /user/atguigu/input /user/atguigu/output","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://kiedeng.github.io/categories/Hadoop/"},{"name":"Mapreduce","slug":"Hadoop/Mapreduce","permalink":"http://kiedeng.github.io/categories/Hadoop/Mapreduce/"}],"tags":[{"name":"案例","slug":"案例","permalink":"http://kiedeng.github.io/tags/%E6%A1%88%E4%BE%8B/"}]},{"title":"HDFS_2.X新特性","slug":"HDFS-2-X新特性","date":"2020-02-13T17:46:00.000Z","updated":"2020-02-14T17:34:24.151Z","comments":true,"path":"2020/02/14/HDFS-2-X新特性/","link":"","permalink":"http://kiedeng.github.io/2020/02/14/HDFS-2-X%E6%96%B0%E7%89%B9%E6%80%A7/","excerpt":"","text":"集群间的数据拷贝scp实现两个远程主机之间的文件复制​ scp -r hello.txt root@hadoop103:/user/atguigu/hello.txt // 推 push​ scp -r [root@hadoop103:/user/atguigu/hello.txt hello.txt](mailto:root@hadoop103:/user/atguigu/hello.txt hello.txt) // 拉 pull​ scp -r root@hadoop103:/user/atguigu/hello.txt root@hadoop104:/user/atguigu //是通过本地主机中转实现两个远程主机的文件复制；如果在两个远程主机之间ssh没有配置的情况下可以使用该方式。采用distcp命令实现两个Hadoop集群之间的递归数据复制12 bin/hadoop distcphdfs://haoop102:9000/user/atguigu/hello.txt hdfs://hadoop103:9000/user/atguigu/hello.txt小文件存档弊端​ 每个文件按块存储，每个块的数据在NameNode的内存中，因此HDFS存储小文件会非常低效。因为大量小文件会用尽NameNode的大部分内存。解决小文件的办法之一​ HDFS存档文件或Har文件，是一个更高效的文件存档工具。HDFS存档文件对内还是一个一个独立文件，对NameNode而言是一个整体，减少NameNode内存实例需要启动YARN进程归档文件​ 把/user/atguigu/input目录里面的所有文件归档成一个叫input.har的归档文件，并把归档后文件存储到/user/atguigu/output路径下。1bin/hadoop archive -archiveName input.har –p /user/atguigu/input /user/atguigu/output查看归档12hadoop fs -lsr /user/atguigu/output/input.harhadoop fs -lsr har:///user/atguigu/output/input.har解归档文件1hadoop fs -cp har:/// user/atguigu/output/input.har/* /user/atguigu回收站开启回收站功能参数说明默认值fs.trash.interval=0,0表示禁用回收站默认值fs.trash.checkpoint.interval=0,检查回收站的间隔时间。如果为0，则该值和fs.tarsh.interval的参数相等要求fs.trash.checkpoint.interval&lt;=fs.trash.interval启动回收站修改core-site.xml,配置垃圾回收时间为1分钟1234&lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt;查看回收站回收站在集群中的路径：/user/atguigu/.Trash/….修改访问垃圾回收站用户名称进入垃圾回收站用户名称，默认是dr.who，修改为atguigu用户1234&lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;atguigu&lt;/value&gt;&lt;/property&gt;调用moveToTrash（）才进入回收站12Trash trash = New Trash(conf);trash.moveToTrash(path);恢复回收站数据12hadoop fs -mv/user/atguigu/.Trash/Current/user/atguigu/input /user/atguigu/input清空回收站1hadoop fs -expunge快照管理12345678hdfs dfsadmin -allowSnapshot 路径（功能描述：开启指定目录的快照功能）hdfs dfsadmin -disallowSnapshot 路径（功能描述：禁用指定目录的快照功能，默认是禁用）hdfs dfs -createSnapshot 路径（功能描述：对目录创建快照）hdfs dfs -createSnapshot 路径名称（功能描述：指定名称创建快照）hdfs dfs -renameSnapshot 略径旧名称新名称（功能描述：重命名快照）hdfs lsSnapshottableDir（功能描述：列出当前用户所有可快照目录）hdfs snapshotDiff 路径1 路径2（功能描述：比较两个快照目录的不同之处）hdfs dfs -deleteSnapshot&lt;path&gt;&lt;snapshotName&gt;（功能描述：删除快照）开启/禁用指定目录的快照功能12hdfs dfsadmin -allowSnapshot /user/atguigu/inputhdfs dfsadmin -disallowSnapshot /user/atguigu/input对目录创建快照12hdfs dfs -createSnapshot /user/atguigu/inputhdfs dfs -lsr /user/atguigu/input/.snapshot/指定名称创建快照1hdfs dfs -createSnapshot /user/atguigu/input miao170508快照重命名1hdfs dfs -renameSnapshot /user/atguigu/input/ miao170508 atguigu170508列出当前用户所有可快照目录1hdfs lsSnapshottableDir比较两个快照目录的不同之处1hdfs snapshotDiff /user/atguigu/input/ . .snapshot/atguigu170508恢复快照1hdfs dfs -cp /user/atguigu/input/.snapshot/s20170708-134303.027 /user","categories":[{"name":"HDFS","slug":"HDFS","permalink":"http://kiedeng.github.io/categories/HDFS/"},{"name":"Hadoop","slug":"HDFS/Hadoop","permalink":"http://kiedeng.github.io/categories/HDFS/Hadoop/"}],"tags":[{"name":"原理","slug":"原理","permalink":"http://kiedeng.github.io/tags/%E5%8E%9F%E7%90%86/"}]},{"title":"NN与DataNode","slug":"NN与DataNode","date":"2020-02-13T17:21:19.000Z","updated":"2020-02-14T01:01:48.935Z","comments":true,"path":"2020/02/14/NN与DataNode/","link":"","permalink":"http://kiedeng.github.io/2020/02/14/NN%E4%B8%8EDataNode/","excerpt":"","text":"NN和2NN工作机制FsImage:磁盘中备份元数据的文件Edits：每当元数据有更新或者添加元数据时，修改内存中元数据并追加到Edits，为防止该文件数据过大影响效率，因此需要定期进行FsImage和Edits合并，引入一个节点SecondaryNamenode,专门用于FsImage和Edits的合并第一阶段：NameNode启动第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。客户端对元数据进行增删改的请求。NameNode记录操作日志，更新滚动日志。NameNode在内存中对数据进行增删改。第二阶段：Secondary NameNode工作Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。Secondary NameNode请求执行CheckPoint。NameNode滚动正在写的Edits日志。将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。Secondary NameNode加载编辑日志和镜像文件到内存，并合并。生成新的镜像文件fsimage.chkpoint。拷贝fsimage.chkpoint到NameNode。NameNode将fsimage.chkpoint重新命名成fsimage。* opt/module/hadoop-2.7.2/data/tmp/dfs/name/current *oiv查看Fsimage语法：hdfs oiv -p 文件类型 -i镜像文件 -o 转换后文件输出路径1hdfs oiv -p XML -i fsimage_0000000000000000025 -o /opt/module/hadoop-2.7.2/fsimage.xmloev查看Edits文件语法：hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径1hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-2.7.2/edits.xmlCheckPoint时间设置通常情况下，SecondaryNameNade每隔一个小时执行一次，在【hdfs-default.xml】中设置1234&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;3600&lt;/value&gt;&lt;/property&gt;一分钟检查一次操作次数，3当操作次数达到1百万时，SecondaryNameNode执行一次。1234567891011&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.txns&lt;/name&gt; &lt;value&gt;1000000&lt;/value&gt;&lt;description&gt;操作动作次数&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.check.period&lt;/name&gt; &lt;value&gt;60&lt;/value&gt;&lt;description&gt; 1分钟检查一次操作次数&lt;/description&gt;&lt;/property &gt;NameNode故障处理方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录kill -9 NameNode过程删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name）拷贝SecondaryNameNode中数据到原NameNode存储数据目录1scp -r atguigu@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* ./name/重新启动NameNode方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中。修改hdfs-site.xml中的123456789&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;120&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp/dfs/name&lt;/value&gt;&lt;/property&gt;kill -9 NameNode进程删除NameNode存储的数据将2NN存储数据的目录拷贝到NN存储数据的评级目录，并删除in_use.lock文件导入检查点数据sbin/hadoop-daemon.sh start namenode启动NameNodesbin/hadoop-daemon.sh start namenode安全模式（1）bin/hdfs dfsadmin -safemode get （功能描述：查看安全模式状态）（2）bin/hdfs dfsadmin -safemode enter （功能描述：进入安全模式状态）（3）bin/hdfs dfsadmin -safemode leave （功能描述：离开安全模式状态）（4）bin/hdfs dfsadmin -safemode wait （功能描述：等待安全模式状态）DataNodeDataNode工作机制一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。集群运行中可以安全加入和退出一些机器。保证DataNode的数据完整性1）当DataNode读取Block的时候，它会计算CheckSum。2）如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。3）Client读取其他DataNode上的Block。4）DataNode在其文件创建后周期验证CheckSum掉线时限参数设置如果定义超时时间为TimeOut，则超时时长的计算公式为：TimeOut =2dfs.namenode.heartbeat.recheck-interval+10dfs.heartbeat interval。而默认的dfs.namenode.heartbeat.recheck-interval 大小为5分钟，dfs.heartbeatinterval默认为3秒。配置位置：hdfs-site.xml，配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。12345678&lt;property&gt; &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;/name&gt; &lt;value&gt;300000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.heartbeat.interval&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt;服役新节点环境准备​ （1）在hadoop104主机上再克隆一台hadoop105主机​ （2）修改IP地址和主机名称​ （3）删除原来HDFS文件系统留存的文件（/opt/module/hadoop-2.7.2/data和log）​ （4）source一下配置文件12sbin&#x2F;hadoop-daemon.sh start datanodesbin&#x2F;yarn-daemon.sh start nodemanager添加白名单在NameNode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts文件，将允许的集群IP写入在NameNode的hdfs-site.xml配置文件中增加dfs.hosts属性1234&lt;property&gt; &lt;name&gt;dfs.hosts&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts&lt;/value&gt;&lt;/property&gt;配置文件分发刷新namenode：hdfs dfsadmin -refreshNodes更新ResourceManager节点：yarn rmadmin -refreshNodes数据如果不平衡，则：start-balancer.sh添加黑名单（不能同时有节点出现在两个中）在NameNode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts.exclude文件在NameNode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性1234&lt;property&gt;&lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude&lt;/value&gt;&lt;/property&gt;刷新namenode和ResourcemanagerDatanode多目录配置DataNode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本hdfs-site.xml1234&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;&lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2&lt;/value&gt;&lt;/property&gt;","categories":[{"name":"HDFS","slug":"HDFS","permalink":"http://kiedeng.github.io/categories/HDFS/"},{"name":"Hadoop","slug":"HDFS/Hadoop","permalink":"http://kiedeng.github.io/categories/HDFS/Hadoop/"}],"tags":[{"name":"原理","slug":"原理","permalink":"http://kiedeng.github.io/tags/%E5%8E%9F%E7%90%86/"}]},{"title":"HDFS的数据流","slug":"HDFS的数据流","date":"2020-02-13T10:03:05.000Z","updated":"2020-02-13T17:20:15.945Z","comments":true,"path":"2020/02/13/HDFS的数据流/","link":"","permalink":"http://kiedeng.github.io/2020/02/13/HDFS%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B5%81/","excerpt":"","text":"HDFS写数据流程客户端通过Distributed FileSysetm模块向NameNode请求上传文件，NameNode检查文件是否已存在，父目录是否存在NameNode返回是否可以上传客户端请求第一个Block上传哪几个DataNode服务器上NameNode返回三个DataNode节点，分别为dn1，dn2，dn3客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成dn1，dn2，dn3逐级应答客户端客户端开始往dn1上传第一个Block（先从磁盘读取数据收到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传个dn3，dn1没传一个packet会放入一个应答队列等待应答当一个Bloc传输完成之后，客户端再次请求NameNode上传第二个Block服务器节点距离：两个节点到达最近的共同祖先的距离总和。机架感知对于常见情况，当复制因子为3时，HDFS的放置策略是将一个副本放在本地机架中的一个节点上，另一个放在本地机架中的另一个节点上，最后一个放在不同机架中的另一个节点上HDFS读数据流程客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。","categories":[{"name":"HDFS","slug":"HDFS","permalink":"http://kiedeng.github.io/categories/HDFS/"},{"name":"Hadoop","slug":"HDFS/Hadoop","permalink":"http://kiedeng.github.io/categories/HDFS/Hadoop/"}],"tags":[{"name":"原理","slug":"原理","permalink":"http://kiedeng.github.io/tags/%E5%8E%9F%E7%90%86/"}]},{"title":"HDFS客户端操作","slug":"HDFS客户端操作","date":"2020-02-13T03:16:18.000Z","updated":"2020-02-13T10:05:11.257Z","comments":true,"path":"2020/02/13/HDFS客户端操作/","link":"","permalink":"http://kiedeng.github.io/2020/02/13/HDFS%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%93%8D%E4%BD%9C/","excerpt":"","text":"客户端环境准备将编译hadoop jar包放在路径下（比如：D:\\environment\\hadoop-2.7.2）配置HADOOP_HOME环境变量HADOOP_HOME : D:\\environment\\hadoop-2.7.2配置Path环境变量%HADOOP_HOME%\\bin创建一个Maven工程导入相应的依赖坐标+日志添加123456789101112131415161718192021222324252627282930313233343536373839404142&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.atguigu&lt;/groupId&gt; &lt;artifactId&gt;HDFS-0529&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;jdk.tools&lt;/groupId&gt; &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;JAVA_HOME&#125;/lib/tools.jar&lt;/systemPath&gt; &lt;/dependency&gt;&lt;/dependencies&gt; &lt;/project&gt;需要再项目的src/main/resources目录下，新建一个文件，命令为”log4j.properties”,在文件中填写log4j.rootLogger=INFO, stdoutlog4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%nlog4j.appender.logfile=org.apache.log4j.FileAppenderlog4j.appender.logfile.File=target/spring.loglog4j.appender.logfile.layout=org.apache.log4j.PatternLayoutlog4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n创建包名：com.atguigu.hdfs创建HdfsClient类12345678910111213141516171819public class HdfsClient&#123; @Testpublic void testMkdirs() throws IOException, InterruptedException, URISyntaxException&#123; // 1 获取文件系统 Configuration configuration = new Configuration(); // 配置在集群上运行 // configuration.set(\"fs.defaultFS\", \"hdfs://hadoop102:9000\"); // FileSystem fs = FileSystem.get(configuration); FileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:9000\"), configuration, \"atguigu\"); // 2 创建目录 fs.mkdirs(new Path(\"/1108/daxian/banzhang\")); // 3 关闭资源 fs.close(); &#125;&#125;执行程序客户端去操作HDFS时，是有一个用户身份的。默认情况下，HDFS客户端API会从JVM中获取一个参数来作为自己的用户身份：-DHADOOP_USER_NAME=atguigu，atguigu为用户名称。1234567891011public static void main(String[] args) throws Exception &#123; // 配置文件 Configuration conf = new Configuration(); conf.set(\"fs.defaultFS\", \"hdfs://hadoop102:9000\"); // 获取客户端对象 FileSystem fs = FileSystem.get(conf); fs.mkdirs(new Path(\"/kangdong\")); // 关闭客户端 fs.close(); System.out.println(\"Over!\"); &#125;HDFS的API操作HDFS文件上传（测试参数优先级）编写源代码1234567891011public static void main(String[] args) throws Exception &#123; // 配置文件 Configuration conf = new Configuration(); conf.set(\"fs.defaultFS\", \"hdfs://hadoop102:9000\"); // 获取客户端对象 FileSystem fs = FileSystem.get(conf); fs.mkdirs(new Path(\"/kangdong\")); // 关闭客户端 fs.close(); System.out.println(\"Over!\"); &#125;将hdfs-site.xml拷贝到项目的根目录（即资源目录下）123456789&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;参数优先级参数优先级排序：（1）客户端代码中设置的值 &gt;（2）ClassPath下的用户自定义配置文件 &gt;（3）然后是服务器的默认配置HDFS文件的下载12345// boolean delSrc 指是否将原文件删除// Path src 指要下载的文件路径// Path dst 指将文件下载到的路径// boolean useRawLocalFileSystem 是否开启文件校验fs.copyToLocalFile(false, new Path(\"/banzhang.txt\"), new Path(\"e:/banhua.txt\"), true);HDFS文件的删除1fs.delete(new Path(\"/0508/\"), true);//第一参数是路径，第二个参数是判断是否递归删除HDFS文件的改名1fs.rename(new Path(\"/banzhang.txt\"), new Path(\"/banhua.txt\"));HDFS文件详情的查看123456789101112131415161718192021222324252627282930313233343536373839404142@Testpublic void testListFiles() throws IOException, InterruptedException, URISyntaxException&#123; // 1获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:9000\"), configuration, \"atguigu\"); // 2 获取文件详情 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path(\"/\"), true); while(listFiles.hasNext())&#123; LocatedFileStatus status = listFiles.next(); // 输出详情 // 文件名称 System.out.println(status.getPath().getName()); // 长度 System.out.println(status.getLen()); // 权限 System.out.println(status.getPermission()); // 分组 System.out.println(status.getGroup()); // 获取存储的块信息 BlockLocation[] blockLocations = status.getBlockLocations(); for (BlockLocation blockLocation : blockLocations) &#123; // 获取块存储的主机节点 String[] hosts = blockLocation.getHosts(); for (String host : hosts) &#123; System.out.println(host); &#125; &#125; System.out.println(\"-----------班长的分割线----------\"); &#125;// 3 关闭资源fs.close();&#125;HDFS的I/O流操作HDFS文件上传12345678910111213@Test public void IOcopyFromLocal() throws Exception&#123; Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:9000\"), conf, \"atguigu\"); FileInputStream fis = new FileInputStream(new File(\"d:/kiedeng.txt\")); FSDataOutputStream fout = fs.create(new Path(\"/kg.txt\")); IOUtils.copyBytes(fis, fout, conf); IOUtils.closeStream(fis); IOUtils.closeStream(fout); fs.close(); &#125;HDFS文件下载12345678910111213141516171819202122// 文件下载@Testpublic void getFileFromHDFS() throws IOException, InterruptedException, URISyntaxException&#123; // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:9000\"), configuration, \"atguigu\"); // 2 获取输入流 FSDataInputStream fis = fs.open(new Path(\"/banhua.txt\")); // 3 获取输出流 FileOutputStream fos = new FileOutputStream(new File(\"e:/banhua.txt\")); // 4 流的对拷 IOUtils.copyBytes(fis, fos, configuration); // 5 关闭资源 IOUtils.closeStream(fos); IOUtils.closeStream(fis); fs.close();&#125;定位文件读取下载第一块1234567891011121314151617181920212223242526@Testpublic void readFileSeek1() throws IOException, InterruptedException, URISyntaxException&#123; // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:9000\"), configuration, \"atguigu\"); // 2 获取输入流 FSDataInputStream fis = fs.open(new Path(\"/hadoop-2.7.2.tar.gz\")); // 3 创建输出流 FileOutputStream fos = new FileOutputStream(new File(\"e:/hadoop-2.7.2.tar.gz.part1\")); // 4 流的拷贝 byte[] buf = new byte[1024]; for(int i =0 ; i &lt; 1024 * 128; i++)&#123; fis.read(buf); fos.write(buf); &#125; // 5关闭资源 IOUtils.closeStream(fis); IOUtils.closeStream(fos);fs.close();&#125;下载第二块1234567891011121314151617181920212223@Testpublic void readFileSeek2() throws IOException, InterruptedException, URISyntaxException&#123; // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop102:9000\"), configuration, \"atguigu\"); // 2 打开输入流 FSDataInputStream fis = fs.open(new Path(\"/hadoop-2.7.2.tar.gz\")); // 3 定位输入数据位置 fis.seek(1024*1024*128); // 4 创建输出流 FileOutputStream fos = new FileOutputStream(new File(\"e:/hadoop-2.7.2.tar.gz.part2\")); // 5 流的对拷 IOUtils.copyBytes(fis, fos, configuration); // 6 关闭资源 IOUtils.closeStream(fis); IOUtils.closeStream(fos);&#125;合并文件​ 在Window命令窗口中进入到目录E:\\，然后执行如下命令，对数据进行合并​ type hadoop-2.7.2.tar.gz.part2 &gt;&gt; hadoop-2.7.2.tar.gz.part1合并完成后，将hadoop-2.7.2.tar.gz.part1重新命名为hadoop-2.7.2.tar.gz。解压发现该tar包非常完整。","categories":[{"name":"HDFS","slug":"HDFS","permalink":"http://kiedeng.github.io/categories/HDFS/"},{"name":"Hadoop","slug":"HDFS/Hadoop","permalink":"http://kiedeng.github.io/categories/HDFS/Hadoop/"}],"tags":[{"name":"代码","slug":"代码","permalink":"http://kiedeng.github.io/tags/%E4%BB%A3%E7%A0%81/"}]},{"title":"Hadoop编译源码","slug":"Hadoop编译源码","date":"2020-02-12T03:29:53.000Z","updated":"2020-02-13T02:02:58.820Z","comments":true,"path":"2020/02/12/Hadoop编译源码/","link":"","permalink":"http://kiedeng.github.io/2020/02/12/Hadoop%E7%BC%96%E8%AF%91%E6%BA%90%E7%A0%81/","excerpt":"","text":"jar包准备hadoop-2.7.2-src.tar.gzjdk-8u144-linux-x64.tar.gzapache-ant-1.9.9-bin.tar.gz（build工具，打包用的）apache-maven-3.0.5-bin.tar.gzprotobuf-2.5.0.tar.gz（序列化的框架）jar包安装JDK解压，配置环境变量JAVA_HOME和PATH#JAVA_HOME：export JAVA_HOME=/opt/module/jdk1.8.0_144export PATH=$PATH:$JAVA_HOME/binsource /etc/profile进行生效Maven解压，配置MAVEN_HOME和PATH[root@hadoop101 software]# tar -zxvf apache-maven-3.0.5-bin.tar.gz -C /opt/module/[root@hadoop101 apache-maven-3.0.5]# vi conf/settings.xml​​nexus-aliyun​central​Nexus aliyun​http://maven.aliyun.com/nexus/content/groups/public​[root@hadoop101 apache-maven-3.0.5]# vi /etc/profile#MAVEN_HOMEexport MAVEN_HOME=/opt/module/apache-maven-3.0.5export PATH=$PATH:$MAVEN_HOME/bin[root@hadoop101 software]#source /etc/profile验证命令：mvn-versionant解压、配置 ANT _HOME和PATH[root@hadoop101 software]# tar -zxvf apache-ant-1.9.9-bin.tar.gz -C /opt/module/[root@hadoop101 apache-ant-1.9.9]# vi /etc/profile#ANT_HOMEexport ANT_HOME=/opt/module/apache-ant-1.9.9export PATH=$PATH:$ANT_HOME/bi[root@hadoop101 software]#source /etc/profile安装 glibc-headers 和 g++ 命令如下[root@hadoop101 apache-ant-1.9.9]# yum install glibc-headers[root@hadoop101 apache-ant-1.9.9]# yum install gcc-c++安装make和cmake[root@hadoop101 apache-ant-1.9.9]# yum install make[root@hadoop101 apache-ant-1.9.9]# yum install cmake解压protobuf ，进入到解压后protobuf主目录，/opt/module/protobuf-2.5.0，然后相继执行命令[root@hadoop101 software]# tar -zxvf protobuf-2.5.0.tar.gz -C /opt/module/[root@hadoop101 opt]# cd /opt/module/protobuf-2.5.0/[root@hadoop101 protobuf-2.5.0]#./configure[root@hadoop101 protobuf-2.5.0]# make[root@hadoop101 protobuf-2.5.0]# make check[root@hadoop101 protobuf-2.5.0]# make install[root@hadoop101 protobuf-2.5.0]# ldconfig[root@hadoop101 hadoop-dist]# vi /etc/profile#LD_LIBRARY_PATHexport LD_LIBRARY_PATH=/opt/module/protobuf-2.5.0export PATH=$PATH:$LD_LIBRARY_PATH[root@hadoop101 software]#source /etc/profile安装openssl库[root@hadoop101 software]#yum install openssl-devel安装 ncurses-devel库[root@hadoop101 software]#yum install ncurses-devel编译源码解压源码到/opt/目录tar -zxvf hadoop-2.7.2-src.tar.gz -C /opt/进入源码主目录通过maven执行编译命令mvn package -Pdist,native -DskipTests -Dtar成功的64位hadoop包在/opt/hadoop-2.7.2-src/hadoop-dist/target下","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://kiedeng.github.io/categories/Hadoop/"}],"tags":[{"name":"默认","slug":"默认","permalink":"http://kiedeng.github.io/tags/%E9%BB%98%E8%AE%A4/"}]},{"title":"软件推荐","slug":"软件推荐","date":"2020-02-11T10:48:17.000Z","updated":"2020-02-11T11:25:52.268Z","comments":true,"path":"2020/02/11/软件推荐/","link":"","permalink":"http://kiedeng.github.io/2020/02/11/%E8%BD%AF%E4%BB%B6%E6%8E%A8%E8%8D%90/","excerpt":"","text":"TinyTask自动化操作工具Ditto复制神器SecureCRT一般大数据使用的远程工具PanDownload网盘下载天若OCR文字识别用于文字识别Snipaste截图贴图工具Typoramarkdown编辑器Notepad++编辑器，可以进行文件夹文字查找Sublime编辑器，可以远程服务器Bandizip解压工具VMware虚拟机Xshell/Xftp远程连接工具Adobe Acrobat DCpdf阅读，编辑Anki记忆工具Everything查看文件PotPlayer播放器Chrome浏览器","categories":[{"name":"工具","slug":"工具","permalink":"http://kiedeng.github.io/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"自动化","slug":"自动化","permalink":"http://kiedeng.github.io/tags/%E8%87%AA%E5%8A%A8%E5%8C%96/"}]},{"title":"Linux使用手册","slug":"Linux使用手册","date":"2020-02-10T08:57:11.000Z","updated":"2020-02-11T04:56:13.605Z","comments":true,"path":"2020/02/10/Linux使用手册/","link":"","permalink":"http://kiedeng.github.io/2020/02/10/Linux%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/","excerpt":"","text":"用户管理命令#添加useradd kiedeng 或者 useradd -g [组名] 用户名#设置密码useradd 用户名#查看用户是否存在id 用户名#查看创建的用户cat /etc/passwd#切换用户su 用户名（没有获得环境变量）su - 用户名 （获得环境变量）#删除用户userdel 用户名userdel -r 用户名 （用户和用户目录全部删除）#设置root权限vim /etc/sudoers在root下中添加一行用户名 ALL=(ALL) ALL用户名 ALL=(ALL) NOPASSWDALL （不需要密码）修改usermod 修改用户usemod -g 用户组 用户名用户组管理添加：groupadd 组名删除：groupdel 组名修改组：groupmod -n 新组名 老组名查看组：cat /etc/group文件权限类总共10位：0~90位：-表示文件，d代表文件，l代表链接文档1-3位确定属主的该文件权限4-6位确定属组的该文件的权限7-9位确定其他用户改文件的权限修改文件权限chmod 777 a.txtchmod -R 777 xiyou （递归删除）改变所有者chown [选项] [最终用户] [文件或者目录] 选项为-R （递归操作）改变所属组chgrp [最终用户组] [文件或者目录]搜索查找类find 查找文件或者目录基本语法：find [搜索范围] [选项]选项-name&lt;查询方式&gt; 按照指定文件名查找-user&lt;用户名&gt; 指定用户查找-size&lt;文件大小&gt; 按照文件大小查找 （+为大于，-为小于）比如：find /home -size +20458locate快速定位文件路径更新：updatedb基本语法：locate 搜索文件grep 过滤查找及“|”管道符基本语法：grep 选项 查找内容 源文件压缩和解压缩gzip/gunzip压缩只能压缩文件，不能压缩目录命令：gzip 文件；gunzip 文件. gzzip/unzip压缩基本语法：zip [选项] xxx.zip 将要压缩的内容 （目录或文件）unzip [选项] xxx.zip 解压文件选项：-d&lt;目录&gt; 指定压缩后文件存放的目录tar 打包tar [选项] xxx.tar.gz 将要打包进去的内容选项-c 产生tar打包文件-v 显示详细信息-f 指定压缩后的文件名-z 打包同时压缩-x 解包.tar文件压缩：tar -zcvf kie.tar.gz a.txt b.txt解压：tar -zxvf kie.tar.gz -C /opt磁盘分区类","categories":[{"name":"使用手册","slug":"使用手册","permalink":"http://kiedeng.github.io/categories/%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kiedeng.github.io/tags/Linux/"}]},{"title":"hadoop集群搭建","slug":"hadoop集群搭建","date":"2020-02-10T06:11:15.000Z","updated":"2020-02-20T05:39:48.560Z","comments":true,"path":"2020/02/10/hadoop集群搭建/","link":"","permalink":"http://kiedeng.github.io/2020/02/10/hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/","excerpt":"","text":"克隆虚拟机配置好的Linux虚拟机-&gt; 管理 -&gt; 克隆删除网卡，复制物理地址：vim /etc/udev/rules.d/70-persistent-net.rules删除eht0的那一行将下一行的eth0改为eth1复制address物理地址配置网络：vim /etc/sysconfig/network-scripts/ifcfg-eth0HWADDR:粘贴物理地址IPADDR=192.168.1.101 设置ipONBOOT=yesNM_CONTROLLED=yesB00TPROTO=staticGATEWAY=192.168.1.2DNS1=192.168.1.2 和网关一致修改主机名称：vim /etc/sysconfig/networkNETWORKING=yesHOSTNAME=hadoopl 01修改主机映射：vim /etc/hosts安装JDK通过sftp将jdk放在/opt/software下tar -zxvf [gz文件名] -C [解压的路径]设置环境路径#JAVA_HOMEexport JAVA_HOME=/opt/module/jdk1.8.0_144export PATH=$PATH:$JAVA_HOME/bin让修改以后的文件生效source /etc/profile测试是否成功指令java -version安装Hadoop将hadoop-2.7.2.tar.gz传入，解压在/etc/profile添加环境变量##HADOOP_HOMEexport HADOOP_HOME=/opt/module/hadoop-2.7.2export PATH=$PATH:$HADOOP_HOME/binexport PATH=$PATH:$HADOOP_HOME/sbin生效文件：source /etc/profileHadoop的目录结构bin目录：存放对Hadoop相关服务（HDFS和YARN）进行操作的脚本etc目录：Hadoop的配置目录，存放Hadoop的配置文件lib目录：存放Hadoop的本地库（对数据进行压缩解压功能）sbin目录：存放启动或者停止Hadoop相关服务的脚本share目录：存放hadoop的jar包，官方文档和案例Hadoop的运行模式​ 本地模式，伪分布式模式，完成分布式模式一，本地模式官方Grep案例在hadoop-2.7.2文件下面创建一个input文件夹复制文件到input文件夹里面执行share文件夹下的MapReduce程序bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output ‘dfs[a-z.]+’查看输出结果cat output/*官方WordCount案例（计算单词个数）创建文件夹（wcinput）在该文件夹创建并编辑文件内容执行hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount wcinput wcoutput查看结果hadoop-2.7.2]$ cat wcoutput/part-r-00000二，伪分布式运行模式启动HDFS并运行MapReduce程序配置集群hadoop-env.sh(添加jdk路径)export JAVA_HOME=/opt/module/jdk1.8.0_144core-site.xml123456789101112131415161718192021&lt;!-- 指定HDFS中NameNode的地址 --&gt;&lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt;&lt;value&gt;hdfs://hadoop101:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;&lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;&lt;/property&gt;hdfs-site.xml123456789&lt;!-- 指定HDFS副本的数量 --&gt;&lt;property&gt;&lt;name&gt;dfs.replication&lt;/name&gt;&lt;value&gt;1&lt;/value&gt;&lt;/property&gt;启动集群格式化NameNode（第一次启动时格式化，以后就不要格式化bin/hdfs namenode -format启动NameNodesbin/hadoop-daemon.sh start namenode启动DataNodesbin/hadoop-daemon.sh start datanode查看集群web访问默认端口：50070打开失败：https://www.cnblogs.com/zlslch/p/6604189.htmllog日志：/opt/module/hadoop-2.7.2/logs不能一直格式化NameNode原因会产生新的集群id，导致NameNode和DataNode的集群id不一致，集群找不到数据，所以格式化前先删除data和log日志操作集群（wordcount）执行操作和本地模式一样启动YARN并运行MapReduce程序配置集群yarn-env.sh(添加路径)export JAVA_HOME=/opt/module/jdk1.8.0_144配置yarn-site.xml123456789101112131415161718192021&lt;!-- Reducer获取数据的方式 --&gt;&lt;property&gt;&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;&lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定YARN的ResourceManager的地址 --&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;&lt;value&gt;hadoop101&lt;/value&gt;&lt;/property&gt;mapred-env.sh(添加jdk路径)(对mapred-site.xml.template重新命名为) mapred-site.xml123456789&lt;!-- 指定MR运行在YARN上 --&gt;&lt;property&gt;&lt;name&gt;mapreduce.framework.name&lt;/name&gt;&lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;启动集群启动前必须先启动NameNode和DataNode启动ResourceManagersbin/yarn-daemon.sh start resourcemanager启动NodeManagersbin/yarn-daemon.sh start nodemanager集群操作yarn访问端口：8088删除文件系统的ouput文件bin/hdfs dfs -rm -R /user/kiedong/output执行MapReduce程序bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/atguigu/input /user/atguigu/output配置历史服务器配置mapred-site.xml,添加12345678910111213141516171819&lt;!-- 历史服务器端地址 --&gt;&lt;property&gt;&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;&lt;value&gt;hadoop101:10020&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史服务器web端地址 --&gt;&lt;property&gt;&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;&lt;value&gt;hadoop101:19888&lt;/value&gt;&lt;/property&gt;启动历史服务器sbin/mr-jobhistory-daemon.sh start historyserver端口：19888配置日志的聚集日志聚集的概念：应用程序运行完成以后，程序运行日志上传到HDFS系统上日志聚集的好处：方便开发调试注意：开启此功能，需要重新启动NodeManager，ResourceManager和HistoryManageryarn-site.xml123456789101112131415161718192021&lt;!-- 日志聚集功能使能 --&gt;&lt;property&gt;&lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 日志保留时间设置7天 --&gt;&lt;property&gt;&lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;&lt;value&gt;604800&lt;/value&gt;&lt;/property&gt;关闭,然后NodeManager 、ResourceManager和HistoryManager执行WordCount案例查看日志端口：19888完全分布式运行模式（重点）主要步骤：准备三台客户机（关闭防火墙，静态ip，主机名称）安装JDK配置环境变量安装Hadoop配置环境变量配置集群单点启动配置ssh群起并测试集群编写集群分发脚本xsyncscp（secure copy）安全拷贝scp可以实现服务器与服务器之间的数据拷贝语法:将hadoop101传给hadoop102scp -r /opt/module root@hadoop102:/opt/modulersync远程同步工具主要用于备份和镜像。与scp区别：rsync只对差异性文件做更新。scp是复制所有文件rsync -rvl /opt/software root@hadoop102:/opt/software-r 递归 -v显示复制过程 -l拷贝符号连接xsync集群分发脚本（本集群使用）需求：循环复制文件到所有节点的相同目录下在home/用户名/bin这个目录下存放脚本，这样次用户可以在系统任何地方直接执行编写代码：vim xsync12345678910111213141516171819202122232425#!/bin/bash#1 获取输入参数个数，如果没有参数，直接退出pcount=$#if((pcount==0)); thenecho no args;exit;fi#2 获取文件名称p1=$1fname=`basename $p1`echo fname=$fname#3 获取上级目录到绝对路径pdir=`cd -P $(dirname $p1); pwd`echo pdir=$pdir#4 获取当前用户名称user=`whoami`#5 循环for((host=103; host&lt;105; host++)); do echo ------------------- hadoop$host -------------- rsync -rvl $pdir/$fname $user@hadoop$host:$pdirdone修改脚本xsync具有执行权限chomod 777 xsync配置集群核心配置文件core-site.xml123456789101112131415161718192021&lt;!-- 指定HDFS中NameNode的地址 --&gt;&lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt;&lt;value&gt;hdfs://hadoop102:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;&lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;&lt;/property&gt;HDFS配置文件hadoop-en.sh：配置环境变量hdfs-site.xml1234567891011121314151617&lt;property&gt;&lt;name&gt;dfs.replication&lt;/name&gt;&lt;value&gt;3&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;&lt;value&gt;hadoop104:50090&lt;/value&gt;&lt;/property&gt;YARN配置文件yarn-env.sh：配置环境变量yarn-site.xml123456789101112131415161718192021&lt;!-- Reducer获取数据的方式 --&gt;&lt;property&gt;&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;&lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定YARN的ResourceManager的地址 --&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;&lt;value&gt;hadoop103&lt;/value&gt;&lt;/property&gt;MapReduce配置文件mapred-env.sh：配置环境变量配置mapred-env.sh1234567891011cp mapred-site.xml.template mapred-site.xml&lt;!-- 指定MR运行在Yarn上 --&gt;&lt;property&gt;&lt;name&gt;mapreduce.framework.name&lt;/name&gt;&lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;集群单点启动第一次启动，需要格式化NameNodehadoop namenode -format在hadoop102上启动NameNodehadoop-daemon.sh start namenode在hadoop102,103,104分别启动DataNodessh无密码登陆配置原理将A服务器生成的公钥拷贝给B服务器，当A将数据用私钥A加密，则B用A的公钥解密，并将数据用公钥加密给A生成公钥和私钥ssh-keygen -t rsa将公钥拷贝到免密的目标机器上ssh-copy-id hadoop102ssh-copy-id hadoop103ssh-copy-id hadoop104注意：root用户需要再进行一次拷贝，在hadoop103生成公私钥拷贝给其他机器.ssh文件夹下文件功能解释known_hosts:记录ssh访问过计算机的公钥id_rsa : 生成的私钥id_rsa.pub:生成的公钥authorized_keys ： 存放授权过得无密登录服务器公钥群起集群配置slaves，并同步slaveshadoop102hadoop103hadoop104启动集群第一次先格式化NameNode（先停止以前运行的namenode和datanode，然后删除data和log数据）启动sbin/start-dfs.sh (hadoop102上)sbin/start-yarn.sh（hadoop03上）","categories":[{"name":"部署","slug":"部署","permalink":"http://kiedeng.github.io/categories/%E9%83%A8%E7%BD%B2/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://kiedeng.github.io/tags/Hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://kiedeng.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"Linux安装与配置","slug":"Linux安装与配置","date":"2020-02-10T05:35:12.000Z","updated":"2020-02-11T02:47:13.265Z","comments":true,"path":"2020/02/10/Linux安装与配置/","link":"","permalink":"http://kiedeng.github.io/2020/02/10/Linux%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/","excerpt":"","text":"安装时注意事项(用VMware安装centos[大数据虚拟机])检查BIOS虚拟化支持内存默认设置为2048MB最大磁盘大小默认为20GB是否对CD媒体进行测试，直接跳过Skip创建自定义分区(都是标准分区)boot 默认： 100MBwap 默认：2048MB/ 默认：15360自定义系统软件基本系统：兼容程序库；基本应用程序：互联网浏览器桌面：除了KDE,其余都选语言支持：中文支持Kdump去掉查看网络IP和网关编辑 -&gt; 虚拟网络编辑器 -&gt; NAT模式 即可看到子网IPNET设置可以看到网关设置IP自动获取登录后，通过界面来设置自动获取指定固定ip​ 直接修改配置文件来指定IP,并可以连接到外网(程序员推荐)，编辑 vi /etc/sysconfig/network-scripts/ifcfg-eth0​ 要求：将ip地址配置的静态的，ip地址为192.168.xxx.xxx1234567891011121314DEVICE&#x3D;eth0 #接口名（设备,网卡）HWADDR&#x3D;00:0C:2x:6x:0x:xx #MAC地址 TYPE&#x3D;Ethernet #网络类型（通常是Ethemet）UUID&#x3D;926a57ba-92c6-4231-bacb-f27e5e6a9f44 #随机id#系统启动的时候网络接口是否有效（yes&#x2F;no）ONBOOT&#x3D;yes # IP的配置方法[none|static|bootp|dhcp]（引导时不使用协议|静态分配IP|BOOTP协议|DHCP协议）BOOTPROTO&#x3D;static #IP地址IPADDR&#x3D;192.168.189.130 #网关 GATEWAY&#x3D;192.168.189.2 #域名解析器DNS1&#x3D;192.168.189.2重启网络服务或者重启系统生效：service network restart 、reboot修改主机名查看当前主机名：hostname修改主机名：/etc/hostname修改主机映射文件：vim /etc/sysconfig/network1234NETWORKING&#x3D;yesNETWORKING_IPV6&#x3D;noHOSTNAME&#x3D; hadoop &#x2F;&#x2F;写入新的主机名注意：主机名称不要有“_”下划线修改ip与主机的映射：/etc/hosts1192.168.102.130 hadoopWindows设置本地dns解析C:\\Windows\\System32\\drivers\\etc\\hosts添加内容：192.168.102.130 hadoop","categories":[{"name":"部署","slug":"部署","permalink":"http://kiedeng.github.io/categories/%E9%83%A8%E7%BD%B2/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kiedeng.github.io/tags/Linux/"},{"name":"VMware","slug":"VMware","permalink":"http://kiedeng.github.io/tags/VMware/"},{"name":"centos","slug":"centos","permalink":"http://kiedeng.github.io/tags/centos/"}]},{"title":"git的使用","slug":"git的使用","date":"2020-02-10T03:44:17.000Z","updated":"2020-02-10T05:13:33.539Z","comments":true,"path":"2020/02/10/git的使用/","link":"","permalink":"http://kiedeng.github.io/2020/02/10/git%E7%9A%84%E4%BD%BF%E7%94%A8/","excerpt":"","text":"Git的优势：大部分操作在本地完成，不需要联网完整性保证尽可能添加数据而不是删除或修改数据分支操作非常快捷流畅与Linux命令全面兼容代码托管中心代码托管中心的任务：维护远程库局域网环境：GitLab服务器外网环境：GitHub，码云Git命令行操作本库初始化（选择文件夹进行初始化）1git init设置签名作用：区分不同开发者的身份辨析：这里设置的签名和登录远程库(代码托管中心)的账号、密码没有任何关系。命令：123456789项目级别&#x2F;仓库级别：仅在当前本地库范围内有效git config user.name kiedenggit config user.email kiedeng@qq.com信息保存位置：.&#x2F;.git&#x2F;config 文件系统用户级别：登录当前操作系统的用户范围git config --global user.name tom_glbgit config --global goodMorning_pro@atguigu.com信息保存位置：~&#x2F;.gitconfig 文件基本操作123456789101112# 状态查看git status# 添加 (将工作区的文件或目录提交到暂存区)git [filename]# 提交 (将暂存区的文件提交的本地库)git commit -m &quot;commit message&quot; [filename]# 查看历史版本git loggit reflog# 版本的前进与后退（基于索引值操作）git reset --hard [局部索引值]git reset --hard a6ace91分支管理分支：在版本控制过程中，使用多条线同时推进多个任务。12345678# 创建分支git branch [分支名]# 查看分支git branch -v# 切换分支git checkout [分支名]# 合并git merge [被和并的分支名]GitHub12345678# 查看所有远程地址别名git remote -v# 创建远程库地址别名git remote add [别名] [远程地址]# 推送 (将本地库上传到github仓库)git push [别名] [分支名]# 克隆(这样克隆：把远程库下载到本地，初始化本地库，创建别名)git origin [远程地址]","categories":[{"name":"使用手册","slug":"使用手册","permalink":"http://kiedeng.github.io/categories/%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://kiedeng.github.io/tags/Git/"},{"name":"GitHub","slug":"GitHub","permalink":"http://kiedeng.github.io/tags/GitHub/"}]},{"title":"hexo搭建","slug":"hexo搭建","date":"2020-02-10T01:10:23.000Z","updated":"2020-02-10T03:42:36.626Z","comments":true,"path":"2020/02/10/hexo搭建/","link":"","permalink":"http://kiedeng.github.io/2020/02/10/hexo%E6%90%AD%E5%BB%BA/","excerpt":"","text":"官方文档： 链接一，使用Windows完成本地部署安装node.js和git,默认安装方式即可安装hexo，打开cmd执行1npm install hexo-cli -gcmd移动到选择的一个文件夹，比如：d:\\blog(下面全部假设初始化在本路径),进行hexo的初始化1hexo init blog在此目录安装1npm install启动服务器，访问的默认地址：http://localhost:4000/1hexo s二，使用GitHub完成远程部署注册，登录github新建仓库步骤如下：点击右上角+号，new repository，在Repository name处填 （你的gitusername）.github.io（比如：kiedeng.github.io），然后直接点Create repository在你初始化的路径（比如的d:\\blog）下有一个_config.xml,用记事本打开此文件，最后几行添加github信息#(对于repo，比如：https://github.com/kiedeng/kiedeng.github.io.git)deploy:type: gitrepo: https://github.com/( yours username)/（your username）.github.io.gitbranck: master将cmd移动到d:\\blog下，安装1npm install hexo-deployer-git --save执行123456# 清理hexo clean# 生成静态文件hexo generate# 上传hexo deploy在弹出的git窗口中输入你的GitHub邮箱和密码部署完成，等待一会，使用比如：http://kiedeng.github.io/访问三，更换hexo主题找到hexo的主题推荐主题：链接选择一款，到达它们的github仓库（如果该主题作者的有文档，按文档即可完成更换）将该主题下载下来（克隆也行），解压到d:\\blog\\themes,将该文件目录更名，比如：kiedeng打开d:\\blog_config.xml,将theme: 后面的参数改为1theme: kiedeng然后就可以部署和上传了四，绑定域名选择一个合适的域名，买下域名在域名的详细界面，打开解析设置dns解析在d:\\blog\\source目录下，新建一个叫CNAME的文件（强调：不能有后缀），里面的内容为你的域名，比如:www.kangdong.store等待一小会即可进行访问","categories":[{"name":"部署","slug":"部署","permalink":"http://kiedeng.github.io/categories/%E9%83%A8%E7%BD%B2/"}],"tags":[{"name":"教程","slug":"教程","permalink":"http://kiedeng.github.io/tags/%E6%95%99%E7%A8%8B/"},{"name":"hexo","slug":"hexo","permalink":"http://kiedeng.github.io/tags/hexo/"}]}]}